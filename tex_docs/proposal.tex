\documentclass[11pt]{article}
\usepackage{fullpage}
\setlength{\parskip}{4pt}

\begin{document}

\title{Optimizing Molecular Graphs in the Wasserstein Space \\
    \large Thesis Proposal}
\author{Octav Dragoi}

\maketitle


\begin{abstract}
    The aim of this project is to create a computational framework for optimization of molecular graphs with respect to certain desirable properties, in the space of Wasserstein point clouds.
\end{abstract}

\section{Motivation}
Automating molecular generative processes is a key step in improving the research process for novel molecules with desirable properties. There is a wide range of applications for automated molecule discovery, for example finding new pharmaceutical drugs or composite materials. Current research processes are mostly done by hand, relying on the intuition and experience of experts. Given the lengthy and expensive testing period until a new drug receives definitive approval, there is a real need for an automated, data-driven process for coming up with novel molecular structures.

This project approaches the problem of graph generation from the perspective of \textbf{graph optimization}, that is, starting with a given graph and modifying it to improve its desirable properties (antiviral action, bioactivity, etc) and reduce its undesirable properties (toxicity, etc). 

\section{Current State and Limitations}
Current state of the art models for molecule generation are based on Graph Neural Networks (GNNs) or Message Passing Neural Networks (MPNNs) specially designed for molecule applications, such as JTVAE \cite{DBLP:journals/corr/abs-1802-04364}. These models learn embeddings of molecules in metric spaces where optimization can be performed, then decode the optimized results back into corresponding molecular structures. Essentially, this pipeline is split in three different parts: encoding, optimization, and decoding.

Existing encoding approaches seek to learn embeddings for every node in the graph, and then create a graph embedding by applying an aggregation function, such as summation or average. This approach is lossy, meaning that at this step it can potentially lose structural information, such as pairwise node interactions. Even state of the art models for molecule property prediction, such as ChemProp \cite{yang2019analyzing} or DimeNet \cite{Klicpera2020Directional}, proceed in this way.

For decoding, existing literature focuses on sequential, or autoregressive, molecular generation. In this process, atoms are generated one by one, together with the connections to the previous atoms. In \cite{DBLP:journals/corr/abs-1802-04364,DBLP:journals/corr/abs-1812-01070,jin2020hierarchical, jin2019hierarchical}, tree and graph decoders pass through the nodes one by one in a depth-first fashion, predicting whether a node has children or not, and what type of connections do they have. Other graph generative models described in \cite{DBLP:journals/corr/abs-1901-00596} use auto-regressive models (e.g GraphRNN) or deconvolutional models (e.g. GraphVAE). All these models have the significant drawback of being dependent on the order in which the atoms are generated, as well as yielding invalid intermediate results that might not satisfy validity constraints, even if the final result does.

\section{Project Overview}
In order to overcome the limitations of the existing pipeline for graph optimization detailed above, we are proposing some improvements that should expand in the critical areas mentioned in the previous section.

To reduce structural and semantic information loss and preserve graph structure, we study two different graph encoding approaches, relying on the theory of optimal transport \cite{vayer2018optimal}. One consists in embedding the graph nodes and transforming the graph into a point cloud, where we can measure distances between these clouds using the Wasserstein distance. Alternatively, we can embed the graph nodes together with the edges into an adjacency tensor, and use the Gromov-Wasserstein distance between such tensors.

As a start, we will begin by studying the simpler method, graph embeddings as point clouds. For our optimization step, we plan to use the Wasserstein distance between point clouds to define the so-called “Wasserstein space”, a metric space of point clouds. Each molecular graph is embedded as a point cloud in this space, and then optimized by moving in certain “directions”, or “velocities”. In a manner similar with tools from Riemannian geometry, these directions contain the information on how should we modify our graph in order to improve its properties. 

This optimization process can be formalized within the Wasserstein space and geometry, which are suitable for dealing with sets of points. To do this in an efficient manner, we construct a Riemannian-like framework in which we can average datapoints, construct geodesics, compute barycenters, gradients, tangent spaces, velocities, exponential and logarithmic maps, all necessary tools to manipulate and optimize functions over the Wasserstein space. We draw inspiration from existing frameworks such as \cite{chowdhury2019gromovwasserstein}, and adapt them to our use cases. We will also explore different options to optimize computational efficiency, like in \cite{seguy2015principal, heeren2018principal,cuturi2013fast}.

This framework also allows us to construct molecular graphs directly and not sequentially. The latent representation as a set of node embeddings is decoded back into a molecular graph all at once, without the need to establish some (naturally biased) order in which to decode the molecule structure. This yields a permutation invariant model, useful to eliminate inductive biases that appear in sequential models.

Finally, another advantage of embedding the graph as a point cloud or adjacency tensor is the preservation of its inter-node structure. Using point cloud embeddings retains a representation for each individual node, and adjacency tensor embeddings additionally encodes edges between nodes. These inter-node relations are more explicitly captured this way, compared to compressing graphs into low-dimensional representations. However, higher complexity comes at a computational cost, both point cloud and adjacency tensor models having harder training stages and more intense distance computations.

\medskip
\bibliographystyle{ieeetr}
\bibliography{thesisbib}

\end{document}