{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600pt\" height=\"600pt\" viewBox=\"0 0 600 600\" version=\"1.1\">\n",
       "<g id=\"surface32\">\n",
       "<rect x=\"0\" y=\"0\" width=\"600\" height=\"600\" style=\"fill:rgb(100%,100%,100%);fill-opacity:1;stroke:none;\"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 488.582031 408.085938 L 464.300781 580 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 464.300781 580 L 580 365.691406 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 488.582031 408.085938 L 408.019531 225.117188 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 499.757812 20 L 408.019531 225.117188 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 580 365.691406 L 408.019531 225.117188 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 20 220.144531 L 204.734375 291.085938 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 48.988281 446.457031 L 204.734375 291.085938 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 153.480469 60.355469 L 204.734375 291.085938 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 408.019531 225.117188 L 204.734375 291.085938 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 464.300781 580 L 270.691406 519.925781 \"/>\n",
       "<path style=\"fill:none;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(26.666667%,26.666667%,26.666667%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 204.734375 291.085938 L 270.691406 519.925781 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 30 220.144531 C 30 225.664062 25.523438 230.144531 20 230.144531 C 14.476562 230.144531 10 225.664062 10 220.144531 C 10 214.621094 14.476562 210.144531 20 210.144531 C 25.523438 210.144531 30 214.621094 30 220.144531 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 498.582031 408.085938 C 498.582031 413.609375 494.105469 418.085938 488.582031 418.085938 C 483.058594 418.085938 478.582031 413.609375 478.582031 408.085938 C 478.582031 402.5625 483.058594 398.085938 488.582031 398.085938 C 494.105469 398.085938 498.582031 402.5625 498.582031 408.085938 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 509.757812 20 C 509.757812 25.523438 505.28125 30 499.757812 30 C 494.234375 30 489.757812 25.523438 489.757812 20 C 489.757812 14.476562 494.234375 10 499.757812 10 C 505.28125 10 509.757812 14.476562 509.757812 20 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 58.988281 446.457031 C 58.988281 451.980469 54.507812 456.457031 48.988281 456.457031 C 43.464844 456.457031 38.988281 451.980469 38.988281 446.457031 C 38.988281 440.9375 43.464844 436.457031 48.988281 436.457031 C 54.507812 436.457031 58.988281 440.9375 58.988281 446.457031 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 163.480469 60.355469 C 163.480469 65.878906 159.003906 70.355469 153.480469 70.355469 C 147.957031 70.355469 143.480469 65.878906 143.480469 60.355469 C 143.480469 54.832031 147.957031 50.355469 153.480469 50.355469 C 159.003906 50.355469 163.480469 54.832031 163.480469 60.355469 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 474.300781 580 C 474.300781 585.523438 469.824219 590 464.300781 590 C 458.777344 590 454.300781 585.523438 454.300781 580 C 454.300781 574.476562 458.777344 570 464.300781 570 C 469.824219 570 474.300781 574.476562 474.300781 580 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 590 365.691406 C 590 371.214844 585.523438 375.691406 580 375.691406 C 574.476562 375.691406 570 371.214844 570 365.691406 C 570 360.167969 574.476562 355.691406 580 355.691406 C 585.523438 355.691406 590 360.167969 590 365.691406 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 418.019531 225.117188 C 418.019531 230.640625 413.542969 235.117188 408.019531 235.117188 C 402.496094 235.117188 398.019531 230.640625 398.019531 225.117188 C 398.019531 219.59375 402.496094 215.117188 408.019531 215.117188 C 413.542969 215.117188 418.019531 219.59375 418.019531 225.117188 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 214.734375 291.085938 C 214.734375 296.609375 210.257812 301.085938 204.734375 301.085938 C 199.210938 301.085938 194.734375 296.609375 194.734375 291.085938 C 194.734375 285.5625 199.210938 281.085938 204.734375 281.085938 C 210.257812 281.085938 214.734375 285.5625 214.734375 291.085938 \"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,0%,0%);fill-opacity:1;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 280.691406 519.925781 C 280.691406 525.445312 276.214844 529.925781 270.691406 529.925781 C 265.167969 529.925781 260.691406 525.445312 260.691406 519.925781 C 260.691406 514.402344 265.167969 509.925781 270.691406 509.925781 C 276.214844 509.925781 280.691406 514.402344 280.691406 519.925781 \"/>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<igraph.drawing.Plot at 0x7f6f12e56278>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = ig.Graph.Erdos_Renyi(n = 10, m = 11)\n",
    "layout = g.layout('kk')\n",
    "ig.plot(g, layout = layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-5.0752, grad_fn=<NegBackward>)\n",
      "tensor(-5.0753, grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "adjM = torch.autograd.Variable(torch.Tensor(g.get_adjacency().data), requires_grad = True)\n",
    "N = adjM.shape[0]\n",
    "\n",
    "# get Laplacian\n",
    "L = torch.diag(torch.matmul(adjM, torch.ones(N))) - adjM\n",
    "L_mod = L + torch.ones_like(L)/N\n",
    "\n",
    "# calculate log dets\n",
    "eps = 10e-6\n",
    "print(-torch.logdet(L_mod))\n",
    "print(-torch.logdet(L_mod + eps * torch.eye(N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(159.9999, grad_fn=<DetBackward>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.det(L_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/octav/gitrepos/tum-thesis\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if \"ntbk\" in os.getcwd():\n",
    "    os.chdir(\"..\")\n",
    "print(os.getcwd())\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), \"otgnn\"))\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from otgnn.models import GCN\n",
    "from otgnn.graph import MolGraph\n",
    "from otgnn.utils import save_model, load_model\n",
    "from otgnn.graph import SYMBOLS, FORMAL_CHARGES, BOND_TYPES, get_bt_index\n",
    "\n",
    "from mol_opt.mol_opt import MolOpt\n",
    "from mol_opt.data_mol_opt import MolOptDataset\n",
    "from mol_opt.data_mol_opt import get_loader\n",
    "from mol_opt.decoder_mol_opt import MolOptDecoder\n",
    "from mol_opt.arguments import get_args\n",
    "from mol_opt.train_mol_opt import main, get_latest_model\n",
    "from mol_opt.ot_utils import encode_target, FGW\n",
    "from mol_opt.train_mol_opt import ft\n",
    "from mol_opt.ot_utils import Penalty as PenaltyOld\n",
    "\n",
    "from rdkit.Chem import MolFromSmiles\n",
    "\n",
    "from molgen.metrics.Penalty import Penalty as PenaltyNew\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = get_loader(\"iclr19-graph2graph/data/qed\", \"val\", 96, False)\n",
    "for i in data_loader:\n",
    "    X = (MolGraph(i[0]))\n",
    "    Y = (MolGraph(i[1]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = [\"\", \"-cuda\"]\n",
    "args = get_args()\n",
    "args.output_dir = \"mol_opt/output/\"\n",
    "\n",
    "mol_opt = MolOpt(args)\n",
    "mol_opt_decoder = MolOptDecoder(args)\n",
    "loss = FGW(alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.452212333679199"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_embedding, x_delta_hat = mol_opt.forward(X)\n",
    "yhat_embedding = x_embedding + x_delta_hat\n",
    "yhat_logits = mol_opt_decoder.forward(yhat_embedding, Y)\n",
    "yhat_labels = mol_opt_decoder.discretize(*yhat_logits)\n",
    "\n",
    "pred_pack = (yhat_labels, yhat_logits, Y.scope), Y\n",
    "loss(*pred_pack).item() / 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/octav/gitrepos/tum-thesis/otgnn/models/gromov_modules.py:394: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  nce_reg = torch.nn.LogSoftmax()(torch.stack(all_nce_dists))[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(331.4124, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(*pred_pack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[rdkit.Chem.rdchem.BondType.SINGLE,\n",
       " rdkit.Chem.rdchem.BondType.DOUBLE,\n",
       " rdkit.Chem.rdchem.BondType.TRIPLE,\n",
       " rdkit.Chem.rdchem.BondType.AROMATIC,\n",
       " None]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pen.BOND_TYPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_logits, charges_logits, bonds_logits = yhat_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([44494, 5])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bonds_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-104.4341, device='cuda:0', grad_fn=<MulBackward0>),\n",
       " tensor(0., device='cuda:0'))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pen = PenaltyNew(ft, connectivity=True, prev_epoch = 5, annealing_rate = 0.04)\n",
    "pen(*pred_pack, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7737809374999998"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pen.tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-81.9809, device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pen2 = PenaltyOld()\n",
    "pen2(*pred_pack) * 0.015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "bond_idx = 0\n",
    "num_atoms = 22\n",
    "adjM = bonds_soft[bond_idx:bond_idx+num_atoms*num_atoms].view(num_atoms, num_atoms, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8456, 0.8452, 0.8513, 0.8494, 0.8474, 0.8477, 0.8517, 0.8507, 0.8534,\n",
       "         0.8487, 0.8479, 0.8479, 0.8479, 0.8484, 0.8507, 0.8486, 0.8499, 0.8449,\n",
       "         0.8535, 0.8545, 0.8514, 0.8570],\n",
       "        [0.8452, 0.8500, 0.8541, 0.8484, 0.8484, 0.8462, 0.8506, 0.8503, 0.8588,\n",
       "         0.8520, 0.8492, 0.8473, 0.8493, 0.8472, 0.8495, 0.8491, 0.8523, 0.8452,\n",
       "         0.8541, 0.8573, 0.8517, 0.8555],\n",
       "        [0.8513, 0.8541, 0.8572, 0.8545, 0.8529, 0.8517, 0.8579, 0.8553, 0.8613,\n",
       "         0.8546, 0.8544, 0.8513, 0.8533, 0.8527, 0.8574, 0.8526, 0.8555, 0.8512,\n",
       "         0.8588, 0.8625, 0.8588, 0.8620],\n",
       "        [0.8494, 0.8484, 0.8545, 0.8485, 0.8509, 0.8497, 0.8536, 0.8531, 0.8593,\n",
       "         0.8508, 0.8504, 0.8498, 0.8521, 0.8513, 0.8547, 0.8492, 0.8545, 0.8490,\n",
       "         0.8564, 0.8561, 0.8548, 0.8589],\n",
       "        [0.8474, 0.8484, 0.8529, 0.8509, 0.8474, 0.8483, 0.8549, 0.8507, 0.8597,\n",
       "         0.8518, 0.8517, 0.8475, 0.8511, 0.8495, 0.8507, 0.8487, 0.8548, 0.8486,\n",
       "         0.8539, 0.8588, 0.8541, 0.8584],\n",
       "        [0.8477, 0.8462, 0.8517, 0.8497, 0.8483, 0.8498, 0.8539, 0.8509, 0.8584,\n",
       "         0.8487, 0.8520, 0.8506, 0.8507, 0.8492, 0.8499, 0.8491, 0.8526, 0.8492,\n",
       "         0.8541, 0.8543, 0.8546, 0.8605],\n",
       "        [0.8517, 0.8506, 0.8579, 0.8536, 0.8549, 0.8539, 0.8568, 0.8572, 0.8614,\n",
       "         0.8552, 0.8540, 0.8523, 0.8547, 0.8546, 0.8545, 0.8561, 0.8578, 0.8521,\n",
       "         0.8581, 0.8578, 0.8571, 0.8602],\n",
       "        [0.8507, 0.8503, 0.8553, 0.8531, 0.8507, 0.8509, 0.8572, 0.8542, 0.8600,\n",
       "         0.8520, 0.8517, 0.8508, 0.8525, 0.8514, 0.8530, 0.8505, 0.8545, 0.8499,\n",
       "         0.8556, 0.8575, 0.8558, 0.8614],\n",
       "        [0.8534, 0.8588, 0.8613, 0.8593, 0.8597, 0.8584, 0.8614, 0.8600, 0.8701,\n",
       "         0.8596, 0.8598, 0.8575, 0.8600, 0.8581, 0.8626, 0.8585, 0.8635, 0.8579,\n",
       "         0.8626, 0.8661, 0.8606, 0.8642],\n",
       "        [0.8487, 0.8520, 0.8546, 0.8508, 0.8518, 0.8487, 0.8552, 0.8520, 0.8596,\n",
       "         0.8555, 0.8510, 0.8500, 0.8509, 0.8499, 0.8540, 0.8505, 0.8544, 0.8498,\n",
       "         0.8549, 0.8590, 0.8520, 0.8570],\n",
       "        [0.8479, 0.8492, 0.8544, 0.8504, 0.8517, 0.8520, 0.8540, 0.8517, 0.8598,\n",
       "         0.8510, 0.8544, 0.8514, 0.8540, 0.8517, 0.8552, 0.8493, 0.8544, 0.8499,\n",
       "         0.8562, 0.8562, 0.8550, 0.8597],\n",
       "        [0.8479, 0.8473, 0.8513, 0.8498, 0.8475, 0.8506, 0.8523, 0.8508, 0.8575,\n",
       "         0.8500, 0.8514, 0.8480, 0.8491, 0.8485, 0.8525, 0.8519, 0.8530, 0.8466,\n",
       "         0.8547, 0.8548, 0.8531, 0.8581],\n",
       "        [0.8479, 0.8493, 0.8533, 0.8521, 0.8511, 0.8507, 0.8547, 0.8525, 0.8600,\n",
       "         0.8509, 0.8540, 0.8491, 0.8531, 0.8516, 0.8535, 0.8508, 0.8547, 0.8498,\n",
       "         0.8557, 0.8577, 0.8561, 0.8590],\n",
       "        [0.8484, 0.8472, 0.8527, 0.8513, 0.8495, 0.8492, 0.8546, 0.8514, 0.8581,\n",
       "         0.8499, 0.8517, 0.8485, 0.8516, 0.8487, 0.8515, 0.8499, 0.8512, 0.8473,\n",
       "         0.8547, 0.8552, 0.8544, 0.8597],\n",
       "        [0.8507, 0.8495, 0.8574, 0.8547, 0.8507, 0.8499, 0.8545, 0.8530, 0.8626,\n",
       "         0.8540, 0.8552, 0.8525, 0.8535, 0.8515, 0.8515, 0.8515, 0.8576, 0.8513,\n",
       "         0.8574, 0.8604, 0.8585, 0.8601],\n",
       "        [0.8486, 0.8491, 0.8526, 0.8492, 0.8487, 0.8491, 0.8561, 0.8505, 0.8585,\n",
       "         0.8505, 0.8493, 0.8519, 0.8508, 0.8499, 0.8515, 0.8521, 0.8545, 0.8465,\n",
       "         0.8538, 0.8548, 0.8531, 0.8583],\n",
       "        [0.8499, 0.8523, 0.8555, 0.8545, 0.8548, 0.8526, 0.8578, 0.8545, 0.8635,\n",
       "         0.8544, 0.8544, 0.8530, 0.8547, 0.8512, 0.8576, 0.8545, 0.8554, 0.8509,\n",
       "         0.8578, 0.8607, 0.8553, 0.8606],\n",
       "        [0.8449, 0.8452, 0.8512, 0.8490, 0.8486, 0.8492, 0.8521, 0.8499, 0.8579,\n",
       "         0.8498, 0.8499, 0.8466, 0.8498, 0.8473, 0.8513, 0.8465, 0.8509, 0.8472,\n",
       "         0.8531, 0.8550, 0.8520, 0.8569],\n",
       "        [0.8535, 0.8541, 0.8588, 0.8564, 0.8539, 0.8541, 0.8581, 0.8556, 0.8626,\n",
       "         0.8549, 0.8562, 0.8547, 0.8557, 0.8547, 0.8574, 0.8538, 0.8578, 0.8531,\n",
       "         0.8611, 0.8603, 0.8606, 0.8637],\n",
       "        [0.8545, 0.8573, 0.8625, 0.8561, 0.8588, 0.8543, 0.8578, 0.8575, 0.8661,\n",
       "         0.8590, 0.8562, 0.8548, 0.8577, 0.8552, 0.8604, 0.8548, 0.8607, 0.8550,\n",
       "         0.8603, 0.8598, 0.8592, 0.8613],\n",
       "        [0.8514, 0.8517, 0.8588, 0.8548, 0.8541, 0.8546, 0.8571, 0.8558, 0.8606,\n",
       "         0.8520, 0.8550, 0.8531, 0.8561, 0.8544, 0.8585, 0.8531, 0.8553, 0.8520,\n",
       "         0.8606, 0.8592, 0.8577, 0.8631],\n",
       "        [0.8570, 0.8555, 0.8620, 0.8589, 0.8584, 0.8605, 0.8602, 0.8614, 0.8642,\n",
       "         0.8570, 0.8597, 0.8581, 0.8590, 0.8597, 0.8601, 0.8583, 0.8606, 0.8569,\n",
       "         0.8637, 0.8613, 0.8631, 0.8672]], device='cuda:0',\n",
       "       grad_fn=<RsubBackward1>)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - adjM[:,:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2219, 0.2304, 0.2179, 0.1751, 0.1548], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjM[0,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 22, 5])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([44494, 5])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bonds_soft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1878,  0.2254,  0.1696, -0.0491, -0.1718], device='cuda:0',\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bonds_logits[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1544, 0.1548, 0.1487,  ..., 0.1446, 0.1421, 0.1404], device='cuda:0',\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bonds_soft[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonds_soft = 1-nn.functional.gumbel_softmax(logits = adjM, hard = True, dim = 2)[:,:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 22, 5])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
       "         1., 1., 0., 0.],\n",
       "        [1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
       "         0., 1., 1., 1.],\n",
       "        [1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
       "         1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,\n",
       "         0., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
       "         1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
       "         1., 1., 0., 1.],\n",
       "        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
       "         1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.],\n",
       "        [1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0.],\n",
       "        [1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         0., 1., 1., 1.],\n",
       "        [1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         0., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         0., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "         1., 0., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 0., 1., 1.],\n",
       "        [1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.],\n",
       "        [1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
       "         1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         0., 1., 0., 1.]], device='cuda:0', grad_fn=<RsubBackward1>)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bonds_soft"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
