{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/octav/gitrepos/tum-thesis\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if \"ntbk\" in os.getcwd():\n",
    "    os.chdir(\"..\")\n",
    "print(os.getcwd())\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), \"otgnn\"))\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from otgnn.models import GCN\n",
    "from otgnn.graph import MolGraph\n",
    "from otgnn.utils import save_model, load_model\n",
    "\n",
    "from mol_opt.mol_opt import MolOpt\n",
    "from mol_opt.data_mol_opt import MolOptDataset\n",
    "from mol_opt.data_mol_opt import get_loader\n",
    "from mol_opt.arguments import get_args\n",
    "from mol_opt.train_mol_opt import main, get_latest_model\n",
    "from mol_opt.ot_utils import compute_barycenter\n",
    "\n",
    "from rdkit.Chem import MolFromSmiles\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(N_transformer=6, agg_func='sum', batch_norm=False, batch_size=200, conn_lambda_end=3, conn_lambda_epochs_end=20, conn_lambda_epochs_start=12, conn_lambda_start=0.001, conn_penalty_function='capped_logdet', connectivity=True, connectivity_hard=False, cross_att_dim=150, cross_att_n_sinkhorn=2, cross_att_random=False, cross_att_sigmoid=False, cross_att_use=False, cross_att_use_gcn2=False, cuda=True, device='cuda:0', dim_tangent_space=40, dropout_ffn=0.0, dropout_gcn=0.0, dropout_transformer=0.1, euler_characteristic_penalty=True, euler_lambda_end=2, euler_lambda_epochs_end=20, euler_lambda_epochs_start=12, euler_lambda_start=0.001, ffn_activation='LeakyReLU', fgw_atoms=True, fgw_bonds=True, init_decoder_model='ffn-qed1_decode', init_model='ffn-qed1', linear_out=False, max_num_atoms=70, model_type='ffn', morgan_bits=0, n_epochs=30, n_ffn_hidden=100, n_ffn_transformer=100, n_heads_transformer=10, n_hidden=400, n_labels=1, n_layers=5, one_batch_train=False, ot_lambda=0, ot_solver='emd', output_dir='mol_opt/output_dev3/ffn-qed1', pc_hidden=150, penalty_gumbel=False, pred_hidden=150, rec_lambda_end=100, rec_lambda_epochs_end=50, rec_lambda_epochs_start=1, rec_lambda_start=1, reconstruction_loss=False, scale_lambdas=True, sinkhorn_entropy=0.1, sinkhorn_max_it=10000, task='qed', tau_end=0.01, tau_epochs_end=16, tau_epochs_start=1, tau_start=1, tb_logs_dir='mol_opt/logs_dev3/ffn-qed1', valency=True, valency_hard=False, valency_lambda_end=5, valency_lambda_epochs_end=20, valency_lambda_epochs_start=12, valency_lambda_start=0.001) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_type = \"ffn\"\n",
    "sys.argv = [\"\", \"-cuda\", \"-model_type\", model_type]\n",
    "args = get_args()\n",
    "outdir_suffix = \"dev3\"\n",
    "args.n_epochs = 30 \n",
    "args.init_model = \"{}-qed1\".format(model_type)\n",
    "# args.init_model = \"{}-test1\".format(model_type)\n",
    "args.init_decoder_model = \"{}_decode\".format(args.init_model)\n",
    "args.output_dir = \"mol_opt/output_{}/{}\".format(outdir_suffix, args.init_model)\n",
    "args.tb_logs_dir = \"mol_opt/logs_{}/{}\".format(outdir_suffix, args.init_model)\n",
    "args.batch_size = 200\n",
    "\n",
    "args.penalty_gumbel = False \n",
    "\n",
    "args.n_hidden = 400\n",
    "\n",
    "# for long runs\n",
    "args.scale_lambdas = True\n",
    "args.connectivity = True \n",
    "args.valency = True \n",
    "args.euler_characteristic_penalty = True\n",
    "args.conn_lambda_start = 0.001\n",
    "args.conn_lambda_end = 3\n",
    "args.conn_lambda_epochs_start = 12 \n",
    "args.conn_lambda_epochs_end = 20\n",
    "args.valency_lambda_start = 0.001\n",
    "args.valency_lambda_end = 5\n",
    "args.valency_lambda_epochs_start = 12\n",
    "args.valency_lambda_epochs_end = 20\n",
    "args.euler_lambda_start = 0.001 \n",
    "args.euler_lambda_end = 2 \n",
    "args.euler_lambda_epochs_start = 12 \n",
    "args.euler_lambda_epochs_end = 20\n",
    "args.tau_start = 1\n",
    "args.tau_end = 0.01\n",
    "args.tau_epochs_start = 1\n",
    "args.tau_epochs_end = 16\n",
    "\n",
    "args.cross_att_use = False\n",
    "args.cross_att_random = False\n",
    "args.cross_att_n_sinkhorn = 2\n",
    "args.cross_att_use_gcn2 = False\n",
    "args.cross_att_sigmoid = False\n",
    "args.ot_lambda = 0\n",
    "\n",
    "\n",
    "args.morgan_bits = 0\n",
    "\n",
    "args.conn_penalty_function = \"capped_logdet\" \n",
    "\n",
    "print(args, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_type = \"molemb\"\n",
    "# sys.argv = [\"\", \"-cuda\", \"-model_type\", model_type, \"-one_batch_train\"]\n",
    "# args = get_args()\n",
    "# outdir_suffix = \"dev3\"\n",
    "# args.n_epochs = 15000 \n",
    "# args.init_model = \"{}-morgan-test1\".format(model_type)\n",
    "# # args.init_model = \"{}-test1\".format(model_type)\n",
    "# args.init_decoder_model = \"{}_decode\".format(args.init_model)\n",
    "# args.output_dir = \"mol_opt/output_{}/{}\".format(outdir_suffix, args.init_model)\n",
    "# args.tb_logs_dir = \"mol_opt/logs_{}/{}\".format(outdir_suffix, args.init_model)\n",
    "# args.batch_size = 50 \n",
    "\n",
    "# args.penalty_gumbel = False \n",
    "\n",
    "# args.scale_lambdas = True\n",
    "# args.connectivity = True \n",
    "# args.valency = True \n",
    "# args.euler_characteristic_penalty = True\n",
    "# args.conn_lambda_start = 0.001\n",
    "# args.conn_lambda_end = 3\n",
    "# args.conn_lambda_epochs_start = 4001\n",
    "# args.conn_lambda_epochs_end = 12000\n",
    "# args.valency_lambda_start = 0.001\n",
    "# args.valency_lambda_end = 5\n",
    "# args.valency_lambda_epochs_start = 4001\n",
    "# args.valency_lambda_epochs_end = 12000\n",
    "# args.euler_lambda_start = 0.001 \n",
    "# args.euler_lambda_end = 2 \n",
    "# args.euler_lambda_epochs_start = 4001\n",
    "# args.euler_lambda_epochs_end = 12000\n",
    "# args.tau_start = 1\n",
    "# args.tau_end = 0.01\n",
    "# args.tau_epochs_start = 1\n",
    "# args.tau_epochs_end = 12000\n",
    "\n",
    "# args.morgan_bits = 1024\n",
    "\n",
    "# args.conn_penalty_function = \"capped_logdet\" \n",
    "\n",
    "# print(args, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = get_loader(\"iclr19-graph2graph/data/qed\", \"train_pairs_split\", args.batch_size)\n",
    "# train_data_loader = get_loader(\"molgen/data/chembl50\", \"train\", args.batch_size, same_number_atoms = True)\n",
    "val_data_loader = get_loader(\"iclr19-graph2graph/data/qed\", \"val_pairs_split\", args.batch_size)\n",
    "\n",
    "# train_data_loader = get_loader(\"molgen/data/chembl50\", \"train_split\", args.batch_size, same_number_atoms = True)\n",
    "# val_data_loader = get_loader(\"molgen/data/chembl50\", \"val_split\", args.batch_size, same_number_atoms = True)\n",
    "# val_data_loader = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No model ffn-qed1 found in mol_opt/output_dev3/ffn-qed1! Starting from scratch.\n",
      "Penalty params: tau=1.00000 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=-1 mode=[0 0 0] conn=True val=True euler=True\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/octav/gitrepos/tum-thesis/otgnn/models/gromov_modules.py:398: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  nce_reg = torch.nn.LogSoftmax()(torch.stack(all_nce_dists))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses Batch 0, train\n",
      " fgw:3.5472052\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:496.6563672\n",
      " euler_penalty:144.6498145\n",
      " total:3.7431479\n",
      "Penalty params: tau=1.00000 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=1 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 30, train\n",
      " fgw:23.0488861\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:16.0211731\n",
      " euler_penalty:4.6661230\n",
      " total:23.0552068\n",
      "Penalty params: tau=1.00000 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=1 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 60, train\n",
      " fgw:11.9575129\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:8.1419077\n",
      " euler_penalty:2.3713084\n",
      " total:11.9607250\n",
      "Penalty params: tau=1.00000 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=1 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 90, train\n",
      " fgw:8.1534231\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:5.4577623\n",
      " euler_penalty:1.5895584\n",
      " total:8.1555764\n",
      "Penalty params: tau=1.00000 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=1 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 120, train\n",
      " fgw:6.2333771\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:4.1045981\n",
      " euler_penalty:1.1954530\n",
      " total:6.2349964\n",
      "Penalty params: tau=1.00000 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=1 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 150, train\n",
      " fgw:5.0747449\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:3.2891150\n",
      " euler_penalty:0.9579458\n",
      " total:5.0760426\n",
      "Penalty params: tau=1.00000 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=1 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 180, train\n",
      " fgw:4.3002235\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:2.7439578\n",
      " euler_penalty:0.7991702\n",
      " total:4.3013060\n",
      "Penalty params: tau=1.00000 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=1 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 210, train\n",
      " fgw:3.7457610\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:2.3538216\n",
      " euler_penalty:0.6855441\n",
      " total:3.7466896\n",
      "Penalty params: tau=1.00000 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=1 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 240, train\n",
      " fgw:3.3280709\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:2.0608148\n",
      " euler_penalty:0.6002067\n",
      " total:3.3288839\n",
      "Penalty params: tau=1.00000 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=1 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 270, train\n",
      " fgw:3.0034616\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:1.8326803\n",
      " euler_penalty:0.5337632\n",
      " total:3.0041846\n",
      "Penalty params: tau=1.00000 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=1 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 300, train\n",
      " fgw:2.7432272\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:1.6500212\n",
      " euler_penalty:0.4805642\n",
      " total:2.7438782\n",
      "Penalty params: tau=1.00000 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=1 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 330, train\n",
      " fgw:2.5298165\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:1.5004724\n",
      " euler_penalty:0.4370085\n",
      " total:2.5304085\n",
      "Penalty params: tau=1.00000 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=1 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 360, train\n",
      " fgw:2.3515565\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:1.3757794\n",
      " euler_penalty:0.4006920\n",
      " total:2.3520993\n",
      "Penalty params: tau=1.00000 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=1 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 390, train\n",
      " fgw:2.2004967\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:1.2702209\n",
      " euler_penalty:0.3699484\n",
      " total:2.2009979\n",
      "Penalty params: tau=1.00000 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=1 mode=[0 0 0] conn=False val=False euler=False\n",
      "Penalty params: tau=1.00000 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=1 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Epoch 1, train\n",
      " fgw:2.1727317\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:1.2506928\n",
      " euler_penalty:0.3642609\n",
      " total:2.1732251\n",
      "Saving model, do not interrupt...\n",
      "Saved at mol_opt/output_dev3/ffn-qed1/model_ffn-qed1_1\n",
      "Losses Batch 0, val\n",
      " fgw:0.3997619\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3997619\n",
      "Losses Batch 30, val\n",
      " fgw:0.3965719\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3965719\n",
      "Losses Epoch 1, val\n",
      " fgw:0.3962552\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3962552\n",
      "Epoch duration: 2542.294445037842\n",
      "Epoch: 2\n",
      "Losses Batch 0, train\n",
      " fgw:0.3985707\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3985707\n",
      "Penalty params: tau=0.73564 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=2 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 30, train\n",
      " fgw:0.3907175\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3907175\n",
      "Penalty params: tau=0.73564 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=2 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 60, train\n",
      " fgw:0.3913180\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3913180\n",
      "Penalty params: tau=0.73564 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=2 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 90, train\n",
      " fgw:0.3896926\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3896926\n",
      "Penalty params: tau=0.73564 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=2 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 120, train\n",
      " fgw:0.3871828\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3871828\n",
      "Penalty params: tau=0.73564 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=2 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 150, train\n",
      " fgw:0.3850470\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3850470\n",
      "Penalty params: tau=0.73564 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=2 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 180, train\n",
      " fgw:0.3829413\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3829413\n",
      "Penalty params: tau=0.73564 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=2 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 210, train\n",
      " fgw:0.3812635\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3812635\n",
      "Penalty params: tau=0.73564 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=2 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 240, train\n",
      " fgw:0.3796702\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3796702\n",
      "Penalty params: tau=0.73564 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=2 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 270, train\n",
      " fgw:0.3784608\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3784608\n",
      "Penalty params: tau=0.73564 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=2 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 300, train\n",
      " fgw:0.3772610\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3772610\n",
      "Penalty params: tau=0.73564 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=2 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 330, train\n",
      " fgw:0.3766077\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3766077\n",
      "Penalty params: tau=0.73564 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=2 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 360, train\n",
      " fgw:0.3759528\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3759528\n",
      "Penalty params: tau=0.73564 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=2 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 390, train\n",
      " fgw:0.3758019\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3758019\n",
      "Penalty params: tau=0.73564 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=2 mode=[0 0 0] conn=False val=False euler=False\n",
      "Penalty params: tau=0.73564 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=2 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Epoch 2, train\n",
      " fgw:0.3757531\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3757531\n",
      "Saving model, do not interrupt...\n",
      "Saved at mol_opt/output_dev3/ffn-qed1/model_ffn-qed1_2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses Batch 0, val\n",
      " fgw:0.3747324\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3747324\n",
      "Losses Batch 30, val\n",
      " fgw:0.3736348\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3736348\n",
      "Losses Epoch 2, val\n",
      " fgw:0.3730891\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3730891\n",
      "Epoch duration: 2759.4115471839905\n",
      "Epoch: 3\n",
      "Losses Batch 0, train\n",
      " fgw:0.3770721\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3770721\n",
      "Penalty params: tau=0.54117 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=3 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 30, train\n",
      " fgw:0.3732325\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3732325\n",
      "Penalty params: tau=0.54117 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=3 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 60, train\n",
      " fgw:0.3717893\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3717893\n",
      "Penalty params: tau=0.54117 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=3 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 90, train\n",
      " fgw:0.3700379\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3700379\n",
      "Penalty params: tau=0.54117 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=3 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 120, train\n",
      " fgw:0.3695726\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3695726\n",
      "Penalty params: tau=0.54117 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=3 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 150, train\n",
      " fgw:0.3689790\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3689790\n",
      "Penalty params: tau=0.54117 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=3 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 180, train\n",
      " fgw:0.3690270\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3690270\n",
      "Penalty params: tau=0.54117 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=3 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 210, train\n",
      " fgw:0.3695590\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3695590\n",
      "Penalty params: tau=0.54117 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=3 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 240, train\n",
      " fgw:0.3706942\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3706942\n",
      "Penalty params: tau=0.54117 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=3 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 270, train\n",
      " fgw:0.3705087\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3705087\n",
      "Penalty params: tau=0.54117 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=3 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 300, train\n",
      " fgw:0.3700479\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3700479\n",
      "Penalty params: tau=0.54117 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=3 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 330, train\n",
      " fgw:0.3693246\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3693246\n",
      "Penalty params: tau=0.54117 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=3 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 360, train\n",
      " fgw:0.3686247\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3686247\n",
      "Penalty params: tau=0.54117 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=3 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Batch 390, train\n",
      " fgw:0.3680878\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3680878\n",
      "Penalty params: tau=0.54117 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=3 mode=[0 0 0] conn=False val=False euler=False\n",
      "Penalty params: tau=0.54117 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=3 mode=[0 0 0] conn=False val=False euler=False\n",
      "Losses Epoch 3, train\n",
      " fgw:0.3679169\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3679169\n",
      "Saving model, do not interrupt...\n",
      "Saved at mol_opt/output_dev3/ffn-qed1/model_ffn-qed1_3\n",
      "Losses Batch 0, val\n",
      " fgw:0.3612020\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3612020\n",
      "Losses Batch 30, val\n",
      " fgw:0.3600381\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3600381\n",
      "Losses Epoch 3, val\n",
      " fgw:0.3593168\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3593168\n",
      "Epoch duration: 2599.5693097114563\n",
      "Epoch: 4\n",
      "Losses Batch 0, train\n",
      " fgw:0.3635226\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3635226\n",
      "Penalty params: tau=0.39811 conn_l=0.00032 val_l=0.00030 euler_l=0.00034 epoch=4 mode=[0 0 0] conn=False val=False euler=False\n"
     ]
    }
   ],
   "source": [
    "molopt, molopt_decoder = main(args, train_data_loader = train_data_loader, val_data_loader = val_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "molopt, molopt_decoder = main(args, train_data_loader = train_data_loader, val_data_loader = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in train_data_loader:\n",
    "    X = (MolGraph(i[0]))\n",
    "    Y = (MolGraph(i[1]))\n",
    "    break\n",
    "    \n",
    "x_embedding = molopt.forward(X)\n",
    "yhat_logits = molopt_decoder.forward(x_embedding, X, Y)\n",
    "yhat_labels = molopt_decoder.discretize_argmax(*yhat_logits)\n",
    "# yhat_labels = molopt_decoder.discretize(*yhat_logits)\n",
    "pred_pack = (yhat_labels, yhat_logits, Y.scope), Y \n",
    "\n",
    "target = Y.get_graph_outputs()\n",
    "symbols_labels, charges_labels, bonds_labels = yhat_labels\n",
    "symbols_logits, charges_logits, bonds_logits = yhat_logits\n",
    "\n",
    "from mol_opt.ot_utils import FGW \n",
    "fgw_loss = FGW(alpha = 0.5)\n",
    "fgw_loss(*pred_pack, tau = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.optim.Adam([torch.autograd.Variable(torch.Tensor([0.]))]).param_groups[0]['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.9 ** (1/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter(\"/home/octav/gitrepos/tum-thesis/mol_opt/dev7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [\n",
    "    \"pointwise10-dev8/train_avg_euler_error/mean\",\n",
    "    \"pointwise10-dev8/train_avg_euler_error/bot_band\",\n",
    "    \"pointwise10-dev8/train_avg_euler_error/top_band\"\n",
    "]\n",
    "# writer.add_custom_scalars_marginchart(tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer.add_scalar(\"plm2/plm2\", 0, 1)\n",
    "# writer.add_scalar(\"plm2/plm2\", 1, 2)\n",
    "# writer.add_scalar(\"plm2/plm2\", 0, 3)\n",
    "# writer.add_scalar(\"plm2/plm2\", 1, 4)\n",
    "tags = [\"plm\", \"plm\", \"plm\"]\n",
    "# layout = {\"plm\" : {\"plm\" : [\"Margin\", tags]}}\n",
    "layout = {\"plm1\" : {\"plm\" : [\"Margin\", tags], \"plm2\": [\"Multiline\", tags]},\n",
    "          \"plm2\": {\"plm\" : [\"Margin\", tags], \"plm2\": [\"Multiline\", tags]}}\n",
    "writer.add_custom_scalars(layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer._get_file_writer().add_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboardX\n",
    "tensorboardX.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX.summary import custom_scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer._get_file_writer().add_summary(custom_scalars(layout), global_step = 3)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_scalars('plm', {'plm0' : 0.0, 'plmt' : 0.1, 'plmb' : -0.1}, 3)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 1\n",
    "symbols_nll, charges_nll, bonds_nll = F.gumbel_softmax(tau = tau, dim=1, logits = symbols_logits), F.gumbel_softmax(tau=tau,dim=1,logits=charges_logits), F.gumbel_softmax(tau=tau,dim=1, logits = bonds_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(symbols_nll.mean(axis = 0).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(molopt.opt0.parameters())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "molopt.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = torch.load('mol_opt/output_pointwise10-onebatch/model_pointwise10-onebatch_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict[\"model\"]['opt0.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "molopt2 = MolOpt(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(molopt2.opt0.parameters())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "molopt2.load_state_dict(model_dict[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(molopt2.opt0.parameters())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "molopt3,args3 = load_model('mol_opt/output_pointwise10-onebatch/model_pointwise10-onebatch_8', MolOpt, args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(molopt3.opt0.parameters())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgw_loss(*pred_pack, tau = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
