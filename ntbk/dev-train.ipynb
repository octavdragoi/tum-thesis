{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/octav/gitrepos/tum-thesis\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if \"ntbk\" in os.getcwd():\n",
    "    os.chdir(\"..\")\n",
    "print(os.getcwd())\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), \"otgnn\"))\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from otgnn.models import GCN\n",
    "from otgnn.graph import MolGraph\n",
    "from otgnn.utils import save_model, load_model\n",
    "\n",
    "from mol_opt.mol_opt import MolOpt\n",
    "from mol_opt.data_mol_opt import MolOptDataset\n",
    "from mol_opt.data_mol_opt import get_loader\n",
    "from mol_opt.arguments import get_args\n",
    "from mol_opt.train_mol_opt import main, get_latest_model\n",
    "from mol_opt.ot_utils import compute_barycenter\n",
    "\n",
    "from rdkit.Chem import MolFromSmiles\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(N_transformer=6, agg_func='sum', batch_norm=False, batch_size=50, conn_lambda_end=30, conn_lambda_epochs_end=14000, conn_lambda_epochs_start=12000, conn_lambda_start=1, conn_penalty_function='capped_logdet', connectivity=False, connectivity_hard=False, cuda=True, device='cuda:0', dim_tangent_space=40, dropout_ffn=0.0, dropout_gcn=0.0, dropout_transformer=0.1, euler_characteristic_penalty=False, euler_lambda_end=20, euler_lambda_epochs_end=14000, euler_lambda_epochs_start=12000, euler_lambda_start=1, ffn_activation='LeakyReLU', init_decoder_model='molemb-base-longer-softmax5_decode', init_model='molemb-base-longer-softmax5', linear_out=False, max_num_atoms=70, model_type='molemb', n_epochs=4000, n_ffn_hidden=100, n_ffn_transformer=100, n_heads_transformer=10, n_hidden=200, n_labels=1, n_layers=5, one_batch_train=True, ot_solver='emd', output_dir='mol_opt/output_dev2/molemb-base-longer-softmax5', pc_hidden=150, penalty_gumbel=False, pred_hidden=150, scale_lambdas=True, sinkhorn_entropy=0.1, sinkhorn_max_it=10000, task='qed', tau_end=0.001, tau_epochs_end=16000, tau_epochs_start=1, tau_start=1, tb_logs_dir='mol_opt/logs_dev2/molemb-base-longer-softmax5', valency=False, valency_hard=False, valency_lambda_end=50, valency_lambda_epochs_end=14000, valency_lambda_epochs_start=12000, valency_lambda_start=10) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_type = \"molemb\"\n",
    "sys.argv = [\"\", \"-cuda\", \"-model_type\", model_type, \"-one_batch_train\"]\n",
    "args = get_args()\n",
    "outdir_suffix = \"dev2\"\n",
    "args.n_epochs = 4000 \n",
    "args.init_model = \"{}-base-longer-softmax5\".format(model_type)\n",
    "# args.init_model = \"{}-test1\".format(model_type)\n",
    "args.init_decoder_model = \"{}_decode\".format(args.init_model)\n",
    "args.output_dir = \"mol_opt/output_{}/{}\".format(outdir_suffix, args.init_model)\n",
    "args.tb_logs_dir = \"mol_opt/logs_{}/{}\".format(outdir_suffix, args.init_model)\n",
    "args.batch_size = 50 \n",
    "\n",
    "args.penalty_gumbel = False \n",
    "\n",
    "args.scale_lambdas = True\n",
    "args.connectivity = False \n",
    "args.valency = False \n",
    "args.euler_characteristic_penalty = False\n",
    "args.conn_lambda_start = 1\n",
    "args.conn_lambda_end = 30\n",
    "args.conn_lambda_epochs_start = 12000\n",
    "args.conn_lambda_epochs_end = 14000\n",
    "args.valency_lambda_start = 10\n",
    "args.valency_lambda_end = 50\n",
    "args.valency_lambda_epochs_start = 12000\n",
    "args.valency_lambda_epochs_end = 14000\n",
    "args.euler_lambda_start = 1\n",
    "args.euler_lambda_end = 20 \n",
    "args.euler_lambda_epochs_start = 12000\n",
    "args.euler_lambda_epochs_end = 14000\n",
    "args.tau_start = 1\n",
    "args.tau_end = 0.001\n",
    "args.tau_epochs_start = 1\n",
    "args.tau_epochs_end = 16000\n",
    "\n",
    "\n",
    "args.conn_penalty_function = \"capped_logdet\" \n",
    "\n",
    "print(args, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_loader = get_loader(\"iclr19-graph2graph/data/qed\", \"train_pairs\", args.batch_size, same_number_atoms = True)\n",
    "train_data_loader = get_loader(\"molgen/data/chembl50\", \"train\", args.batch_size, same_number_atoms = True)\n",
    "# val_data_loader = get_loader(\"iclr19-graph2graph/data/qed\", \"valid\", 36, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found previous model mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_1, epoch 1. Overwriting args.\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/octav/gitrepos/tum-thesis/otgnn/models/gromov_modules.py:394: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  nce_reg = torch.nn.LogSoftmax()(torch.stack(all_nce_dists))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FGW torch.Size([29508, 5]) 7.077011105138808e-05\n",
      "Penalty params: tau=0.99914 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=2 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 2, train\n",
      " fgw:9.4266620\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:9.4266620\n",
      "Measure Epoch 2, train\n",
      " similarity:0.0024464\n",
      " penlog:1.1317933\n",
      "Metrics Epoch 2, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:65.4782609\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:23.0000000\n",
      " batch_invalid_valency_nodes:100.0000000\n",
      " batch_nodes_0degree:23.0000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-23.0000000\n",
      " batch_node_degree:0.0000000\n",
      "Logits [7.925220012664795, 6.284742832183838, 86.83892059326172]\n",
      "Epoch duration: 2.524968385696411\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_2\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/octav/gitrepos/tum-thesis/otgnn/models/gromov_modules.py:394: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  nce_reg = torch.nn.LogSoftmax()(torch.stack(all_nce_dists))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FGW torch.Size([29508, 5]) 0.00036452969652600586\n",
      "Penalty params: tau=0.99871 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=3 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 3, train\n",
      " fgw:52.9514990\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:52.9514990\n",
      "Measure Epoch 3, train\n",
      " similarity:0.0000000\n",
      " penlog:-100.0000000\n",
      "Metrics Epoch 3, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:11.6521739\n",
      " batch_molecular_disconnected_validity:0.0000000\n",
      " batch_connected_components:1.6200000\n",
      " batch_invalid_valency_nodes:59.0434783\n",
      " batch_nodes_0degree:0.6200000\n",
      " batch_nodes_7plus_degree:19.5800000\n",
      " invalid_euler_toofew:0.0000000\n",
      " invalid_euler_toomany:100.0000000\n",
      " avg_euler_error:127.0600000\n",
      " batch_node_degree:26.4608696\n",
      "Logits [19.55836296081543, 14.915124893188477, 104.22269439697266]\n",
      "Epoch duration: 2.141371488571167\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_3\n",
      "Epoch: 4\n",
      "FGW torch.Size([29508, 5]) 7.077011105138808e-05\n",
      "Penalty params: tau=0.99827 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=4 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 4, train\n",
      " fgw:40.6352734\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:40.6352734\n",
      "Measure Epoch 4, train\n",
      " similarity:0.0014105\n",
      " penlog:-2.0883391\n",
      "Metrics Epoch 4, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:28.9565217\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:23.0000000\n",
      " batch_invalid_valency_nodes:100.0000000\n",
      " batch_nodes_0degree:23.0000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-23.0000000\n",
      " batch_node_degree:0.0000000\n",
      "Logits [16.06967544555664, 12.372575759887695, 223.0470428466797]\n",
      "Epoch duration: 1.917607069015503\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_4\n",
      "Epoch: 5\n",
      "FGW torch.Size([29508, 5]) 0.0005948340985924006\n",
      "Penalty params: tau=0.99784 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=5 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 5, train\n",
      " fgw:315.6324219\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:315.6324219\n",
      "Measure Epoch 5, train\n",
      " similarity:0.0000000\n",
      " penlog:-100.0000000\n",
      "Metrics Epoch 5, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:55.3043478\n",
      " batch_molecular_disconnected_validity:0.0000000\n",
      " batch_connected_components:1.0000000\n",
      " batch_invalid_valency_nodes:100.0000000\n",
      " batch_nodes_0degree:0.0000000\n",
      " batch_nodes_7plus_degree:22.8600000\n",
      " invalid_euler_toofew:0.0000000\n",
      " invalid_euler_toomany:100.0000000\n",
      " avg_euler_error:260.5800000\n",
      " batch_node_degree:24.6591304\n",
      "Logits [24.56020736694336, 9.418069839477539, 232.69168090820312]\n",
      "Epoch duration: 3.349971055984497\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_5\n",
      "Epoch: 6\n",
      "FGW torch.Size([29508, 5]) 7.402243500109762e-05\n",
      "Penalty params: tau=0.99741 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=6 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 6, train\n",
      " fgw:11.9934131\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:11.9934131\n",
      "Measure Epoch 6, train\n",
      " similarity:0.0223244\n",
      " penlog:-0.8742389\n",
      "Metrics Epoch 6, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:85.4782609\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:20.7800000\n",
      " batch_invalid_valency_nodes:86.8695652\n",
      " batch_nodes_0degree:19.9400000\n",
      " batch_nodes_7plus_degree:0.0200000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-20.6600000\n",
      " batch_node_degree:0.2034783\n",
      "Logits [16.373985290527344, 7.574342727661133, 154.39744567871094]\n",
      "Epoch duration: 1.7999837398529053\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_6\n",
      "Epoch: 7\n",
      "FGW torch.Size([29508, 5]) 7.846381049603224e-05\n",
      "Penalty params: tau=0.99698 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=7 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 7, train\n",
      " fgw:15.5651794\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:15.5651794\n",
      "Measure Epoch 7, train\n",
      " similarity:0.0221538\n",
      " penlog:2.7508215\n",
      "Metrics Epoch 7, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:85.8260870\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:19.9600000\n",
      " batch_invalid_valency_nodes:82.9565217\n",
      " batch_nodes_0degree:19.0800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-18.5600000\n",
      " batch_node_degree:0.3860870\n",
      "Logits [27.039613723754883, 11.488727569580078, 264.9526672363281]\n",
      "Epoch duration: 1.9052259922027588\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_7\n",
      "Epoch: 8\n",
      "FGW torch.Size([29508, 5]) 0.0005948110483586788\n",
      "Penalty params: tau=0.99655 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=8 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 8, train\n",
      " fgw:61.4459766\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:61.4459766\n",
      "Measure Epoch 8, train\n",
      " similarity:0.0000000\n",
      " penlog:-100.0000000\n",
      "Metrics Epoch 8, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:88.2608696\n",
      " batch_molecular_disconnected_validity:0.0000000\n",
      " batch_connected_components:1.0000000\n",
      " batch_invalid_valency_nodes:100.0000000\n",
      " batch_nodes_0degree:0.0000000\n",
      " batch_nodes_7plus_degree:22.8600000\n",
      " invalid_euler_toofew:0.0000000\n",
      " invalid_euler_toomany:100.0000000\n",
      " avg_euler_error:260.5400000\n",
      " batch_node_degree:24.6556522\n",
      "Logits [32.40397644042969, 12.21017074584961, 278.9707336425781]\n",
      "Epoch duration: 3.442906618118286\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_8\n",
      "Epoch: 9\n",
      "FGW torch.Size([29508, 5]) 7.099990762071684e-05\n",
      "Penalty params: tau=0.99612 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=9 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 9, train\n",
      " fgw:8.0277991\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:8.0277991\n",
      "Measure Epoch 9, train\n",
      " similarity:0.0025413\n",
      " penlog:-1.9892317\n",
      "Metrics Epoch 9, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:80.8695652\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:22.9600000\n",
      " batch_invalid_valency_nodes:99.7391304\n",
      " batch_nodes_0degree:22.9400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-22.9600000\n",
      " batch_node_degree:0.0069565\n",
      "Logits [24.08658790588379, 8.722267150878906, 177.47743225097656]\n",
      "Epoch duration: 1.8513195514678955\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_9\n",
      "Epoch: 10\n",
      "FGW torch.Size([29508, 5]) 0.00019295061065349728\n",
      "Penalty params: tau=0.99569 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=10 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 10, train\n",
      " fgw:9.0285242\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:9.0285242\n",
      "Measure Epoch 10, train\n",
      " similarity:0.0000000\n",
      " penlog:-98.1143602\n",
      "Metrics Epoch 10, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:76.1739130\n",
      " batch_molecular_disconnected_validity:2.0000000\n",
      " batch_connected_components:9.1400000\n",
      " batch_invalid_valency_nodes:86.0869565\n",
      " batch_nodes_0degree:8.1600000\n",
      " batch_nodes_7plus_degree:11.0200000\n",
      " invalid_euler_toofew:6.0000000\n",
      " invalid_euler_toomany:90.0000000\n",
      " avg_euler_error:31.8400000\n",
      " batch_node_degree:9.5373913\n",
      "Logits [14.57706069946289, 5.145698070526123, 99.80976104736328]\n",
      "Epoch duration: 1.8621668815612793\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_10\n",
      "Epoch: 11\n",
      "FGW torch.Size([29508, 5]) 0.0002932144852820784\n",
      "Penalty params: tau=0.99526 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=11 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 11, train\n",
      " fgw:6.4711487\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:6.4711487\n",
      "Measure Epoch 11, train\n",
      " similarity:0.0000000\n",
      " penlog:-98.0879770\n",
      "Metrics Epoch 11, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:83.5652174\n",
      " batch_molecular_disconnected_validity:2.0000000\n",
      " batch_connected_components:5.6400000\n",
      " batch_invalid_valency_nodes:95.7391304\n",
      " batch_nodes_0degree:4.6600000\n",
      " batch_nodes_7plus_degree:16.9000000\n",
      " invalid_euler_toofew:2.0000000\n",
      " invalid_euler_toomany:98.0000000\n",
      " avg_euler_error:77.8200000\n",
      " batch_node_degree:17.5339130\n",
      "Logits [8.430685043334961, 2.659543514251709, 74.44709014892578]\n",
      "Epoch duration: 2.041626453399658\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12\n",
      "FGW torch.Size([29508, 5]) 7.079311762936413e-05\n",
      "Penalty params: tau=0.99483 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=12 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 12, train\n",
      " fgw:3.6236429\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:3.6236429\n",
      "Measure Epoch 12, train\n",
      " similarity:0.0032896\n",
      " penlog:3.1101020\n",
      "Metrics Epoch 12, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:82.4347826\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:23.0000000\n",
      " batch_invalid_valency_nodes:100.0000000\n",
      " batch_nodes_0degree:23.0000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-23.0000000\n",
      " batch_node_degree:0.0000000\n",
      "Logits [6.490198612213135, 2.0171403884887695, 67.85481262207031]\n",
      "Epoch duration: 1.8558290004730225\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_12\n",
      "Epoch: 13\n",
      "FGW torch.Size([29508, 5]) 7.077318150550127e-05\n",
      "Penalty params: tau=0.99440 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=13 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 13, train\n",
      " fgw:3.0142670\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:3.0142670\n",
      "Measure Epoch 13, train\n",
      " similarity:0.0019530\n",
      " penlog:2.2653857\n",
      "Metrics Epoch 13, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:83.6521739\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:23.0000000\n",
      " batch_invalid_valency_nodes:100.0000000\n",
      " batch_nodes_0degree:23.0000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-23.0000000\n",
      " batch_node_degree:0.0000000\n",
      "Logits [5.54044771194458, 1.562188982963562, 61.25303649902344]\n",
      "Epoch duration: 1.8805384635925293\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_13\n",
      "Epoch: 14\n",
      "FGW torch.Size([29508, 5]) 0.0001699253189144656\n",
      "Penalty params: tau=0.99397 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=14 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 14, train\n",
      " fgw:7.7805457\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:7.7805457\n",
      "Measure Epoch 14, train\n",
      " similarity:0.0006667\n",
      " penlog:-96.2579758\n",
      "Metrics Epoch 14, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:40.3478261\n",
      " batch_molecular_disconnected_validity:4.0000000\n",
      " batch_connected_components:9.9000000\n",
      " batch_invalid_valency_nodes:88.0000000\n",
      " batch_nodes_0degree:8.9200000\n",
      " batch_nodes_7plus_degree:9.2600000\n",
      " invalid_euler_toofew:26.0000000\n",
      " invalid_euler_toomany:62.0000000\n",
      " avg_euler_error:30.7800000\n",
      " batch_node_degree:9.3530435\n",
      "Logits [13.393373489379883, 2.0622096061706543, 88.61975860595703]\n",
      "Epoch duration: 2.005371332168579\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_14\n",
      "Epoch: 15\n",
      "FGW torch.Size([29508, 5]) 7.12567925802432e-05\n",
      "Penalty params: tau=0.99354 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=15 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 15, train\n",
      " fgw:2.6482397\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:2.6482397\n",
      "Measure Epoch 15, train\n",
      " similarity:0.0020097\n",
      " penlog:0.5488041\n",
      "Metrics Epoch 15, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:83.5652174\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:23.0000000\n",
      " batch_invalid_valency_nodes:100.0000000\n",
      " batch_nodes_0degree:23.0000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-23.0000000\n",
      " batch_node_degree:0.0000000\n",
      "Logits [9.663182258605957, 2.143848419189453, 63.07295608520508]\n",
      "Epoch duration: 1.8895211219787598\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_15\n",
      "Epoch: 16\n",
      "FGW torch.Size([29508, 5]) 7.077074405970052e-05\n",
      "Penalty params: tau=0.99312 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=16 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 16, train\n",
      " fgw:5.4661389\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:5.4661389\n",
      "Measure Epoch 16, train\n",
      " similarity:0.0021001\n",
      " penlog:4.9156676\n",
      "Metrics Epoch 16, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:76.0000000\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:23.0000000\n",
      " batch_invalid_valency_nodes:100.0000000\n",
      " batch_nodes_0degree:23.0000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-23.0000000\n",
      " batch_node_degree:0.0000000\n",
      "Logits [9.913155555725098, 2.3559463024139404, 85.68862915039062]\n",
      "Epoch duration: 1.9170668125152588\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_16\n",
      "Epoch: 17\n",
      "FGW torch.Size([29508, 5]) 7.540792284999043e-05\n",
      "Penalty params: tau=0.99269 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=17 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 17, train\n",
      " fgw:2.1505972\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:2.1505972\n",
      "Measure Epoch 17, train\n",
      " similarity:0.0067651\n",
      " penlog:-3.2022068\n",
      "Metrics Epoch 17, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:81.9130435\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:22.1000000\n",
      " batch_invalid_valency_nodes:94.6956522\n",
      " batch_nodes_0degree:21.7200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-22.1000000\n",
      " batch_node_degree:0.0782609\n",
      "Logits [9.448665618896484, 2.2018496990203857, 75.75420379638672]\n",
      "Epoch duration: 1.9894721508026123\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_17\n",
      "Epoch: 18\n",
      "FGW torch.Size([29508, 5]) 0.0001925082760863006\n",
      "Penalty params: tau=0.99226 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=18 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 18, train\n",
      " fgw:3.2813867\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:3.2813867\n",
      "Measure Epoch 18, train\n",
      " similarity:0.0030000\n",
      " penlog:-98.0446173\n",
      "Metrics Epoch 18, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:82.6956522\n",
      " batch_molecular_disconnected_validity:2.0000000\n",
      " batch_connected_components:3.5600000\n",
      " batch_invalid_valency_nodes:69.1304348\n",
      " batch_nodes_0degree:2.5600000\n",
      " batch_nodes_7plus_degree:10.6800000\n",
      " invalid_euler_toofew:6.0000000\n",
      " invalid_euler_toomany:92.0000000\n",
      " avg_euler_error:48.8800000\n",
      " batch_node_degree:6.2504348\n",
      "Logits [11.663960456848145, 2.0541322231292725, 77.10768127441406]\n",
      "Epoch duration: 1.9723567962646484\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_18\n",
      "Epoch: 19\n",
      "FGW torch.Size([29508, 5]) 0.00011910838657058775\n",
      "Penalty params: tau=0.99183 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=19 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 19, train\n",
      " fgw:3.1546960\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:3.1546960\n",
      "Measure Epoch 19, train\n",
      " similarity:0.0045933\n",
      " penlog:-80.6422672\n",
      "Metrics Epoch 19, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:86.1739130\n",
      " batch_molecular_disconnected_validity:20.0000000\n",
      " batch_connected_components:9.4800000\n",
      " batch_invalid_valency_nodes:62.3478261\n",
      " batch_nodes_0degree:8.4600000\n",
      " batch_nodes_7plus_degree:1.7800000\n",
      " invalid_euler_toofew:22.0000000\n",
      " invalid_euler_toomany:44.0000000\n",
      " avg_euler_error:4.9000000\n",
      " batch_node_degree:2.4260870\n",
      "Logits [12.839884757995605, 2.0187931060791016, 80.7461929321289]\n",
      "Epoch duration: 1.9274091720581055\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_19\n",
      "Epoch: 20\n",
      "FGW torch.Size([29508, 5]) 7.734356768196449e-05\n",
      "Penalty params: tau=0.99140 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=20 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 20, train\n",
      " fgw:2.1873381\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:2.1873381\n",
      "Measure Epoch 20, train\n",
      " similarity:0.0177358\n",
      " penlog:0.4245233\n",
      "Metrics Epoch 20, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:87.0434783\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:21.2000000\n",
      " batch_invalid_valency_nodes:88.5217391\n",
      " batch_nodes_0degree:20.3600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-21.2000000\n",
      " batch_node_degree:0.1565217\n",
      "Logits [11.413536071777344, 1.9274017810821533, 75.63685607910156]\n",
      "Epoch duration: 1.8894660472869873\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21\n",
      "FGW torch.Size([29508, 5]) 7.420186011586338e-05\n",
      "Penalty params: tau=0.99097 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=21 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 21, train\n",
      " fgw:1.4717282\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:1.4717282\n",
      "Measure Epoch 21, train\n",
      " similarity:0.0024824\n",
      " penlog:1.3155489\n",
      "Metrics Epoch 21, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:87.6521739\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:23.0000000\n",
      " batch_invalid_valency_nodes:100.0000000\n",
      " batch_nodes_0degree:23.0000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-23.0000000\n",
      " batch_node_degree:0.0000000\n",
      "Logits [9.249890327453613, 1.7765036821365356, 73.54931640625]\n",
      "Epoch duration: 1.8858890533447266\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_21\n",
      "Epoch: 22\n",
      "FGW torch.Size([29508, 5]) 7.942404772620648e-05\n",
      "Penalty params: tau=0.99055 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=22 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 22, train\n",
      " fgw:2.0780290\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:2.0780290\n",
      "Measure Epoch 22, train\n",
      " similarity:0.0272019\n",
      " penlog:2.5225612\n",
      "Metrics Epoch 22, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:89.0434783\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:20.8600000\n",
      " batch_invalid_valency_nodes:84.0869565\n",
      " batch_nodes_0degree:19.3400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-20.8000000\n",
      " batch_node_degree:0.1913043\n",
      "Logits [8.057873725891113, 1.6195087432861328, 78.74646759033203]\n",
      "Epoch duration: 1.972372055053711\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_22\n",
      "Epoch: 23\n",
      "FGW torch.Size([29508, 5]) 9.97402094071731e-05\n",
      "Penalty params: tau=0.99012 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=23 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 23, train\n",
      " fgw:1.5827475\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:1.5827475\n",
      "Measure Epoch 23, train\n",
      " similarity:0.0185663\n",
      " penlog:-56.0599559\n",
      "Metrics Epoch 23, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:88.0000000\n",
      " batch_molecular_disconnected_validity:44.0000000\n",
      " batch_connected_components:12.8600000\n",
      " batch_invalid_valency_nodes:54.6086957\n",
      " batch_nodes_0degree:11.5000000\n",
      " batch_nodes_7plus_degree:0.2400000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-9.5400000\n",
      " batch_node_degree:1.2504348\n",
      "Logits [7.288839340209961, 1.4278637170791626, 74.31868743896484]\n",
      "Epoch duration: 1.931419849395752\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_23\n",
      "Epoch: 24\n",
      "FGW torch.Size([29508, 5]) 0.0001069888094207272\n",
      "Penalty params: tau=0.98969 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=24 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 24, train\n",
      " fgw:0.8403725\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.8403725\n",
      "Measure Epoch 24, train\n",
      " similarity:0.0031237\n",
      " penlog:-92.2045184\n",
      "Metrics Epoch 24, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:87.4782609\n",
      " batch_molecular_disconnected_validity:8.0000000\n",
      " batch_connected_components:7.8600000\n",
      " batch_invalid_valency_nodes:42.8695652\n",
      " batch_nodes_0degree:6.0800000\n",
      " batch_nodes_7plus_degree:0.6400000\n",
      " invalid_euler_toofew:66.0000000\n",
      " invalid_euler_toomany:6.0000000\n",
      " avg_euler_error:-2.4400000\n",
      " batch_node_degree:2.0260870\n",
      "Logits [6.985165119171143, 1.2741129398345947, 64.64946746826172]\n",
      "Epoch duration: 1.8972499370574951\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_24\n",
      "Epoch: 25\n",
      "FGW torch.Size([29508, 5]) 9.492389654042199e-05\n",
      "Penalty params: tau=0.98926 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=25 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 25, train\n",
      " fgw:1.0661018\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:1.0661018\n",
      "Measure Epoch 25, train\n",
      " similarity:0.0015915\n",
      " penlog:-78.6440384\n",
      "Metrics Epoch 25, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:85.5652174\n",
      " batch_molecular_disconnected_validity:22.0000000\n",
      " batch_connected_components:14.4400000\n",
      " batch_invalid_valency_nodes:64.1739130\n",
      " batch_nodes_0degree:12.8400000\n",
      " batch_nodes_7plus_degree:0.7600000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-14.0400000\n",
      " batch_node_degree:1.2747826\n",
      "Logits [7.3132829666137695, 1.1826465129852295, 61.66412353515625]\n",
      "Epoch duration: 1.8746423721313477\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_25\n",
      "Epoch: 26\n",
      "FGW torch.Size([29508, 5]) 0.00010359346924815327\n",
      "Penalty params: tau=0.98884 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=26 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 26, train\n",
      " fgw:1.1701952\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:1.1701952\n",
      "Measure Epoch 26, train\n",
      " similarity:0.0000000\n",
      " penlog:-92.2701151\n",
      "Metrics Epoch 26, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:89.3043478\n",
      " batch_molecular_disconnected_validity:8.0000000\n",
      " batch_connected_components:13.3200000\n",
      " batch_invalid_valency_nodes:67.3913043\n",
      " batch_nodes_0degree:12.2400000\n",
      " batch_nodes_7plus_degree:1.7000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.8800000\n",
      " batch_node_degree:1.9095652\n",
      "Logits [7.343574523925781, 1.1399803161621094, 60.80978012084961]\n",
      "Epoch duration: 1.9971041679382324\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_26\n",
      "Epoch: 27\n",
      "FGW torch.Size([29508, 5]) 0.00014600875147152692\n",
      "Penalty params: tau=0.98841 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=27 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 27, train\n",
      " fgw:0.8493985\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.8493985\n",
      "Measure Epoch 27, train\n",
      " similarity:0.0004878\n",
      " penlog:-94.3260103\n",
      "Metrics Epoch 27, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:88.6956522\n",
      " batch_molecular_disconnected_validity:6.0000000\n",
      " batch_connected_components:6.1800000\n",
      " batch_invalid_valency_nodes:67.4782609\n",
      " batch_nodes_0degree:5.0400000\n",
      " batch_nodes_7plus_degree:5.8400000\n",
      " invalid_euler_toofew:32.0000000\n",
      " invalid_euler_toomany:54.0000000\n",
      " avg_euler_error:7.1600000\n",
      " batch_node_degree:4.3408696\n",
      "Logits [6.802750587463379, 1.1339125633239746, 59.124942779541016]\n",
      "Epoch duration: 1.9200549125671387\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_27\n",
      "Epoch: 28\n",
      "FGW torch.Size([29508, 5]) 0.00015272217569872737\n",
      "Penalty params: tau=0.98798 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=28 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 28, train\n",
      " fgw:0.7635798\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.7635798\n",
      "Measure Epoch 28, train\n",
      " similarity:0.0036842\n",
      " penlog:-96.0924282\n",
      "Metrics Epoch 28, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:87.8260870\n",
      " batch_molecular_disconnected_validity:4.0000000\n",
      " batch_connected_components:4.2800000\n",
      " batch_invalid_valency_nodes:70.5217391\n",
      " batch_nodes_0degree:3.2800000\n",
      " batch_nodes_7plus_degree:6.9800000\n",
      " invalid_euler_toofew:12.0000000\n",
      " invalid_euler_toomany:82.0000000\n",
      " avg_euler_error:23.2200000\n",
      " batch_node_degree:5.3165217\n",
      "Logits [6.6597819328308105, 1.1500179767608643, 61.417274475097656]\n",
      "Epoch duration: 1.8986680507659912\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_28\n",
      "Epoch: 29\n",
      "FGW torch.Size([29508, 5]) 0.00011546166206244379\n",
      "Penalty params: tau=0.98756 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=29 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 29, train\n",
      " fgw:0.8475027\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.8475027\n",
      "Measure Epoch 29, train\n",
      " similarity:0.0035304\n",
      " penlog:-84.3443415\n",
      "Metrics Epoch 29, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:87.6521739\n",
      " batch_molecular_disconnected_validity:16.0000000\n",
      " batch_connected_components:10.1600000\n",
      " batch_invalid_valency_nodes:57.7391304\n",
      " batch_nodes_0degree:8.8200000\n",
      " batch_nodes_7plus_degree:1.5000000\n",
      " invalid_euler_toofew:62.0000000\n",
      " invalid_euler_toomany:18.0000000\n",
      " avg_euler_error:-2.0000000\n",
      " batch_node_degree:2.2400000\n",
      "Logits [7.0483598709106445, 1.1566911935806274, 65.51848602294922]\n",
      "Epoch duration: 1.9351649284362793\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30\n",
      "FGW torch.Size([29508, 5]) 0.00010331815428799018\n",
      "Penalty params: tau=0.98713 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=30 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 30, train\n",
      " fgw:0.7437271\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.7437271\n",
      "Measure Epoch 30, train\n",
      " similarity:0.0040719\n",
      " penlog:-76.3510226\n",
      "Metrics Epoch 30, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:89.9130435\n",
      " batch_molecular_disconnected_validity:24.0000000\n",
      " batch_connected_components:16.5400000\n",
      " batch_invalid_valency_nodes:68.6086957\n",
      " batch_nodes_0degree:14.7600000\n",
      " batch_nodes_7plus_degree:0.7200000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-15.8800000\n",
      " batch_node_degree:0.8365217\n",
      "Logits [7.17760705947876, 1.2060223817825317, 63.67353057861328]\n",
      "Epoch duration: 1.870713710784912\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_30\n",
      "Epoch: 31\n",
      "FGW torch.Size([29508, 5]) 0.00011456717038527131\n",
      "Penalty params: tau=0.98670 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=31 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 31, train\n",
      " fgw:0.6007223\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.6007223\n",
      "Measure Epoch 31, train\n",
      " similarity:0.0277617\n",
      " penlog:-8.5318344\n",
      "Metrics Epoch 31, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:89.1304348\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:16.9000000\n",
      " batch_invalid_valency_nodes:66.8695652\n",
      " batch_nodes_0degree:15.0200000\n",
      " batch_nodes_7plus_degree:0.1200000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-16.0400000\n",
      " batch_node_degree:0.6817391\n",
      "Logits [7.233397006988525, 1.3112173080444336, 58.313480377197266]\n",
      "Epoch duration: 1.9564785957336426\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_31\n",
      "Epoch: 32\n",
      "FGW torch.Size([29508, 5]) 0.00011500117398099974\n",
      "Penalty params: tau=0.98628 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=32 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 32, train\n",
      " fgw:0.6542460\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.6542460\n",
      "Measure Epoch 32, train\n",
      " similarity:0.0099763\n",
      " penlog:-28.8904109\n",
      "Metrics Epoch 32, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:86.9565217\n",
      " batch_molecular_disconnected_validity:70.0000000\n",
      " batch_connected_components:17.5000000\n",
      " batch_invalid_valency_nodes:71.3913043\n",
      " batch_nodes_0degree:15.8400000\n",
      " batch_nodes_7plus_degree:0.1600000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-17.1400000\n",
      " batch_node_degree:0.7808696\n",
      "Logits [7.59174108505249, 1.4073512554168701, 55.19557571411133]\n",
      "Epoch duration: 1.8418948650360107\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_32\n",
      "Epoch: 33\n",
      "FGW torch.Size([29508, 5]) 9.83404679573141e-05\n",
      "Penalty params: tau=0.98585 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=33 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 33, train\n",
      " fgw:0.6902895\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.6902895\n",
      "Measure Epoch 33, train\n",
      " similarity:0.0087120\n",
      " penlog:-1.5589100\n",
      "Metrics Epoch 33, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:86.8695652\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:22.0800000\n",
      " batch_invalid_valency_nodes:93.6521739\n",
      " batch_nodes_0degree:21.5000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-22.0800000\n",
      " batch_node_degree:0.1060870\n",
      "Logits [8.002432823181152, 1.440767765045166, 54.41354751586914]\n",
      "Epoch duration: 2.000826120376587\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_33\n",
      "Epoch: 34\n",
      "FGW torch.Size([29508, 5]) 8.901202818378806e-05\n",
      "Penalty params: tau=0.98543 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=34 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 34, train\n",
      " fgw:0.6347003\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.6347003\n",
      "Measure Epoch 34, train\n",
      " similarity:0.0085534\n",
      " penlog:-0.9389929\n",
      "Metrics Epoch 34, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:90.2608696\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:22.2400000\n",
      " batch_invalid_valency_nodes:94.6086957\n",
      " batch_nodes_0degree:21.7400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-22.2400000\n",
      " batch_node_degree:0.0730435\n",
      "Logits [8.175108909606934, 1.4116718769073486, 54.360347747802734]\n",
      "Epoch duration: 2.050921678543091\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_34\n",
      "Epoch: 35\n",
      "FGW torch.Size([29508, 5]) 9.431334910914302e-05\n",
      "Penalty params: tau=0.98500 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=35 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 35, train\n",
      " fgw:0.5667194\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.5667194\n",
      "Measure Epoch 35, train\n",
      " similarity:0.0166848\n",
      " penlog:-11.9707358\n",
      "Metrics Epoch 35, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:90.1739130\n",
      " batch_molecular_disconnected_validity:88.0000000\n",
      " batch_connected_components:20.3000000\n",
      " batch_invalid_valency_nodes:84.8695652\n",
      " batch_nodes_0degree:19.3000000\n",
      " batch_nodes_7plus_degree:0.0200000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-20.0400000\n",
      " batch_node_degree:0.2643478\n",
      "Logits [8.087087631225586, 1.3612996339797974, 54.64302444458008]\n",
      "Epoch duration: 1.8933401107788086\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_35\n",
      "Epoch: 36\n",
      "FGW torch.Size([29508, 5]) 0.00011601619189605117\n",
      "Penalty params: tau=0.98458 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=36 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 36, train\n",
      " fgw:0.5773106\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.5773106\n",
      "Measure Epoch 36, train\n",
      " similarity:0.0069540\n",
      " penlog:-62.4328528\n",
      "Metrics Epoch 36, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:89.1304348\n",
      " batch_molecular_disconnected_validity:38.0000000\n",
      " batch_connected_components:13.2000000\n",
      " batch_invalid_valency_nodes:61.2173913\n",
      " batch_nodes_0degree:12.0800000\n",
      " batch_nodes_7plus_degree:0.8200000\n",
      " invalid_euler_toofew:86.0000000\n",
      " invalid_euler_toomany:6.0000000\n",
      " avg_euler_error:-8.8200000\n",
      " batch_node_degree:1.2330435\n",
      "Logits [7.8753228187561035, 1.32811439037323, 54.87157440185547]\n",
      "Epoch duration: 1.8241462707519531\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_36\n",
      "Epoch: 37\n",
      "FGW torch.Size([29508, 5]) 0.00013170461170375347\n",
      "Penalty params: tau=0.98415 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=37 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 37, train\n",
      " fgw:0.6084859\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.6084859\n",
      "Measure Epoch 37, train\n",
      " similarity:0.0105335\n",
      " penlog:-62.4919970\n",
      "Metrics Epoch 37, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:89.4782609\n",
      " batch_molecular_disconnected_validity:38.0000000\n",
      " batch_connected_components:10.4400000\n",
      " batch_invalid_valency_nodes:52.7826087\n",
      " batch_nodes_0degree:9.2600000\n",
      " batch_nodes_7plus_degree:1.2600000\n",
      " invalid_euler_toofew:70.0000000\n",
      " invalid_euler_toomany:16.0000000\n",
      " avg_euler_error:-1.2600000\n",
      " batch_node_degree:1.8904348\n",
      "Logits [7.784124851226807, 1.304495096206665, 54.64613723754883]\n",
      "Epoch duration: 1.955934762954712\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_37\n",
      "Epoch: 38\n",
      "FGW torch.Size([29508, 5]) 0.00011462326074251905\n",
      "Penalty params: tau=0.98373 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=38 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 38, train\n",
      " fgw:0.5539538\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.5539538\n",
      "Measure Epoch 38, train\n",
      " similarity:0.0075262\n",
      " penlog:-60.5557634\n",
      "Metrics Epoch 38, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:88.7826087\n",
      " batch_molecular_disconnected_validity:40.0000000\n",
      " batch_connected_components:13.9400000\n",
      " batch_invalid_valency_nodes:62.0869565\n",
      " batch_nodes_0degree:12.9200000\n",
      " batch_nodes_7plus_degree:0.2600000\n",
      " invalid_euler_toofew:92.0000000\n",
      " invalid_euler_toomany:2.0000000\n",
      " avg_euler_error:-11.4000000\n",
      " batch_node_degree:1.0156522\n",
      "Logits [7.957429885864258, 1.281556248664856, 53.86497116088867]\n",
      "Epoch duration: 1.8787274360656738\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39\n",
      "FGW torch.Size([29508, 5]) 0.00010209332685917616\n",
      "Penalty params: tau=0.98330 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=39 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 39, train\n",
      " fgw:0.5159024\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.5159024\n",
      "Measure Epoch 39, train\n",
      " similarity:0.0183374\n",
      " penlog:-1.9066284\n",
      "Metrics Epoch 39, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:89.6521739\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:20.7400000\n",
      " batch_invalid_valency_nodes:84.0000000\n",
      " batch_nodes_0degree:19.2800000\n",
      " batch_nodes_7plus_degree:0.0200000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-20.6600000\n",
      " batch_node_degree:0.2278261\n",
      "Logits [8.22976016998291, 1.2857766151428223, 52.355342864990234]\n",
      "Epoch duration: 1.8846335411071777\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_39\n",
      "Epoch: 40\n",
      "FGW torch.Size([29508, 5]) 0.0001011644781101495\n",
      "Penalty params: tau=0.98288 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=40 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 40, train\n",
      " fgw:0.5056454\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.5056454\n",
      "Measure Epoch 40, train\n",
      " similarity:0.0055419\n",
      " penlog:-3.1718423\n",
      "Metrics Epoch 40, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:90.0000000\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:21.8400000\n",
      " batch_invalid_valency_nodes:91.9130435\n",
      " batch_nodes_0degree:21.0600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-21.8400000\n",
      " batch_node_degree:0.1686957\n",
      "Logits [8.456326484680176, 1.3200145959854126, 50.78036880493164]\n",
      "Epoch duration: 2.01011061668396\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_40\n",
      "Epoch: 41\n",
      "FGW torch.Size([29508, 5]) 0.00010725607717176899\n",
      "Penalty params: tau=0.98245 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=41 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 41, train\n",
      " fgw:0.5267741\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.5267741\n",
      "Measure Epoch 41, train\n",
      " similarity:0.0036735\n",
      " penlog:-7.2174986\n",
      "Metrics Epoch 41, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:89.7391304\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:21.7800000\n",
      " batch_invalid_valency_nodes:91.8260870\n",
      " batch_nodes_0degree:21.0000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-21.7800000\n",
      " batch_node_degree:0.1982609\n",
      "Logits [8.626331329345703, 1.3506112098693848, 49.883506774902344]\n",
      "Epoch duration: 1.888645887374878\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_41\n",
      "Epoch: 42\n",
      "FGW torch.Size([29508, 5]) 0.00011191995145054534\n",
      "Penalty params: tau=0.98203 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=42 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 42, train\n",
      " fgw:0.5216140\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.5216140\n",
      "Measure Epoch 42, train\n",
      " similarity:0.0067820\n",
      " penlog:-11.3828965\n",
      "Metrics Epoch 42, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:89.6521739\n",
      " batch_molecular_disconnected_validity:88.0000000\n",
      " batch_connected_components:20.3400000\n",
      " batch_invalid_valency_nodes:83.8260870\n",
      " batch_nodes_0degree:19.0000000\n",
      " batch_nodes_7plus_degree:0.0800000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-20.3000000\n",
      " batch_node_degree:0.4208696\n",
      "Logits [8.723806381225586, 1.3403431177139282, 49.632347106933594]\n",
      "Epoch duration: 1.9317457675933838\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_42\n",
      "Epoch: 43\n",
      "FGW torch.Size([29508, 5]) 0.00011225523485336453\n",
      "Penalty params: tau=0.98161 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=43 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 43, train\n",
      " fgw:0.4964825\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.4964825\n",
      "Measure Epoch 43, train\n",
      " similarity:0.0211350\n",
      " penlog:-7.4788904\n",
      "Metrics Epoch 43, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:90.2608696\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:19.4600000\n",
      " batch_invalid_valency_nodes:78.8695652\n",
      " batch_nodes_0degree:17.9000000\n",
      " batch_nodes_7plus_degree:0.0200000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-19.4200000\n",
      " batch_node_degree:0.4469565\n",
      "Logits [8.784137725830078, 1.2923318147659302, 50.10075759887695]\n",
      "Epoch duration: 1.9652316570281982\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_43\n",
      "Epoch: 44\n",
      "FGW torch.Size([29508, 5]) 0.00010921595094259828\n",
      "Penalty params: tau=0.98118 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=44 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 44, train\n",
      " fgw:0.4884565\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.4884565\n",
      "Measure Epoch 44, train\n",
      " similarity:0.0304274\n",
      " penlog:-7.6438651\n",
      "Metrics Epoch 44, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:90.3478261\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:17.3600000\n",
      " batch_invalid_valency_nodes:65.4782609\n",
      " batch_nodes_0degree:14.9200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-17.0800000\n",
      " batch_node_degree:0.6017391\n",
      "Logits [8.854557037353516, 1.2438859939575195, 51.07630920410156]\n",
      "Epoch duration: 1.9765381813049316\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_44\n",
      "Epoch: 45\n",
      "FGW torch.Size([29508, 5]) 0.00011309102410450578\n",
      "Penalty params: tau=0.98076 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=45 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 45, train\n",
      " fgw:0.4963551\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.4963551\n",
      "Measure Epoch 45, train\n",
      " similarity:0.0360868\n",
      " penlog:-7.8449031\n",
      "Metrics Epoch 45, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:90.6086957\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:15.8200000\n",
      " batch_invalid_valency_nodes:57.7391304\n",
      " batch_nodes_0degree:13.1000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-15.1000000\n",
      " batch_node_degree:0.7565217\n",
      "Logits [8.897445678710938, 1.2277960777282715, 51.58803176879883]\n",
      "Epoch duration: 1.8676085472106934\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_45\n",
      "Epoch: 46\n",
      "FGW torch.Size([29508, 5]) 0.00012009621423203498\n",
      "Penalty params: tau=0.98033 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=46 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 46, train\n",
      " fgw:0.4878964\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.4878964\n",
      "Measure Epoch 46, train\n",
      " similarity:0.0141096\n",
      " penlog:-43.8124427\n",
      "Metrics Epoch 46, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:90.6086957\n",
      " batch_molecular_disconnected_validity:56.0000000\n",
      " batch_connected_components:13.5400000\n",
      " batch_invalid_valency_nodes:50.7826087\n",
      " batch_nodes_0degree:10.9200000\n",
      " batch_nodes_7plus_degree:0.0200000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.4800000\n",
      " batch_node_degree:0.9791304\n",
      "Logits [8.955571174621582, 1.256604552268982, 51.002532958984375]\n",
      "Epoch duration: 1.9635961055755615\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_46\n",
      "Epoch: 47\n",
      "FGW torch.Size([29508, 5]) 0.00011827360867755488\n",
      "Penalty params: tau=0.97991 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=47 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 47, train\n",
      " fgw:0.4727797\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.4727797\n",
      "Measure Epoch 47, train\n",
      " similarity:0.0324131\n",
      " penlog:-15.5776660\n",
      "Metrics Epoch 47, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:90.8695652\n",
      " batch_molecular_disconnected_validity:84.0000000\n",
      " batch_connected_components:14.7400000\n",
      " batch_invalid_valency_nodes:53.6521739\n",
      " batch_nodes_0degree:11.9600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.9600000\n",
      " batch_node_degree:0.8365217\n",
      "Logits [9.162137985229492, 1.3026375770568848, 50.172760009765625]\n",
      "Epoch duration: 1.9368963241577148\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48\n",
      "FGW torch.Size([29508, 5]) 0.00010938785271719098\n",
      "Penalty params: tau=0.97949 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=48 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 48, train\n",
      " fgw:0.4640410\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.4640410\n",
      "Measure Epoch 48, train\n",
      " similarity:0.0361200\n",
      " penlog:-5.2987565\n",
      "Metrics Epoch 48, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:90.9565217\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:17.4200000\n",
      " batch_invalid_valency_nodes:60.5217391\n",
      " batch_nodes_0degree:13.8200000\n",
      " batch_nodes_7plus_degree:0.0200000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-17.1800000\n",
      " batch_node_degree:0.5582609\n",
      "Logits [9.503725051879883, 1.3392672538757324, 49.68512725830078]\n",
      "Epoch duration: 2.099091053009033\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_48\n",
      "Epoch: 49\n",
      "FGW torch.Size([29508, 5]) 0.00010002560884458944\n",
      "Penalty params: tau=0.97907 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=49 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 49, train\n",
      " fgw:0.4634382\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.4634382\n",
      "Measure Epoch 49, train\n",
      " similarity:0.0269370\n",
      " penlog:-1.3318252\n",
      "Metrics Epoch 49, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:90.0869565\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:20.2600000\n",
      " batch_invalid_valency_nodes:78.6956522\n",
      " batch_nodes_0degree:18.0800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-20.2400000\n",
      " batch_node_degree:0.2852174\n",
      "Logits [9.913936614990234, 1.3536282777786255, 49.65430450439453]\n",
      "Epoch duration: 2.137789726257324\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_49\n",
      "Epoch: 50\n",
      "FGW torch.Size([29508, 5]) 9.511151438346133e-05\n",
      "Penalty params: tau=0.97864 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=50 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 50, train\n",
      " fgw:0.4685975\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.4685975\n",
      "Measure Epoch 50, train\n",
      " similarity:0.0160151\n",
      " penlog:1.1927324\n",
      "Metrics Epoch 50, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:90.0000000\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:21.9400000\n",
      " batch_invalid_valency_nodes:91.8260870\n",
      " batch_nodes_0degree:21.1200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-21.9400000\n",
      " batch_node_degree:0.1182609\n",
      "Logits [10.305646896362305, 1.351082682609558, 49.91714096069336]\n",
      "Epoch duration: 1.9995627403259277\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_50\n",
      "Epoch: 51\n",
      "FGW torch.Size([29508, 5]) 9.310063614975661e-05\n",
      "Penalty params: tau=0.97822 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=51 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 51, train\n",
      " fgw:0.4618385\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.4618385\n",
      "Measure Epoch 51, train\n",
      " similarity:0.0216176\n",
      " penlog:1.0486746\n",
      "Metrics Epoch 51, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:90.5217391\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:21.7400000\n",
      " batch_invalid_valency_nodes:90.8695652\n",
      " batch_nodes_0degree:20.9000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-21.7400000\n",
      " batch_node_degree:0.1182609\n",
      "Logits [10.571425437927246, 1.3455991744995117, 50.15559387207031]\n",
      "Epoch duration: 2.0767159461975098\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_51\n",
      "Epoch: 52\n",
      "FGW torch.Size([29508, 5]) 9.768909512786195e-05\n",
      "Penalty params: tau=0.97780 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=52 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 52, train\n",
      " fgw:0.4533955\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.4533955\n",
      "Measure Epoch 52, train\n",
      " similarity:0.0294362\n",
      " penlog:1.2331598\n",
      "Metrics Epoch 52, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:90.4347826\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:20.0000000\n",
      " batch_invalid_valency_nodes:77.7391304\n",
      " batch_nodes_0degree:17.8800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-19.9600000\n",
      " batch_node_degree:0.2747826\n",
      "Logits [10.690103530883789, 1.3553274869918823, 50.125579833984375]\n",
      "Epoch duration: 2.0377042293548584\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_52\n",
      "Epoch: 53\n",
      "FGW torch.Size([29508, 5]) 0.00010635320359142497\n",
      "Penalty params: tau=0.97738 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=53 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 53, train\n",
      " fgw:0.4490495\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.4490495\n",
      "Measure Epoch 53, train\n",
      " similarity:0.0346751\n",
      " penlog:-3.2654207\n",
      "Metrics Epoch 53, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:90.1739130\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:16.9000000\n",
      " batch_invalid_valency_nodes:58.0869565\n",
      " batch_nodes_0degree:13.3200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-16.8000000\n",
      " batch_node_degree:0.5565217\n",
      "Logits [10.723613739013672, 1.3780268430709839, 49.89656448364258]\n",
      "Epoch duration: 1.9010651111602783\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_53\n",
      "Epoch: 54\n",
      "FGW torch.Size([29508, 5]) 0.0001137835206463933\n",
      "Penalty params: tau=0.97695 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=54 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 54, train\n",
      " fgw:0.4493177\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.4493177\n",
      "Measure Epoch 54, train\n",
      " similarity:0.0458224\n",
      " penlog:-9.8857817\n",
      "Metrics Epoch 54, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:90.6086957\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:13.0400000\n",
      " batch_invalid_valency_nodes:43.9130435\n",
      " batch_nodes_0degree:9.8400000\n",
      " batch_nodes_7plus_degree:0.0200000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.6600000\n",
      " batch_node_degree:0.9200000\n",
      "Logits [10.807387351989746, 1.3994529247283936, 49.77237319946289]\n",
      "Epoch duration: 2.1171774864196777\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_54\n",
      "Epoch: 55\n",
      "FGW torch.Size([29508, 5]) 0.00011551815259736031\n",
      "Penalty params: tau=0.97653 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=55 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 55, train\n",
      " fgw:0.4462990\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.4462990\n",
      "Measure Epoch 55, train\n",
      " similarity:0.0481907\n",
      " penlog:-10.1520858\n",
      "Metrics Epoch 55, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:91.3043478\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:12.4600000\n",
      " batch_invalid_valency_nodes:42.5217391\n",
      " batch_nodes_0degree:9.4600000\n",
      " batch_nodes_7plus_degree:0.0200000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.9400000\n",
      " batch_node_degree:0.9843478\n",
      "Logits [11.01180648803711, 1.4104853868484497, 49.82261657714844]\n",
      "Epoch duration: 2.0285260677337646\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_55\n",
      "Epoch: 56\n",
      "FGW torch.Size([29508, 5]) 0.00010895335435634479\n",
      "Penalty params: tau=0.97611 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=56 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 56, train\n",
      " fgw:0.4354381\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.4354381\n",
      "Measure Epoch 56, train\n",
      " similarity:0.0402119\n",
      " penlog:-9.4162877\n",
      "Metrics Epoch 56, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:91.0434783\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:14.9600000\n",
      " batch_invalid_valency_nodes:54.3478261\n",
      " batch_nodes_0degree:12.2600000\n",
      " batch_nodes_7plus_degree:0.0200000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-14.6800000\n",
      " batch_node_degree:0.7478261\n",
      "Logits [11.327024459838867, 1.4089797735214233, 50.04014205932617]\n",
      "Epoch duration: 2.0406596660614014\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57\n",
      "FGW torch.Size([29508, 5]) 0.00010333488171454519\n",
      "Penalty params: tau=0.97569 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=57 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 57, train\n",
      " fgw:0.4418819\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.4418819\n",
      "Measure Epoch 57, train\n",
      " similarity:0.0322887\n",
      " penlog:-2.4372364\n",
      "Metrics Epoch 57, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:91.2173913\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:18.6800000\n",
      " batch_invalid_valency_nodes:71.6521739\n",
      " batch_nodes_0degree:16.4200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-18.6600000\n",
      " batch_node_degree:0.3965217\n",
      "Logits [11.694164276123047, 1.415959358215332, 50.14701461791992]\n",
      "Epoch duration: 2.0217137336730957\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_57\n",
      "Epoch: 58\n",
      "FGW torch.Size([29508, 5]) 9.98880568658933e-05\n",
      "Penalty params: tau=0.97527 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=58 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 58, train\n",
      " fgw:0.4359808\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.4359808\n",
      "Measure Epoch 58, train\n",
      " similarity:0.0299708\n",
      " penlog:-0.2015972\n",
      "Metrics Epoch 58, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:91.2173913\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:20.1000000\n",
      " batch_invalid_valency_nodes:79.7391304\n",
      " batch_nodes_0degree:18.3200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-20.1000000\n",
      " batch_node_degree:0.2713043\n",
      "Logits [12.006292343139648, 1.4398040771484375, 49.97540283203125]\n",
      "Epoch duration: 1.9397449493408203\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_58\n",
      "Epoch: 59\n",
      "FGW torch.Size([29508, 5]) 0.00010007438686443493\n",
      "Penalty params: tau=0.97485 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=59 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 59, train\n",
      " fgw:0.4360053\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.4360053\n",
      "Measure Epoch 59, train\n",
      " similarity:0.0280961\n",
      " penlog:-2.2930677\n",
      "Metrics Epoch 59, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:91.3043478\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:20.4400000\n",
      " batch_invalid_valency_nodes:81.3913043\n",
      " batch_nodes_0degree:18.6800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-20.4400000\n",
      " batch_node_degree:0.2469565\n",
      "Logits [12.218785285949707, 1.467949628829956, 49.72120666503906]\n",
      "Epoch duration: 2.055950403213501\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_59\n",
      "Epoch: 60\n",
      "FGW torch.Size([29508, 5]) 0.00010117291094502434\n",
      "Penalty params: tau=0.97443 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=60 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 60, train\n",
      " fgw:0.4337907\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.4337907\n",
      "Measure Epoch 60, train\n",
      " similarity:0.0276958\n",
      " penlog:-2.4383167\n",
      "Metrics Epoch 60, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:91.2173913\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:20.1800000\n",
      " batch_invalid_valency_nodes:80.1739130\n",
      " batch_nodes_0degree:18.4000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-20.1600000\n",
      " batch_node_degree:0.2782609\n",
      "Logits [12.303253173828125, 1.4879670143127441, 49.535221099853516]\n",
      "Epoch duration: 2.201500177383423\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_60\n",
      "Epoch: 61\n",
      "FGW torch.Size([29508, 5]) 0.00010212277265964076\n",
      "Penalty params: tau=0.97401 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=61 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 61, train\n",
      " fgw:0.4291003\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.4291003\n",
      "Measure Epoch 61, train\n",
      " similarity:0.0337000\n",
      " penlog:-2.8144440\n",
      "Metrics Epoch 61, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:91.9130435\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:19.0600000\n",
      " batch_invalid_valency_nodes:74.0869565\n",
      " batch_nodes_0degree:16.9800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-19.0600000\n",
      " batch_node_degree:0.3617391\n",
      "Logits [12.312033653259277, 1.4862971305847168, 49.59369659423828]\n",
      "Epoch duration: 2.0964202880859375\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_61\n",
      "Epoch: 62\n",
      "FGW torch.Size([29508, 5]) 0.0001048066551447846\n",
      "Penalty params: tau=0.97359 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=62 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 62, train\n",
      " fgw:0.4236985\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.4236985\n",
      "Measure Epoch 62, train\n",
      " similarity:0.0403305\n",
      " penlog:-0.9011342\n",
      "Metrics Epoch 62, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:92.0000000\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:17.2800000\n",
      " batch_invalid_valency_nodes:63.6521739\n",
      " batch_nodes_0degree:14.6200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-17.2800000\n",
      " batch_node_degree:0.5043478\n",
      "Logits [12.322132110595703, 1.4753105640411377, 49.808929443359375]\n",
      "Epoch duration: 2.074007511138916\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_62\n",
      "Epoch: 63\n",
      "FGW torch.Size([29508, 5]) 0.00010638017556630075\n",
      "Penalty params: tau=0.97317 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=63 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 63, train\n",
      " fgw:0.4259486\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.4259486\n",
      "Measure Epoch 63, train\n",
      " similarity:0.0473232\n",
      " penlog:0.9073537\n",
      "Metrics Epoch 63, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:91.4782609\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:15.4200000\n",
      " batch_invalid_valency_nodes:53.5652174\n",
      " batch_nodes_0degree:12.3200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-15.4200000\n",
      " batch_node_degree:0.6643478\n",
      "Logits [12.394076347351074, 1.4748018980026245, 49.94194412231445]\n",
      "Epoch duration: 2.0199503898620605\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_63\n",
      "Epoch: 64\n",
      "FGW torch.Size([29508, 5]) 0.00010808817751239985\n",
      "Penalty params: tau=0.97275 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=64 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 64, train\n",
      " fgw:0.4235489\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.4235489\n",
      "Measure Epoch 64, train\n",
      " similarity:0.0477570\n",
      " penlog:-1.2701195\n",
      "Metrics Epoch 64, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:91.3913043\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:14.5200000\n",
      " batch_invalid_valency_nodes:50.5217391\n",
      " batch_nodes_0degree:11.6000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-14.5000000\n",
      " batch_node_degree:0.7443478\n",
      "Logits [12.574963569641113, 1.494101881980896, 49.88800048828125]\n",
      "Epoch duration: 2.027245283126831\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_64\n",
      "Epoch: 65\n",
      "FGW torch.Size([29508, 5]) 0.00010689309419831261\n",
      "Penalty params: tau=0.97233 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=65 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 65, train\n",
      " fgw:0.4226080\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.4226080\n",
      "Measure Epoch 65, train\n",
      " similarity:0.0489827\n",
      " penlog:0.8623173\n",
      "Metrics Epoch 65, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:91.1304348\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:14.5000000\n",
      " batch_invalid_valency_nodes:49.9130435\n",
      " batch_nodes_0degree:11.4800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-14.4600000\n",
      " batch_node_degree:0.7478261\n",
      "Logits [12.857379913330078, 1.5239925384521484, 49.75450134277344]\n",
      "Epoch duration: 2.0660693645477295\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66\n",
      "FGW torch.Size([29508, 5]) 0.00010354816913604736\n",
      "Penalty params: tau=0.97191 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=66 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 66, train\n",
      " fgw:0.4118288\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.4118288\n",
      "Measure Epoch 66, train\n",
      " similarity:0.0439690\n",
      " penlog:-1.0248379\n",
      "Metrics Epoch 66, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:91.5652174\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:15.7000000\n",
      " batch_invalid_valency_nodes:53.8260870\n",
      " batch_nodes_0degree:12.3600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-15.6400000\n",
      " batch_node_degree:0.6469565\n",
      "Logits [13.16187858581543, 1.545501947402954, 49.68433380126953]\n",
      "Epoch duration: 2.0762686729431152\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_66\n",
      "Epoch: 67\n",
      "FGW torch.Size([29508, 5]) 0.00010047118121292442\n",
      "Penalty params: tau=0.97149 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=67 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 67, train\n",
      " fgw:0.4095829\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.4095829\n",
      "Measure Epoch 67, train\n",
      " similarity:0.0402742\n",
      " penlog:1.2585291\n",
      "Metrics Epoch 67, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:91.7391304\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:16.7200000\n",
      " batch_invalid_valency_nodes:57.1304348\n",
      " batch_nodes_0degree:13.1400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-16.6800000\n",
      " batch_node_degree:0.5530435\n",
      "Logits [13.401042938232422, 1.5554674863815308, 49.812217712402344]\n",
      "Epoch duration: 2.0245673656463623\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_67\n",
      "Epoch: 68\n",
      "FGW torch.Size([29508, 5]) 9.784504800336435e-05\n",
      "Penalty params: tau=0.97107 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=68 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 68, train\n",
      " fgw:0.4133893\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.4133893\n",
      "Measure Epoch 68, train\n",
      " similarity:0.0377789\n",
      " penlog:1.3046012\n",
      "Metrics Epoch 68, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:92.0000000\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:17.2600000\n",
      " batch_invalid_valency_nodes:60.4347826\n",
      " batch_nodes_0degree:13.9000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-17.2600000\n",
      " batch_node_degree:0.5008696\n",
      "Logits [13.56264591217041, 1.550407886505127, 50.088233947753906]\n",
      "Epoch duration: 2.012798547744751\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_68\n",
      "Epoch: 69\n",
      "FGW torch.Size([29508, 5]) 9.780184336705133e-05\n",
      "Penalty params: tau=0.97065 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=69 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 69, train\n",
      " fgw:0.4143655\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.4143655\n",
      "Measure Epoch 69, train\n",
      " similarity:0.0380706\n",
      " penlog:1.1867165\n",
      "Metrics Epoch 69, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:92.1739130\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:17.1800000\n",
      " batch_invalid_valency_nodes:60.0869565\n",
      " batch_nodes_0degree:13.8200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-17.1600000\n",
      " batch_node_degree:0.5095652\n",
      "Logits [13.651599884033203, 1.548852562904358, 50.26579284667969]\n",
      "Epoch duration: 2.0054733753204346\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_69\n",
      "Epoch: 70\n",
      "FGW torch.Size([29508, 5]) 9.929069346981123e-05\n",
      "Penalty params: tau=0.97023 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=70 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 70, train\n",
      " fgw:0.4148555\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.4148555\n",
      "Measure Epoch 70, train\n",
      " similarity:0.0382733\n",
      " penlog:-0.7891092\n",
      "Metrics Epoch 70, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:92.3478261\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:16.3000000\n",
      " batch_invalid_valency_nodes:53.9130435\n",
      " batch_nodes_0degree:12.3800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-16.2600000\n",
      " batch_node_degree:0.5913043\n",
      "Logits [13.702990531921387, 1.5558066368103027, 50.298282623291016]\n",
      "Epoch duration: 2.1215639114379883\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_70\n",
      "Epoch: 71\n",
      "FGW torch.Size([29508, 5]) 0.00010271168866893277\n",
      "Penalty params: tau=0.96981 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=71 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 71, train\n",
      " fgw:0.4114867\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.4114867\n",
      "Measure Epoch 71, train\n",
      " similarity:0.0405387\n",
      " penlog:-1.0205509\n",
      "Metrics Epoch 71, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:92.6956522\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:15.4600000\n",
      " batch_invalid_valency_nodes:48.6956522\n",
      " batch_nodes_0degree:11.1800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-15.4400000\n",
      " batch_node_degree:0.6800000\n",
      "Logits [13.768011093139648, 1.5674362182617188, 50.24559020996094]\n",
      "Epoch duration: 2.0889477729797363\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_71\n",
      "Epoch: 72\n",
      "FGW torch.Size([29508, 5]) 0.00010423693311167881\n",
      "Penalty params: tau=0.96939 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=72 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 72, train\n",
      " fgw:0.4071699\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.4071699\n",
      "Measure Epoch 72, train\n",
      " similarity:0.0412307\n",
      " penlog:-0.9330795\n",
      "Metrics Epoch 72, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:93.1304348\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:15.1200000\n",
      " batch_invalid_valency_nodes:45.5652174\n",
      " batch_nodes_0degree:10.4600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-15.1000000\n",
      " batch_node_degree:0.7182609\n",
      "Logits [13.901040077209473, 1.5750397443771362, 50.23735046386719]\n",
      "Epoch duration: 2.1478800773620605\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_72\n",
      "Epoch: 73\n",
      "FGW torch.Size([29508, 5]) 0.00010401863255538046\n",
      "Penalty params: tau=0.96897 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=73 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 73, train\n",
      " fgw:0.4030931\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.4030931\n",
      "Measure Epoch 73, train\n",
      " similarity:0.0404092\n",
      " penlog:-0.7073546\n",
      "Metrics Epoch 73, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:92.1739130\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:15.3000000\n",
      " batch_invalid_valency_nodes:46.0869565\n",
      " batch_nodes_0degree:10.5800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-15.3000000\n",
      " batch_node_degree:0.7008696\n",
      "Logits [14.100902557373047, 1.5767239332199097, 50.32084655761719]\n",
      "Epoch duration: 2.027339220046997\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_73\n",
      "Epoch: 74\n",
      "FGW torch.Size([29508, 5]) 0.00010256774112349376\n",
      "Penalty params: tau=0.96855 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=74 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 74, train\n",
      " fgw:0.4020377\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.4020377\n",
      "Measure Epoch 74, train\n",
      " similarity:0.0367606\n",
      " penlog:1.4165574\n",
      "Metrics Epoch 74, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:92.0869565\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:16.1400000\n",
      " batch_invalid_valency_nodes:51.0434783\n",
      " batch_nodes_0degree:11.7400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-16.1400000\n",
      " batch_node_degree:0.6139130\n",
      "Logits [14.303223609924316, 1.5809452533721924, 50.419673919677734]\n",
      "Epoch duration: 1.983851671218872\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75\n",
      "FGW torch.Size([29508, 5]) 0.00010139369260286912\n",
      "Penalty params: tau=0.96814 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=75 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 75, train\n",
      " fgw:0.4020040\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.4020040\n",
      "Measure Epoch 75, train\n",
      " similarity:0.0353158\n",
      " penlog:1.0293556\n",
      "Metrics Epoch 75, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:92.8695652\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:16.7600000\n",
      " batch_invalid_valency_nodes:53.5652174\n",
      " batch_nodes_0degree:12.3200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-16.7600000\n",
      " batch_node_degree:0.5547826\n",
      "Logits [14.450095176696777, 1.5956752300262451, 50.41508102416992]\n",
      "Epoch duration: 2.0466275215148926\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_75\n",
      "Epoch: 76\n",
      "FGW torch.Size([29508, 5]) 0.00010031920828623697\n",
      "Penalty params: tau=0.96772 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=76 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 76, train\n",
      " fgw:0.4001653\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.4001653\n",
      "Measure Epoch 76, train\n",
      " similarity:0.0370957\n",
      " penlog:1.0382809\n",
      "Metrics Epoch 76, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:93.3913043\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:17.0400000\n",
      " batch_invalid_valency_nodes:54.8695652\n",
      " batch_nodes_0degree:12.6200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-17.0400000\n",
      " batch_node_degree:0.5252174\n",
      "Logits [14.568235397338867, 1.6112533807754517, 50.42750549316406]\n",
      "Epoch duration: 2.156691789627075\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_76\n",
      "Epoch: 77\n",
      "FGW torch.Size([29508, 5]) 9.967482765205204e-05\n",
      "Penalty params: tau=0.96730 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=77 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 77, train\n",
      " fgw:0.3960144\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3960144\n",
      "Measure Epoch 77, train\n",
      " similarity:0.0372540\n",
      " penlog:1.0367613\n",
      "Metrics Epoch 77, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:93.5652174\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:17.1000000\n",
      " batch_invalid_valency_nodes:55.5652174\n",
      " batch_nodes_0degree:12.7800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-17.1000000\n",
      " batch_node_degree:0.5182609\n",
      "Logits [14.696086883544922, 1.623885154724121, 50.475914001464844]\n",
      "Epoch duration: 2.1213326454162598\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_77\n",
      "Epoch: 78\n",
      "FGW torch.Size([29508, 5]) 9.97234892565757e-05\n",
      "Penalty params: tau=0.96688 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=78 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 78, train\n",
      " fgw:0.3989442\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3989442\n",
      "Measure Epoch 78, train\n",
      " similarity:0.0386611\n",
      " penlog:0.9622770\n",
      "Metrics Epoch 78, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:93.6521739\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:16.9200000\n",
      " batch_invalid_valency_nodes:55.0434783\n",
      " batch_nodes_0degree:12.6600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-16.9200000\n",
      " batch_node_degree:0.5321739\n",
      "Logits [14.860859870910645, 1.6288107633590698, 50.60979461669922]\n",
      "Epoch duration: 2.1289877891540527\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_78\n",
      "Epoch: 79\n",
      "FGW torch.Size([29508, 5]) 9.889337525237352e-05\n",
      "Penalty params: tau=0.96647 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=79 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 79, train\n",
      " fgw:0.3902235\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3902235\n",
      "Measure Epoch 79, train\n",
      " similarity:0.0392927\n",
      " penlog:0.9410721\n",
      "Metrics Epoch 79, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:93.5652174\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:16.7400000\n",
      " batch_invalid_valency_nodes:54.5217391\n",
      " batch_nodes_0degree:12.5400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-16.7400000\n",
      " batch_node_degree:0.5478261\n",
      "Logits [15.010961532592773, 1.6304748058319092, 50.71632766723633]\n",
      "Epoch duration: 2.0805745124816895\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_79\n",
      "Epoch: 80\n",
      "FGW torch.Size([29508, 5]) 9.954837878467515e-05\n",
      "Penalty params: tau=0.96605 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=80 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 80, train\n",
      " fgw:0.3897442\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3897442\n",
      "Measure Epoch 80, train\n",
      " similarity:0.0397680\n",
      " penlog:1.0520733\n",
      "Metrics Epoch 80, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:93.3043478\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:16.6800000\n",
      " batch_invalid_valency_nodes:53.8260870\n",
      " batch_nodes_0degree:12.3800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-16.6800000\n",
      " batch_node_degree:0.5530435\n",
      "Logits [15.13660717010498, 1.6362686157226562, 50.71409606933594]\n",
      "Epoch duration: 1.9988903999328613\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_80\n",
      "Epoch: 81\n",
      "FGW torch.Size([29508, 5]) 0.0001009322950267233\n",
      "Penalty params: tau=0.96563 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=81 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 81, train\n",
      " fgw:0.3909350\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3909350\n",
      "Measure Epoch 81, train\n",
      " similarity:0.0403541\n",
      " penlog:0.9353416\n",
      "Metrics Epoch 81, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:93.5652174\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:16.2800000\n",
      " batch_invalid_valency_nodes:51.9130435\n",
      " batch_nodes_0degree:11.9400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-16.2800000\n",
      " batch_node_degree:0.6139130\n",
      "Logits [15.216355323791504, 1.6462808847427368, 50.62071990966797]\n",
      "Epoch duration: 2.078432083129883\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_81\n",
      "Epoch: 82\n",
      "FGW torch.Size([29508, 5]) 0.00010173101327382028\n",
      "Penalty params: tau=0.96521 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=82 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 82, train\n",
      " fgw:0.3906852\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3906852\n",
      "Measure Epoch 82, train\n",
      " similarity:0.0394447\n",
      " penlog:0.7896324\n",
      "Metrics Epoch 82, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:93.5652174\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:16.2600000\n",
      " batch_invalid_valency_nodes:52.2608696\n",
      " batch_nodes_0degree:12.0200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-16.2600000\n",
      " batch_node_degree:0.6434783\n",
      "Logits [15.314701080322266, 1.655504584312439, 50.542694091796875]\n",
      "Epoch duration: 2.115082263946533\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_82\n",
      "Epoch: 83\n",
      "FGW torch.Size([29508, 5]) 0.00010008085519075394\n",
      "Penalty params: tau=0.96480 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=83 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 83, train\n",
      " fgw:0.3827733\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3827733\n",
      "Measure Epoch 83, train\n",
      " similarity:0.0397124\n",
      " penlog:0.6777631\n",
      "Metrics Epoch 83, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:93.7391304\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:16.1000000\n",
      " batch_invalid_valency_nodes:51.3913043\n",
      " batch_nodes_0degree:11.8200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-16.1000000\n",
      " batch_node_degree:0.6643478\n",
      "Logits [15.411558151245117, 1.6565139293670654, 50.626216888427734]\n",
      "Epoch duration: 2.0349464416503906\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 84\n",
      "FGW torch.Size([29508, 5]) 9.966050129150972e-05\n",
      "Penalty params: tau=0.96438 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=84 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 84, train\n",
      " fgw:0.3824805\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3824805\n",
      "Measure Epoch 84, train\n",
      " similarity:0.0383922\n",
      " penlog:0.6987774\n",
      "Metrics Epoch 84, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:93.3913043\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:16.2200000\n",
      " batch_invalid_valency_nodes:52.6086957\n",
      " batch_nodes_0degree:12.1000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-16.2200000\n",
      " batch_node_degree:0.6400000\n",
      "Logits [15.555793762207031, 1.6529676914215088, 50.81266403198242]\n",
      "Epoch duration: 1.9491455554962158\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_84\n",
      "Epoch: 85\n",
      "FGW torch.Size([29508, 5]) 9.762129047885537e-05\n",
      "Penalty params: tau=0.96397 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=85 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 85, train\n",
      " fgw:0.3780950\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3780950\n",
      "Measure Epoch 85, train\n",
      " similarity:0.0392332\n",
      " penlog:0.8455707\n",
      "Metrics Epoch 85, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:93.7391304\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:16.9800000\n",
      " batch_invalid_valency_nodes:55.5652174\n",
      " batch_nodes_0degree:12.7800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-16.9800000\n",
      " batch_node_degree:0.5686957\n",
      "Logits [15.747522354125977, 1.6608121395111084, 50.87330627441406]\n",
      "Epoch duration: 2.1456263065338135\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_85\n",
      "Epoch: 86\n",
      "FGW torch.Size([29508, 5]) 9.829035116126761e-05\n",
      "Penalty params: tau=0.96355 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=86 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 86, train\n",
      " fgw:0.3839035\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3839035\n",
      "Measure Epoch 86, train\n",
      " similarity:0.0306800\n",
      " penlog:-7.2695125\n",
      "Metrics Epoch 86, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:94.2608696\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:17.2600000\n",
      " batch_invalid_valency_nodes:55.9130435\n",
      " batch_nodes_0degree:12.7800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-17.2600000\n",
      " batch_node_degree:0.5634783\n",
      "Logits [15.96076774597168, 1.6870434284210205, 50.72307205200195]\n",
      "Epoch duration: 2.005476236343384\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_86\n",
      "Epoch: 87\n",
      "FGW torch.Size([29508, 5]) 9.913575922837481e-05\n",
      "Penalty params: tau=0.96313 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=87 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 87, train\n",
      " fgw:0.3847226\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3847226\n",
      "Measure Epoch 87, train\n",
      " similarity:0.0316996\n",
      " penlog:-7.3779484\n",
      "Metrics Epoch 87, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:94.2608696\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:16.8000000\n",
      " batch_invalid_valency_nodes:53.4782609\n",
      " batch_nodes_0degree:12.2200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-16.8000000\n",
      " batch_node_degree:0.6226087\n",
      "Logits [16.07585906982422, 1.7044696807861328, 50.7369499206543]\n",
      "Epoch duration: 2.1533379554748535\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_87\n",
      "Epoch: 88\n",
      "FGW torch.Size([29508, 5]) 9.923431935021654e-05\n",
      "Penalty params: tau=0.96272 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=88 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 88, train\n",
      " fgw:0.3760447\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3760447\n",
      "Measure Epoch 88, train\n",
      " similarity:0.0355737\n",
      " penlog:-3.0910208\n",
      "Metrics Epoch 88, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:94.0869565\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:16.9600000\n",
      " batch_invalid_valency_nodes:54.2608696\n",
      " batch_nodes_0degree:12.4400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-16.9600000\n",
      " batch_node_degree:0.5634783\n",
      "Logits [16.173078536987305, 1.7008600234985352, 50.918460845947266]\n",
      "Epoch duration: 2.1817774772644043\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_88\n",
      "Epoch: 89\n",
      "FGW torch.Size([29508, 5]) 9.92414788925089e-05\n",
      "Penalty params: tau=0.96230 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=89 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 89, train\n",
      " fgw:0.3755025\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3755025\n",
      "Measure Epoch 89, train\n",
      " similarity:0.0338395\n",
      " penlog:-1.2110529\n",
      "Metrics Epoch 89, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:93.5652174\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:17.0600000\n",
      " batch_invalid_valency_nodes:54.3478261\n",
      " batch_nodes_0degree:12.4800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-17.0600000\n",
      " batch_node_degree:0.5495652\n",
      "Logits [16.312833786010742, 1.6995750665664673, 51.014888763427734]\n",
      "Epoch duration: 2.0792648792266846\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_89\n",
      "Epoch: 90\n",
      "FGW torch.Size([29508, 5]) 9.980624599847943e-05\n",
      "Penalty params: tau=0.96189 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=90 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 90, train\n",
      " fgw:0.3759940\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3759940\n",
      "Measure Epoch 90, train\n",
      " similarity:0.0343320\n",
      " penlog:-1.0999320\n",
      "Metrics Epoch 90, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:94.0000000\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:17.0400000\n",
      " batch_invalid_valency_nodes:53.5652174\n",
      " batch_nodes_0degree:12.3000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-17.0400000\n",
      " batch_node_degree:0.5686957\n",
      "Logits [16.44098663330078, 1.7182797193527222, 50.86832809448242]\n",
      "Epoch duration: 2.0392019748687744\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_90\n",
      "Epoch: 91\n",
      "FGW torch.Size([29508, 5]) 9.985904034692794e-05\n",
      "Penalty params: tau=0.96147 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=91 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 91, train\n",
      " fgw:0.3775130\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3775130\n",
      "Measure Epoch 91, train\n",
      " similarity:0.0314692\n",
      " penlog:-2.9887246\n",
      "Metrics Epoch 91, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:94.0869565\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:17.0600000\n",
      " batch_invalid_valency_nodes:53.7391304\n",
      " batch_nodes_0degree:12.3200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-17.0600000\n",
      " batch_node_degree:0.5930435\n",
      "Logits [16.552753448486328, 1.7373156547546387, 50.73225021362305]\n",
      "Epoch duration: 1.9081389904022217\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_91\n",
      "Epoch: 92\n",
      "FGW torch.Size([29508, 5]) 9.884566679829732e-05\n",
      "Penalty params: tau=0.96106 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=92 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 92, train\n",
      " fgw:0.3756727\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3756727\n",
      "Measure Epoch 92, train\n",
      " similarity:0.0332463\n",
      " penlog:-0.9055659\n",
      "Metrics Epoch 92, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:94.2608696\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:17.0800000\n",
      " batch_invalid_valency_nodes:53.6521739\n",
      " batch_nodes_0degree:12.3200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-17.0800000\n",
      " batch_node_degree:0.5773913\n",
      "Logits [16.651294708251953, 1.742209553718567, 50.79712677001953]\n",
      "Epoch duration: 2.0357744693756104\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 93\n",
      "FGW torch.Size([29508, 5]) 9.843769657891244e-05\n",
      "Penalty params: tau=0.96064 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=93 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 93, train\n",
      " fgw:0.3728574\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3728574\n",
      "Measure Epoch 93, train\n",
      " similarity:0.0356681\n",
      " penlog:-0.9583657\n",
      "Metrics Epoch 93, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:94.2608696\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:16.8000000\n",
      " batch_invalid_valency_nodes:52.7826087\n",
      " batch_nodes_0degree:12.1200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-16.8000000\n",
      " batch_node_degree:0.5947826\n",
      "Logits [16.740812301635742, 1.7355464696884155, 50.993019104003906]\n",
      "Epoch duration: 2.041869878768921\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_93\n",
      "Epoch: 94\n",
      "FGW torch.Size([29508, 5]) 9.86797604127787e-05\n",
      "Penalty params: tau=0.96023 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=94 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 94, train\n",
      " fgw:0.3720355\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3720355\n",
      "Measure Epoch 94, train\n",
      " similarity:0.0323032\n",
      " penlog:-1.0820817\n",
      "Metrics Epoch 94, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:93.7391304\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:16.9400000\n",
      " batch_invalid_valency_nodes:53.3043478\n",
      " batch_nodes_0degree:12.2400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-16.9400000\n",
      " batch_node_degree:0.5791304\n",
      "Logits [16.85738182067871, 1.7350410223007202, 51.082210540771484]\n",
      "Epoch duration: 2.0284805297851562\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_94\n",
      "Epoch: 95\n",
      "FGW torch.Size([29508, 5]) 9.872817463474348e-05\n",
      "Penalty params: tau=0.95981 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=95 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 95, train\n",
      " fgw:0.3702217\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3702217\n",
      "Measure Epoch 95, train\n",
      " similarity:0.0337195\n",
      " penlog:-1.0605167\n",
      "Metrics Epoch 95, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:94.5217391\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:16.7800000\n",
      " batch_invalid_valency_nodes:52.3478261\n",
      " batch_nodes_0degree:12.0200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-16.7800000\n",
      " batch_node_degree:0.6173913\n",
      "Logits [16.967575073242188, 1.7505468130111694, 50.993202209472656]\n",
      "Epoch duration: 1.9990570545196533\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_95\n",
      "Epoch: 96\n",
      "FGW torch.Size([29508, 5]) 9.950274397851899e-05\n",
      "Penalty params: tau=0.95940 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=96 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 96, train\n",
      " fgw:0.3685368\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3685368\n",
      "Measure Epoch 96, train\n",
      " similarity:0.0347628\n",
      " penlog:-1.1151260\n",
      "Metrics Epoch 96, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:94.5217391\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:16.1600000\n",
      " batch_invalid_valency_nodes:48.9565217\n",
      " batch_nodes_0degree:11.2400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-16.1600000\n",
      " batch_node_degree:0.7113043\n",
      "Logits [17.05292320251465, 1.7703726291656494, 50.84870910644531]\n",
      "Epoch duration: 2.1540474891662598\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_96\n",
      "Epoch: 97\n",
      "FGW torch.Size([29508, 5]) 9.971013787435368e-05\n",
      "Penalty params: tau=0.95898 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=97 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 97, train\n",
      " fgw:0.3721513\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3721513\n",
      "Measure Epoch 97, train\n",
      " similarity:0.0355394\n",
      " penlog:0.7622841\n",
      "Metrics Epoch 97, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:94.4347826\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:16.0400000\n",
      " batch_invalid_valency_nodes:48.6956522\n",
      " batch_nodes_0degree:11.2000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-16.0400000\n",
      " batch_node_degree:0.7113043\n",
      "Logits [17.150733947753906, 1.7724883556365967, 50.89583969116211]\n",
      "Epoch duration: 2.071265697479248\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_97\n",
      "Epoch: 98\n",
      "FGW torch.Size([29508, 5]) 9.841902647167444e-05\n",
      "Penalty params: tau=0.95857 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=98 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 98, train\n",
      " fgw:0.3638075\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3638075\n",
      "Measure Epoch 98, train\n",
      " similarity:0.0340023\n",
      " penlog:0.6949896\n",
      "Metrics Epoch 98, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:94.4347826\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:16.5200000\n",
      " batch_invalid_valency_nodes:50.6086957\n",
      " batch_nodes_0degree:11.6400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-16.5200000\n",
      " batch_node_degree:0.6313043\n",
      "Logits [17.283306121826172, 1.7608110904693604, 51.057838439941406]\n",
      "Epoch duration: 1.9901390075683594\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_98\n",
      "Epoch: 99\n",
      "FGW torch.Size([29508, 5]) 9.817629324970767e-05\n",
      "Penalty params: tau=0.95816 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=99 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 99, train\n",
      " fgw:0.3656174\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3656174\n",
      "Measure Epoch 99, train\n",
      " similarity:0.0345723\n",
      " penlog:0.7820064\n",
      "Metrics Epoch 99, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:94.4347826\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:16.6800000\n",
      " batch_invalid_valency_nodes:52.0869565\n",
      " batch_nodes_0degree:11.9800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-16.6800000\n",
      " batch_node_degree:0.6086957\n",
      "Logits [17.43929672241211, 1.7632555961608887, 51.08790969848633]\n",
      "Epoch duration: 2.0146539211273193\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_99\n",
      "Epoch: 100\n",
      "FGW torch.Size([29508, 5]) 9.831264469539747e-05\n",
      "Penalty params: tau=0.95774 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=100 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 100, train\n",
      " fgw:0.3635072\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3635072\n",
      "Measure Epoch 100, train\n",
      " similarity:0.0346185\n",
      " penlog:1.0253535\n",
      "Metrics Epoch 100, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:94.7826087\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:16.4400000\n",
      " batch_invalid_valency_nodes:51.4782609\n",
      " batch_nodes_0degree:11.8400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-16.4400000\n",
      " batch_node_degree:0.6486957\n",
      "Logits [17.544113159179688, 1.7855902910232544, 50.92412185668945]\n",
      "Epoch duration: 1.9991660118103027\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_100\n",
      "Epoch: 101\n",
      "FGW torch.Size([29508, 5]) 9.863622108241543e-05\n",
      "Penalty params: tau=0.95733 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=101 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 101, train\n",
      " fgw:0.3644058\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3644058\n",
      "Measure Epoch 101, train\n",
      " similarity:0.0353385\n",
      " penlog:-1.0034045\n",
      "Metrics Epoch 101, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:95.1304348\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:16.2600000\n",
      " batch_invalid_valency_nodes:51.7391304\n",
      " batch_nodes_0degree:11.8800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-16.2400000\n",
      " batch_node_degree:0.6904348\n",
      "Logits [17.641313552856445, 1.797418236732483, 50.81546401977539]\n",
      "Epoch duration: 1.9219894409179688\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 102\n",
      "FGW torch.Size([29508, 5]) 9.846903412835672e-05\n",
      "Penalty params: tau=0.95692 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=102 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 102, train\n",
      " fgw:0.3607626\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3607626\n",
      "Measure Epoch 102, train\n",
      " similarity:0.0392916\n",
      " penlog:-3.3494704\n",
      "Metrics Epoch 102, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:94.9565217\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:15.6200000\n",
      " batch_invalid_valency_nodes:48.5217391\n",
      " batch_nodes_0degree:11.1200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-15.6200000\n",
      " batch_node_degree:0.7443478\n",
      "Logits [17.696548461914062, 1.7920746803283691, 50.92263412475586]\n",
      "Epoch duration: 2.023681163787842\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_102\n",
      "Epoch: 103\n",
      "FGW torch.Size([29508, 5]) 9.896317351376638e-05\n",
      "Penalty params: tau=0.95650 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=103 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 103, train\n",
      " fgw:0.3601033\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3601033\n",
      "Measure Epoch 103, train\n",
      " similarity:0.0419736\n",
      " penlog:0.6323165\n",
      "Metrics Epoch 103, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:94.6086957\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:15.3000000\n",
      " batch_invalid_valency_nodes:46.4347826\n",
      " batch_nodes_0degree:10.6800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-15.3000000\n",
      " batch_node_degree:0.7652174\n",
      "Logits [17.709304809570312, 1.7913323640823364, 51.02219772338867]\n",
      "Epoch duration: 1.9769189357757568\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_103\n",
      "Epoch: 104\n",
      "FGW torch.Size([29508, 5]) 9.83318459475413e-05\n",
      "Penalty params: tau=0.95609 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=104 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 104, train\n",
      " fgw:0.3585566\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3585566\n",
      "Measure Epoch 104, train\n",
      " similarity:0.0385066\n",
      " penlog:0.6779929\n",
      "Metrics Epoch 104, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:94.6956522\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:15.1800000\n",
      " batch_invalid_valency_nodes:45.3043478\n",
      " batch_nodes_0degree:10.4200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-15.1800000\n",
      " batch_node_degree:0.7982609\n",
      "Logits [17.775184631347656, 1.8023724555969238, 51.02590560913086]\n",
      "Epoch duration: 2.0289528369903564\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_104\n",
      "Epoch: 105\n",
      "FGW torch.Size([29508, 5]) 9.661862713983282e-05\n",
      "Penalty params: tau=0.95568 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=105 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 105, train\n",
      " fgw:0.3540791\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3540791\n",
      "Measure Epoch 105, train\n",
      " similarity:0.0391510\n",
      " penlog:0.8829173\n",
      "Metrics Epoch 105, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:94.8695652\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:15.2200000\n",
      " batch_invalid_valency_nodes:45.1304348\n",
      " batch_nodes_0degree:10.3800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-15.2200000\n",
      " batch_node_degree:0.8069565\n",
      "Logits [17.922391891479492, 1.814329743385315, 51.06008529663086]\n",
      "Epoch duration: 1.876058578491211\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_105\n",
      "Epoch: 106\n",
      "FGW torch.Size([29508, 5]) 9.71147819655016e-05\n",
      "Penalty params: tau=0.95526 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=106 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 106, train\n",
      " fgw:0.3588387\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3588387\n",
      "Measure Epoch 106, train\n",
      " similarity:0.0394584\n",
      " penlog:0.9385270\n",
      "Metrics Epoch 106, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:94.8695652\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:15.3200000\n",
      " batch_invalid_valency_nodes:45.7391304\n",
      " batch_nodes_0degree:10.5200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-15.3200000\n",
      " batch_node_degree:0.8034783\n",
      "Logits [18.05855941772461, 1.8147735595703125, 51.195491790771484]\n",
      "Epoch duration: 1.894120216369629\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_106\n",
      "Epoch: 107\n",
      "FGW torch.Size([29508, 5]) 9.693639003671706e-05\n",
      "Penalty params: tau=0.95485 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=107 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 107, train\n",
      " fgw:0.3550066\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3550066\n",
      "Measure Epoch 107, train\n",
      " similarity:0.0384798\n",
      " penlog:0.7449467\n",
      "Metrics Epoch 107, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:94.5217391\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:15.2200000\n",
      " batch_invalid_valency_nodes:45.1304348\n",
      " batch_nodes_0degree:10.3800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-15.2200000\n",
      " batch_node_degree:0.8208696\n",
      "Logits [18.07839584350586, 1.8115674257278442, 51.35527801513672]\n",
      "Epoch duration: 2.0896174907684326\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_107\n",
      "Epoch: 108\n",
      "FGW torch.Size([29508, 5]) 9.653194138081744e-05\n",
      "Penalty params: tau=0.95444 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=108 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 108, train\n",
      " fgw:0.3523666\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3523666\n",
      "Measure Epoch 108, train\n",
      " similarity:0.0397214\n",
      " penlog:-3.3441514\n",
      "Metrics Epoch 108, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:94.6956522\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:14.8400000\n",
      " batch_invalid_valency_nodes:42.6956522\n",
      " batch_nodes_0degree:9.7800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-14.8400000\n",
      " batch_node_degree:0.8782609\n",
      "Logits [18.125612258911133, 1.8119349479675293, 51.41222381591797]\n",
      "Epoch duration: 2.235657215118408\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_108\n",
      "Epoch: 109\n",
      "FGW torch.Size([29508, 5]) 9.677745401859283e-05\n",
      "Penalty params: tau=0.95403 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=109 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 109, train\n",
      " fgw:0.3511341\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3511341\n",
      "Measure Epoch 109, train\n",
      " similarity:0.0361333\n",
      " penlog:-5.2959901\n",
      "Metrics Epoch 109, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:95.2173913\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:14.9400000\n",
      " batch_invalid_valency_nodes:44.0000000\n",
      " batch_nodes_0degree:10.0600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-14.9200000\n",
      " batch_node_degree:0.9008696\n",
      "Logits [18.252826690673828, 1.8158029317855835, 51.43284225463867]\n",
      "Epoch duration: 2.13920259475708\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_109\n",
      "Epoch: 110\n",
      "FGW torch.Size([29508, 5]) 9.752204641699791e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penalty params: tau=0.95362 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=110 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 110, train\n",
      " fgw:0.3504994\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3504994\n",
      "Measure Epoch 110, train\n",
      " similarity:0.0331063\n",
      " penlog:-9.4743323\n",
      "Metrics Epoch 110, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:95.3043478\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:14.7000000\n",
      " batch_invalid_valency_nodes:42.1739130\n",
      " batch_nodes_0degree:9.6000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-14.6800000\n",
      " batch_node_degree:0.9617391\n",
      "Logits [18.331438064575195, 1.8259060382843018, 51.41981887817383]\n",
      "Epoch duration: 1.9977240562438965\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_110\n",
      "Epoch: 111\n",
      "FGW torch.Size([29508, 5]) 9.74474532995373e-05\n",
      "Penalty params: tau=0.95320 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=111 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 111, train\n",
      " fgw:0.3488060\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3488060\n",
      "Measure Epoch 111, train\n",
      " similarity:0.0344277\n",
      " penlog:-7.4226723\n",
      "Metrics Epoch 111, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:95.3043478\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:14.5600000\n",
      " batch_invalid_valency_nodes:41.9130435\n",
      " batch_nodes_0degree:9.5600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-14.5400000\n",
      " batch_node_degree:0.9669565\n",
      "Logits [18.424884796142578, 1.8309599161148071, 51.48518753051758]\n",
      "Epoch duration: 2.049499750137329\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_111\n",
      "Epoch: 112\n",
      "FGW torch.Size([29508, 5]) 9.62997946771793e-05\n",
      "Penalty params: tau=0.95279 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=112 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 112, train\n",
      " fgw:0.3472425\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3472425\n",
      "Measure Epoch 112, train\n",
      " similarity:0.0407130\n",
      " penlog:-1.3566215\n",
      "Metrics Epoch 112, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:94.8695652\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:14.8200000\n",
      " batch_invalid_valency_nodes:43.5652174\n",
      " batch_nodes_0degree:10.0000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-14.8200000\n",
      " batch_node_degree:0.9043478\n",
      "Logits [18.553367614746094, 1.8287901878356934, 51.62372970581055]\n",
      "Epoch duration: 2.001396417617798\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_112\n",
      "Epoch: 113\n",
      "FGW torch.Size([29508, 5]) 9.691105515230447e-05\n",
      "Penalty params: tau=0.95238 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=113 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 113, train\n",
      " fgw:0.3531083\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3531083\n",
      "Measure Epoch 113, train\n",
      " similarity:0.0387604\n",
      " penlog:-1.2965979\n",
      "Metrics Epoch 113, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:95.1304348\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:15.1200000\n",
      " batch_invalid_valency_nodes:45.2173913\n",
      " batch_nodes_0degree:10.3800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-15.1200000\n",
      " batch_node_degree:0.8643478\n",
      "Logits [18.685115814208984, 1.8397672176361084, 51.648372650146484]\n",
      "Epoch duration: 1.9834794998168945\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_113\n",
      "Epoch: 114\n",
      "FGW torch.Size([29508, 5]) 9.536089783068746e-05\n",
      "Penalty params: tau=0.95197 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=114 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 114, train\n",
      " fgw:0.3502359\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3502359\n",
      "Measure Epoch 114, train\n",
      " similarity:0.0387113\n",
      " penlog:-1.2302459\n",
      "Metrics Epoch 114, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:95.3913043\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:15.2000000\n",
      " batch_invalid_valency_nodes:44.7826087\n",
      " batch_nodes_0degree:10.2800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-15.2000000\n",
      " batch_node_degree:0.8782609\n",
      "Logits [18.803102493286133, 1.8611555099487305, 51.54385757446289]\n",
      "Epoch duration: 2.1412949562072754\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_114\n",
      "Epoch: 115\n",
      "FGW torch.Size([29508, 5]) 9.65661893133074e-05\n",
      "Penalty params: tau=0.95156 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=115 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 115, train\n",
      " fgw:0.3504055\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3504055\n",
      "Measure Epoch 115, train\n",
      " similarity:0.0385327\n",
      " penlog:-1.0801170\n",
      "Metrics Epoch 115, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:95.4782609\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:15.2800000\n",
      " batch_invalid_valency_nodes:45.2173913\n",
      " batch_nodes_0degree:10.3800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-15.2800000\n",
      " batch_node_degree:0.8417391\n",
      "Logits [18.933574676513672, 1.8633038997650146, 51.564735412597656]\n",
      "Epoch duration: 2.089372396469116\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_115\n",
      "Epoch: 116\n",
      "FGW torch.Size([29508, 5]) 9.68333042692393e-05\n",
      "Penalty params: tau=0.95115 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=116 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 116, train\n",
      " fgw:0.3424237\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3424237\n",
      "Measure Epoch 116, train\n",
      " similarity:0.0401717\n",
      " penlog:1.0653522\n",
      "Metrics Epoch 116, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:95.4782609\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:14.8200000\n",
      " batch_invalid_valency_nodes:42.3478261\n",
      " batch_nodes_0degree:9.7400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-14.8200000\n",
      " batch_node_degree:0.8730435\n",
      "Logits [18.991308212280273, 1.8648953437805176, 51.614341735839844]\n",
      "Epoch duration: 2.0355136394500732\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_116\n",
      "Epoch: 117\n",
      "FGW torch.Size([29508, 5]) 9.837718243943527e-05\n",
      "Penalty params: tau=0.95074 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=117 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 117, train\n",
      " fgw:0.3441547\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3441547\n",
      "Measure Epoch 117, train\n",
      " similarity:0.0428847\n",
      " penlog:-0.7629083\n",
      "Metrics Epoch 117, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:95.6521739\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:14.3000000\n",
      " batch_invalid_valency_nodes:41.1304348\n",
      " batch_nodes_0degree:9.4400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-14.2800000\n",
      " batch_node_degree:0.9217391\n",
      "Logits [18.986207962036133, 1.8701956272125244, 51.619537353515625]\n",
      "Epoch duration: 2.021684169769287\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_117\n",
      "Epoch: 118\n",
      "FGW torch.Size([29508, 5]) 9.645821410231292e-05\n",
      "Penalty params: tau=0.95033 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=118 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 118, train\n",
      " fgw:0.3397299\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3397299\n",
      "Measure Epoch 118, train\n",
      " similarity:0.0435288\n",
      " penlog:-0.7203674\n",
      "Metrics Epoch 118, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:95.3913043\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:14.5400000\n",
      " batch_invalid_valency_nodes:42.2608696\n",
      " batch_nodes_0degree:9.7000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-14.5200000\n",
      " batch_node_degree:0.9078261\n",
      "Logits [19.089324951171875, 1.8724000453948975, 51.65794372558594]\n",
      "Epoch duration: 1.964162826538086\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 119\n",
      "FGW torch.Size([29508, 5]) 9.694613254396245e-05\n",
      "Penalty params: tau=0.94992 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=119 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 119, train\n",
      " fgw:0.3441141\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3441141\n",
      "Measure Epoch 119, train\n",
      " similarity:0.0400932\n",
      " penlog:1.0696995\n",
      "Metrics Epoch 119, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:95.5652174\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:14.7600000\n",
      " batch_invalid_valency_nodes:41.9130435\n",
      " batch_nodes_0degree:9.6400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-14.7600000\n",
      " batch_node_degree:0.9078261\n",
      "Logits [19.14662742614746, 1.8752632141113281, 51.69951248168945]\n",
      "Epoch duration: 2.0661847591400146\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_119\n",
      "Epoch: 120\n",
      "FGW torch.Size([29508, 5]) 9.633404260966927e-05\n",
      "Penalty params: tau=0.94951 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=120 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 120, train\n",
      " fgw:0.3440180\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3440180\n",
      "Measure Epoch 120, train\n",
      " similarity:0.0370698\n",
      " penlog:-1.3192292\n",
      "Metrics Epoch 120, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:95.6521739\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:14.6200000\n",
      " batch_invalid_valency_nodes:40.7826087\n",
      " batch_nodes_0degree:9.3600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-14.6200000\n",
      " batch_node_degree:0.9895652\n",
      "Logits [19.156147003173828, 1.8793491125106812, 51.69614791870117]\n",
      "Epoch duration: 1.9145197868347168\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_120\n",
      "Epoch: 121\n",
      "FGW torch.Size([29508, 5]) 9.579108882462606e-05\n",
      "Penalty params: tau=0.94910 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=121 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 121, train\n",
      " fgw:0.3396735\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3396735\n",
      "Measure Epoch 121, train\n",
      " similarity:0.0363578\n",
      " penlog:-3.3532389\n",
      "Metrics Epoch 121, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:95.2173913\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:14.6000000\n",
      " batch_invalid_valency_nodes:40.5217391\n",
      " batch_nodes_0degree:9.2800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-14.6000000\n",
      " batch_node_degree:0.9913043\n",
      "Logits [19.265962600708008, 1.8747258186340332, 51.779029846191406]\n",
      "Epoch duration: 2.030407667160034\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_121\n",
      "Epoch: 122\n",
      "FGW torch.Size([29508, 5]) 9.736089850775898e-05\n",
      "Penalty params: tau=0.94869 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=122 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 122, train\n",
      " fgw:0.3385475\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3385475\n",
      "Measure Epoch 122, train\n",
      " similarity:0.0367832\n",
      " penlog:-1.1359754\n",
      "Metrics Epoch 122, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:95.6521739\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:14.4800000\n",
      " batch_invalid_valency_nodes:39.0434783\n",
      " batch_nodes_0degree:8.9600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-14.4600000\n",
      " batch_node_degree:1.0017391\n",
      "Logits [19.283061981201172, 1.8836568593978882, 51.77013397216797]\n",
      "Epoch duration: 2.0069754123687744\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_122\n",
      "Epoch: 123\n",
      "FGW torch.Size([29508, 5]) 9.705541015136987e-05\n",
      "Penalty params: tau=0.94828 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=123 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 123, train\n",
      " fgw:0.3382154\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3382154\n",
      "Measure Epoch 123, train\n",
      " similarity:0.0374264\n",
      " penlog:0.8353649\n",
      "Metrics Epoch 123, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:95.6521739\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:14.3800000\n",
      " batch_invalid_valency_nodes:38.6086957\n",
      " batch_nodes_0degree:8.8800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-14.3600000\n",
      " batch_node_degree:1.0069565\n",
      "Logits [19.39314842224121, 1.889404296875, 51.764102935791016]\n",
      "Epoch duration: 1.957979679107666\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_123\n",
      "Epoch: 124\n",
      "FGW torch.Size([29508, 5]) 9.800270345294848e-05\n",
      "Penalty params: tau=0.94787 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=124 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 124, train\n",
      " fgw:0.3449023\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3449023\n",
      "Measure Epoch 124, train\n",
      " similarity:0.0383823\n",
      " penlog:0.8136524\n",
      "Metrics Epoch 124, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:95.4782609\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:14.5600000\n",
      " batch_invalid_valency_nodes:39.4782609\n",
      " batch_nodes_0degree:9.0800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-14.5400000\n",
      " batch_node_degree:0.9808696\n",
      "Logits [19.502531051635742, 1.8942607641220093, 51.75128173828125]\n",
      "Epoch duration: 2.0219075679779053\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_124\n",
      "Epoch: 125\n",
      "FGW torch.Size([29508, 5]) 9.714013140182942e-05\n",
      "Penalty params: tau=0.94746 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=125 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 125, train\n",
      " fgw:0.3352620\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3352620\n",
      "Measure Epoch 125, train\n",
      " similarity:0.0380768\n",
      " penlog:0.8127783\n",
      "Metrics Epoch 125, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:95.6521739\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:14.6800000\n",
      " batch_invalid_valency_nodes:40.1739130\n",
      " batch_nodes_0degree:9.2400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-14.6800000\n",
      " batch_node_degree:0.9565217\n",
      "Logits [19.519603729248047, 1.8991674184799194, 51.74871826171875]\n",
      "Epoch duration: 1.9161319732666016\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_125\n",
      "Epoch: 126\n",
      "FGW torch.Size([29508, 5]) 9.727034921525046e-05\n",
      "Penalty params: tau=0.94705 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=126 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 126, train\n",
      " fgw:0.3377090\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3377090\n",
      "Measure Epoch 126, train\n",
      " similarity:0.0385583\n",
      " penlog:0.8700509\n",
      "Metrics Epoch 126, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:95.9130435\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:14.6400000\n",
      " batch_invalid_valency_nodes:39.8260870\n",
      " batch_nodes_0degree:9.1600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-14.6400000\n",
      " batch_node_degree:0.9478261\n",
      "Logits [19.547718048095703, 1.9007350206375122, 51.80568313598633]\n",
      "Epoch duration: 1.9804837703704834\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_126\n",
      "Epoch: 127\n",
      "FGW torch.Size([29508, 5]) 9.655507892603055e-05\n",
      "Penalty params: tau=0.94664 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=127 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 127, train\n",
      " fgw:0.3345498\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3345498\n",
      "Measure Epoch 127, train\n",
      " similarity:0.0381505\n",
      " penlog:0.9265840\n",
      "Metrics Epoch 127, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:95.2173913\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:14.8000000\n",
      " batch_invalid_valency_nodes:41.0434783\n",
      " batch_nodes_0degree:9.4400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-14.8000000\n",
      " batch_node_degree:0.9286957\n",
      "Logits [19.70175552368164, 1.902679681777954, 51.86292266845703]\n",
      "Epoch duration: 1.8799171447753906\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 128\n",
      "FGW torch.Size([29508, 5]) 9.68915774137713e-05\n",
      "Penalty params: tau=0.94623 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=128 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 128, train\n",
      " fgw:0.3334887\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3334887\n",
      "Measure Epoch 128, train\n",
      " similarity:0.0380670\n",
      " penlog:0.9436968\n",
      "Metrics Epoch 128, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:96.4347826\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:14.6000000\n",
      " batch_invalid_valency_nodes:39.4782609\n",
      " batch_nodes_0degree:9.0800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-14.6000000\n",
      " batch_node_degree:0.9756522\n",
      "Logits [19.63658332824707, 1.9192157983779907, 51.74970245361328]\n",
      "Epoch duration: 1.9799792766571045\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_128\n",
      "Epoch: 129\n",
      "FGW torch.Size([29508, 5]) 9.688159479992464e-05\n",
      "Penalty params: tau=0.94583 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=129 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 129, train\n",
      " fgw:0.3307150\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3307150\n",
      "Measure Epoch 129, train\n",
      " similarity:0.0389919\n",
      " penlog:-1.3077266\n",
      "Metrics Epoch 129, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:95.5652174\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:14.2600000\n",
      " batch_invalid_valency_nodes:38.3478261\n",
      " batch_nodes_0degree:8.8000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-14.2600000\n",
      " batch_node_degree:1.0121739\n",
      "Logits [19.823516845703125, 1.9120278358459473, 51.8720817565918]\n",
      "Epoch duration: 1.8153965473175049\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_129\n",
      "Epoch: 130\n",
      "FGW torch.Size([29508, 5]) 9.640312055125833e-05\n",
      "Penalty params: tau=0.94542 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=130 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 130, train\n",
      " fgw:0.3311078\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3311078\n",
      "Measure Epoch 130, train\n",
      " similarity:0.0377507\n",
      " penlog:-3.3218721\n",
      "Metrics Epoch 130, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:96.0000000\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.8200000\n",
      " batch_invalid_valency_nodes:35.8260870\n",
      " batch_nodes_0degree:8.2000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.8200000\n",
      " batch_node_degree:1.0817391\n",
      "Logits [19.837312698364258, 1.9193681478500366, 51.890254974365234]\n",
      "Epoch duration: 2.159533739089966\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_130\n",
      "Epoch: 131\n",
      "FGW torch.Size([29508, 5]) 9.669104474596679e-05\n",
      "Penalty params: tau=0.94501 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=131 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 131, train\n",
      " fgw:0.3302074\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3302074\n",
      "Measure Epoch 131, train\n",
      " similarity:0.0370370\n",
      " penlog:-3.3431489\n",
      "Metrics Epoch 131, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:96.3478261\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.5600000\n",
      " batch_invalid_valency_nodes:35.1304348\n",
      " batch_nodes_0degree:8.0400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.5600000\n",
      " batch_node_degree:1.1321739\n",
      "Logits [19.858457565307617, 1.9324754476547241, 51.89067459106445]\n",
      "Epoch duration: 2.0399491786956787\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_131\n",
      "Epoch: 132\n",
      "FGW torch.Size([29508, 5]) 9.538268204778433e-05\n",
      "Penalty params: tau=0.94460 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=132 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 132, train\n",
      " fgw:0.3246734\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3246734\n",
      "Measure Epoch 132, train\n",
      " similarity:0.0390725\n",
      " penlog:-3.2978456\n",
      "Metrics Epoch 132, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:95.8260870\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.7600000\n",
      " batch_invalid_valency_nodes:37.2173913\n",
      " batch_nodes_0degree:8.5200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.7600000\n",
      " batch_node_degree:1.1113043\n",
      "Logits [20.054738998413086, 1.9320794343948364, 52.05506134033203]\n",
      "Epoch duration: 1.9721860885620117\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_132\n",
      "Epoch: 133\n",
      "FGW torch.Size([29508, 5]) 9.481386950938031e-05\n",
      "Penalty params: tau=0.94419 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=133 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 133, train\n",
      " fgw:0.3250309\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3250309\n",
      "Measure Epoch 133, train\n",
      " similarity:0.0439289\n",
      " penlog:-3.3046864\n",
      "Metrics Epoch 133, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:96.2608696\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.8600000\n",
      " batch_invalid_valency_nodes:38.9565217\n",
      " batch_nodes_0degree:8.9200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.8600000\n",
      " batch_node_degree:1.1043478\n",
      "Logits [20.057571411132812, 1.9436169862747192, 52.08005142211914]\n",
      "Epoch duration: 1.9441888332366943\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_133\n",
      "Epoch: 134\n",
      "FGW torch.Size([29508, 5]) 9.414798114448786e-05\n",
      "Penalty params: tau=0.94379 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=134 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 134, train\n",
      " fgw:0.3213854\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3213854\n",
      "Measure Epoch 134, train\n",
      " similarity:0.0453931\n",
      " penlog:-3.4003257\n",
      "Metrics Epoch 134, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:95.9130435\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.7400000\n",
      " batch_invalid_valency_nodes:38.2608696\n",
      " batch_nodes_0degree:8.7600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.7400000\n",
      " batch_node_degree:1.1252174\n",
      "Logits [20.229780197143555, 1.9534635543823242, 52.17643737792969]\n",
      "Epoch duration: 1.9032888412475586\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_134\n",
      "Epoch: 135\n",
      "FGW torch.Size([29508, 5]) 9.489928197581321e-05\n",
      "Penalty params: tau=0.94338 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=135 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 135, train\n",
      " fgw:0.3235691\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3235691\n",
      "Measure Epoch 135, train\n",
      " similarity:0.0442665\n",
      " penlog:-5.5168442\n",
      "Metrics Epoch 135, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:95.8260870\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:13.6400000\n",
      " batch_invalid_valency_nodes:36.8695652\n",
      " batch_nodes_0degree:8.4200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.6200000\n",
      " batch_node_degree:1.1686957\n",
      "Logits [20.452804565429688, 1.970582127571106, 52.21299362182617]\n",
      "Epoch duration: 2.0907483100891113\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_135\n",
      "Epoch: 136\n",
      "FGW torch.Size([29508, 5]) 9.544990462018177e-05\n",
      "Penalty params: tau=0.94297 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=136 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 136, train\n",
      " fgw:0.3249526\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3249526\n",
      "Measure Epoch 136, train\n",
      " similarity:0.0442053\n",
      " penlog:-7.3912966\n",
      "Metrics Epoch 136, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:96.6956522\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:13.2200000\n",
      " batch_invalid_valency_nodes:34.4347826\n",
      " batch_nodes_0degree:7.8400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.2200000\n",
      " batch_node_degree:1.2347826\n",
      "Logits [20.467239379882812, 1.9894366264343262, 52.14613723754883]\n",
      "Epoch duration: 2.0350067615509033\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 137\n",
      "FGW torch.Size([29508, 5]) 9.519679588265717e-05\n",
      "Penalty params: tau=0.94256 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=137 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 137, train\n",
      " fgw:0.3231057\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3231057\n",
      "Measure Epoch 137, train\n",
      " similarity:0.0437887\n",
      " penlog:-5.5026817\n",
      "Metrics Epoch 137, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:96.2608696\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:13.6400000\n",
      " batch_invalid_valency_nodes:36.0869565\n",
      " batch_nodes_0degree:8.2400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.6400000\n",
      " batch_node_degree:1.1652174\n",
      "Logits [20.71532440185547, 1.983086347579956, 52.38121032714844]\n",
      "Epoch duration: 2.0713412761688232\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_137\n",
      "Epoch: 138\n",
      "FGW torch.Size([29508, 5]) 9.534528362564743e-05\n",
      "Penalty params: tau=0.94216 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=138 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 138, train\n",
      " fgw:0.3208106\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3208106\n",
      "Measure Epoch 138, train\n",
      " similarity:0.0417006\n",
      " penlog:-3.4672839\n",
      "Metrics Epoch 138, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:96.8695652\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.5600000\n",
      " batch_invalid_valency_nodes:34.6086957\n",
      " batch_nodes_0degree:7.9200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.5600000\n",
      " batch_node_degree:1.2034783\n",
      "Logits [20.811677932739258, 1.9978935718536377, 52.37907409667969]\n",
      "Epoch duration: 1.9597959518432617\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_138\n",
      "Epoch: 139\n",
      "FGW torch.Size([29508, 5]) 9.523604239802808e-05\n",
      "Penalty params: tau=0.94175 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=139 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 139, train\n",
      " fgw:0.3209599\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3209599\n",
      "Measure Epoch 139, train\n",
      " similarity:0.0390244\n",
      " penlog:-3.4107232\n",
      "Metrics Epoch 139, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:96.8695652\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.5400000\n",
      " batch_invalid_valency_nodes:33.2173913\n",
      " batch_nodes_0degree:7.6000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.5400000\n",
      " batch_node_degree:1.2260870\n",
      "Logits [20.928808212280273, 2.0146188735961914, 52.4034538269043]\n",
      "Epoch duration: 1.9807679653167725\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_139\n",
      "Epoch: 140\n",
      "FGW torch.Size([29508, 5]) 9.503761975793168e-05\n",
      "Penalty params: tau=0.94134 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=140 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 140, train\n",
      " fgw:0.3209348\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3209348\n",
      "Measure Epoch 140, train\n",
      " similarity:0.0411776\n",
      " penlog:-3.3037354\n",
      "Metrics Epoch 140, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:96.8695652\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.9000000\n",
      " batch_invalid_valency_nodes:34.8695652\n",
      " batch_nodes_0degree:7.9800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.9000000\n",
      " batch_node_degree:1.1756522\n",
      "Logits [21.050048828125, 2.0196642875671387, 52.53364562988281]\n",
      "Epoch duration: 2.1266090869903564\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_140\n",
      "Epoch: 141\n",
      "FGW torch.Size([29508, 5]) 9.51166293816641e-05\n",
      "Penalty params: tau=0.94094 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=141 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 141, train\n",
      " fgw:0.3190835\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3190835\n",
      "Measure Epoch 141, train\n",
      " similarity:0.0455575\n",
      " penlog:-5.5112625\n",
      "Metrics Epoch 141, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:96.6956522\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:13.3600000\n",
      " batch_invalid_valency_nodes:32.2608696\n",
      " batch_nodes_0degree:7.3600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.3600000\n",
      " batch_node_degree:1.2365217\n",
      "Logits [21.01473617553711, 2.027033805847168, 52.587745666503906]\n",
      "Epoch duration: 2.172874689102173\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_141\n",
      "Epoch: 142\n",
      "FGW torch.Size([29508, 5]) 9.54316055867821e-05\n",
      "Penalty params: tau=0.94053 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=142 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 142, train\n",
      " fgw:0.3161660\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3161660\n",
      "Measure Epoch 142, train\n",
      " similarity:0.0426184\n",
      " penlog:-5.5158152\n",
      "Metrics Epoch 142, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:96.3478261\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:13.2200000\n",
      " batch_invalid_valency_nodes:31.4782609\n",
      " batch_nodes_0degree:7.1800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.2200000\n",
      " batch_node_degree:1.2747826\n",
      "Logits [21.113494873046875, 2.0377964973449707, 52.62918472290039]\n",
      "Epoch duration: 2.0363779067993164\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_142\n",
      "Epoch: 143\n",
      "FGW torch.Size([29508, 5]) 9.496686834609136e-05\n",
      "Penalty params: tau=0.94013 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=143 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 143, train\n",
      " fgw:0.3144070\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3144070\n",
      "Measure Epoch 143, train\n",
      " similarity:0.0395881\n",
      " penlog:-9.6342576\n",
      "Metrics Epoch 143, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:96.4347826\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:13.0600000\n",
      " batch_invalid_valency_nodes:31.2173913\n",
      " batch_nodes_0degree:7.0600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.0600000\n",
      " batch_node_degree:1.3060870\n",
      "Logits [21.23951530456543, 2.0472116470336914, 52.6836051940918]\n",
      "Epoch duration: 1.985241174697876\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_143\n",
      "Epoch: 144\n",
      "FGW torch.Size([29508, 5]) 9.348843013867736e-05\n",
      "Penalty params: tau=0.93972 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=144 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 144, train\n",
      " fgw:0.3126757\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3126757\n",
      "Measure Epoch 144, train\n",
      " similarity:0.0393941\n",
      " penlog:-7.6612130\n",
      "Metrics Epoch 144, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:96.7826087\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:13.0000000\n",
      " batch_invalid_valency_nodes:30.6956522\n",
      " batch_nodes_0degree:6.9400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.0000000\n",
      " batch_node_degree:1.3165217\n",
      "Logits [21.30436134338379, 2.055048942565918, 52.73076248168945]\n",
      "Epoch duration: 2.1932430267333984\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_144\n",
      "Epoch: 145\n",
      "FGW torch.Size([29508, 5]) 9.415628301212564e-05\n",
      "Penalty params: tau=0.93931 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=145 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 145, train\n",
      " fgw:0.3158635\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3158635\n",
      "Measure Epoch 145, train\n",
      " similarity:0.0402893\n",
      " penlog:-5.3596891\n",
      "Metrics Epoch 145, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:96.3478261\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:13.7000000\n",
      " batch_invalid_valency_nodes:32.9565217\n",
      " batch_nodes_0degree:7.5000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.7000000\n",
      " batch_node_degree:1.2208696\n",
      "Logits [21.582326889038086, 2.060385227203369, 52.777549743652344]\n",
      "Epoch duration: 1.971397876739502\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 146\n",
      "FGW torch.Size([29508, 5]) 9.40915706451051e-05\n",
      "Penalty params: tau=0.93891 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=146 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 146, train\n",
      " fgw:0.3121161\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3121161\n",
      "Measure Epoch 146, train\n",
      " similarity:0.0407715\n",
      " penlog:-5.7485619\n",
      "Metrics Epoch 146, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:96.7826087\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:13.0800000\n",
      " batch_invalid_valency_nodes:31.1304348\n",
      " batch_nodes_0degree:7.1000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.0800000\n",
      " batch_node_degree:1.3182609\n",
      "Logits [21.547792434692383, 2.080974578857422, 52.620155334472656]\n",
      "Epoch duration: 2.0539801120758057\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_146\n",
      "Epoch: 147\n",
      "FGW torch.Size([29508, 5]) 9.500051965005696e-05\n",
      "Penalty params: tau=0.93850 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=147 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 147, train\n",
      " fgw:0.3144232\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3144232\n",
      "Measure Epoch 147, train\n",
      " similarity:0.0418036\n",
      " penlog:-3.4741531\n",
      "Metrics Epoch 147, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:96.8695652\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.4800000\n",
      " batch_invalid_valency_nodes:32.3478261\n",
      " batch_nodes_0degree:7.4000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.4800000\n",
      " batch_node_degree:1.2486957\n",
      "Logits [21.778446197509766, 2.0876097679138184, 52.727020263671875]\n",
      "Epoch duration: 2.072122573852539\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_147\n",
      "Epoch: 148\n",
      "FGW torch.Size([29508, 5]) 9.451121150050312e-05\n",
      "Penalty params: tau=0.93810 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=148 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 148, train\n",
      " fgw:0.3066317\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3066317\n",
      "Measure Epoch 148, train\n",
      " similarity:0.0446071\n",
      " penlog:-3.4055398\n",
      "Metrics Epoch 148, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.0434783\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.5200000\n",
      " batch_invalid_valency_nodes:32.5217391\n",
      " batch_nodes_0degree:7.4400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.5200000\n",
      " batch_node_degree:1.2365217\n",
      "Logits [21.799833297729492, 2.0998244285583496, 52.74818420410156]\n",
      "Epoch duration: 2.305863380432129\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_148\n",
      "Epoch: 149\n",
      "FGW torch.Size([29508, 5]) 9.417926048627123e-05\n",
      "Penalty params: tau=0.93769 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=149 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 149, train\n",
      " fgw:0.3073887\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3073887\n",
      "Measure Epoch 149, train\n",
      " similarity:0.0443983\n",
      " penlog:-3.4919256\n",
      "Metrics Epoch 149, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.3043478\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.2800000\n",
      " batch_invalid_valency_nodes:30.3478261\n",
      " batch_nodes_0degree:6.9400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.2800000\n",
      " batch_node_degree:1.2765217\n",
      "Logits [21.771739959716797, 2.1047661304473877, 52.85316848754883]\n",
      "Epoch duration: 1.9735469818115234\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_149\n",
      "Epoch: 150\n",
      "FGW torch.Size([29508, 5]) 9.219840285368264e-05\n",
      "Penalty params: tau=0.93729 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=150 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 150, train\n",
      " fgw:0.3083416\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3083416\n",
      "Measure Epoch 150, train\n",
      " similarity:0.0450494\n",
      " penlog:-5.4047821\n",
      "Metrics Epoch 150, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.1304348\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:13.3600000\n",
      " batch_invalid_valency_nodes:29.0434783\n",
      " batch_nodes_0degree:6.6200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.3600000\n",
      " batch_node_degree:1.2939130\n",
      "Logits [21.996444702148438, 2.1090242862701416, 53.04014587402344]\n",
      "Epoch duration: 2.1641347408294678\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_150\n",
      "Epoch: 151\n",
      "FGW torch.Size([29508, 5]) 9.248993592336774e-05\n",
      "Penalty params: tau=0.93688 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=151 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 151, train\n",
      " fgw:0.3070768\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3070768\n",
      "Measure Epoch 151, train\n",
      " similarity:0.0440940\n",
      " penlog:-11.5918151\n",
      "Metrics Epoch 151, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:96.9565217\n",
      " batch_molecular_disconnected_validity:88.0000000\n",
      " batch_connected_components:13.0200000\n",
      " batch_invalid_valency_nodes:28.4347826\n",
      " batch_nodes_0degree:6.4200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.0200000\n",
      " batch_node_degree:1.3565217\n",
      "Logits [22.04405975341797, 2.1257567405700684, 53.020042419433594]\n",
      "Epoch duration: 1.992865800857544\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_151\n",
      "Epoch: 152\n",
      "FGW torch.Size([29508, 5]) 9.345328726340085e-05\n",
      "Penalty params: tau=0.93648 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=152 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 152, train\n",
      " fgw:0.3032843\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3032843\n",
      "Measure Epoch 152, train\n",
      " similarity:0.0459854\n",
      " penlog:-11.6164928\n",
      "Metrics Epoch 152, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.2173913\n",
      " batch_molecular_disconnected_validity:88.0000000\n",
      " batch_connected_components:12.8000000\n",
      " batch_invalid_valency_nodes:27.0434783\n",
      " batch_nodes_0degree:6.1000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.8000000\n",
      " batch_node_degree:1.3947826\n",
      "Logits [22.118236541748047, 2.1360135078430176, 53.017513275146484]\n",
      "Epoch duration: 2.1074652671813965\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_152\n",
      "Epoch: 153\n",
      "FGW torch.Size([29508, 5]) 9.506366768619046e-05\n",
      "Penalty params: tau=0.93608 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=153 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 153, train\n",
      " fgw:0.3063738\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3063738\n",
      "Measure Epoch 153, train\n",
      " similarity:0.0491679\n",
      " penlog:-5.5216468\n",
      "Metrics Epoch 153, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.2173913\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:12.9200000\n",
      " batch_invalid_valency_nodes:28.4347826\n",
      " batch_nodes_0degree:6.4400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.9200000\n",
      " batch_node_degree:1.3843478\n",
      "Logits [22.242412567138672, 2.144362688064575, 53.03382110595703]\n",
      "Epoch duration: 2.1317174434661865\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_153\n",
      "Epoch: 154\n",
      "FGW torch.Size([29508, 5]) 9.556468285154551e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penalty params: tau=0.93567 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=154 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 154, train\n",
      " fgw:0.3100258\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3100258\n",
      "Measure Epoch 154, train\n",
      " similarity:0.0475343\n",
      " penlog:-3.6012408\n",
      "Metrics Epoch 154, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:96.8695652\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.3200000\n",
      " batch_invalid_valency_nodes:30.6086957\n",
      " batch_nodes_0degree:6.9800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.3200000\n",
      " batch_node_degree:1.3356522\n",
      "Logits [22.250139236450195, 2.1477248668670654, 53.09438705444336]\n",
      "Epoch duration: 2.1287128925323486\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_154\n",
      "Epoch: 155\n",
      "FGW torch.Size([29508, 5]) 9.407189645571634e-05\n",
      "Penalty params: tau=0.93527 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=155 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 155, train\n",
      " fgw:0.3065868\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3065868\n",
      "Measure Epoch 155, train\n",
      " similarity:0.0511793\n",
      " penlog:-3.4492516\n",
      "Metrics Epoch 155, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.0434783\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.5400000\n",
      " batch_invalid_valency_nodes:31.3043478\n",
      " batch_nodes_0degree:7.1400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.5400000\n",
      " batch_node_degree:1.2991304\n",
      "Logits [22.505537033081055, 2.158297061920166, 53.150970458984375]\n",
      "Epoch duration: 2.0221545696258545\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_155\n",
      "Epoch: 156\n",
      "FGW torch.Size([29508, 5]) 9.369855979457498e-05\n",
      "Penalty params: tau=0.93486 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=156 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 156, train\n",
      " fgw:0.3072108\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3072108\n",
      "Measure Epoch 156, train\n",
      " similarity:0.0471864\n",
      " penlog:-3.3996412\n",
      "Metrics Epoch 156, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.0434783\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:14.1400000\n",
      " batch_invalid_valency_nodes:34.9565217\n",
      " batch_nodes_0degree:8.0000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-14.1400000\n",
      " batch_node_degree:1.2208696\n",
      "Logits [22.46980857849121, 2.1696279048919678, 53.11770248413086]\n",
      "Epoch duration: 1.9888746738433838\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_156\n",
      "Epoch: 157\n",
      "FGW torch.Size([29508, 5]) 9.379436960443854e-05\n",
      "Penalty params: tau=0.93446 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=157 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 157, train\n",
      " fgw:0.3051357\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3051357\n",
      "Measure Epoch 157, train\n",
      " similarity:0.0514720\n",
      " penlog:-3.5167563\n",
      "Metrics Epoch 157, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.2173913\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.8800000\n",
      " batch_invalid_valency_nodes:33.6521739\n",
      " batch_nodes_0degree:7.7000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.8800000\n",
      " batch_node_degree:1.2486957\n",
      "Logits [22.602109909057617, 2.167965888977051, 53.22506332397461]\n",
      "Epoch duration: 2.1526496410369873\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_157\n",
      "Epoch: 158\n",
      "FGW torch.Size([29508, 5]) 9.465180482948199e-05\n",
      "Penalty params: tau=0.93406 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=158 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 158, train\n",
      " fgw:0.3040193\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3040193\n",
      "Measure Epoch 158, train\n",
      " similarity:0.0533480\n",
      " penlog:-3.5831674\n",
      "Metrics Epoch 158, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.3913043\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.4600000\n",
      " batch_invalid_valency_nodes:31.4782609\n",
      " batch_nodes_0degree:7.2000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.4600000\n",
      " batch_node_degree:1.3130435\n",
      "Logits [22.692737579345703, 2.1856131553649902, 53.17761993408203]\n",
      "Epoch duration: 2.0225841999053955\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_158\n",
      "Epoch: 159\n",
      "FGW torch.Size([29508, 5]) 9.544573549646884e-05\n",
      "Penalty params: tau=0.93365 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=159 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 159, train\n",
      " fgw:0.3051092\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3051092\n",
      "Measure Epoch 159, train\n",
      " similarity:0.0491737\n",
      " penlog:-3.5607330\n",
      "Metrics Epoch 159, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.4782609\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.6400000\n",
      " batch_invalid_valency_nodes:31.4782609\n",
      " batch_nodes_0degree:7.2000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.6400000\n",
      " batch_node_degree:1.3095652\n",
      "Logits [22.75691795349121, 2.1995749473571777, 53.18231201171875]\n",
      "Epoch duration: 2.0520715713500977\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_159\n",
      "Epoch: 160\n",
      "FGW torch.Size([29508, 5]) 9.429962665308267e-05\n",
      "Penalty params: tau=0.93325 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=160 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 160, train\n",
      " fgw:0.3023761\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3023761\n",
      "Measure Epoch 160, train\n",
      " similarity:0.0460332\n",
      " penlog:-5.4548243\n",
      "Metrics Epoch 160, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.2173913\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:13.9800000\n",
      " batch_invalid_valency_nodes:31.9130435\n",
      " batch_nodes_0degree:7.2800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.9800000\n",
      " batch_node_degree:1.2747826\n",
      "Logits [22.90602684020996, 2.202824354171753, 53.29793930053711]\n",
      "Epoch duration: 1.8945763111114502\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_160\n",
      "Epoch: 161\n",
      "FGW torch.Size([29508, 5]) 9.276151831727475e-05\n",
      "Penalty params: tau=0.93285 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=161 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 161, train\n",
      " fgw:0.3033717\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3033717\n",
      "Measure Epoch 161, train\n",
      " similarity:0.0483661\n",
      " penlog:-1.5092069\n",
      "Metrics Epoch 161, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.1304348\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:14.2600000\n",
      " batch_invalid_valency_nodes:33.0434783\n",
      " batch_nodes_0degree:7.5800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-14.2600000\n",
      " batch_node_degree:1.2313043\n",
      "Logits [22.896888732910156, 2.2082817554473877, 53.34974670410156]\n",
      "Epoch duration: 2.249124765396118\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_161\n",
      "Epoch: 162\n",
      "FGW torch.Size([29508, 5]) 9.252311429008842e-05\n",
      "Penalty params: tau=0.93244 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=162 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 162, train\n",
      " fgw:0.3013051\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3013051\n",
      "Measure Epoch 162, train\n",
      " similarity:0.0472519\n",
      " penlog:-1.4280930\n",
      "Metrics Epoch 162, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.3913043\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:14.0600000\n",
      " batch_invalid_valency_nodes:31.8260870\n",
      " batch_nodes_0degree:7.3000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-14.0600000\n",
      " batch_node_degree:1.2626087\n",
      "Logits [23.08892059326172, 2.2252237796783447, 53.3697509765625]\n",
      "Epoch duration: 2.062055826187134\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 163\n",
      "FGW torch.Size([29508, 5]) 9.386665624333546e-05\n",
      "Penalty params: tau=0.93204 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=163 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 163, train\n",
      " fgw:0.2987600\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2987600\n",
      "Measure Epoch 163, train\n",
      " similarity:0.0442948\n",
      " penlog:-7.6275809\n",
      "Metrics Epoch 163, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.4782609\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:13.0600000\n",
      " batch_invalid_valency_nodes:28.6956522\n",
      " batch_nodes_0degree:6.5200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.0600000\n",
      " batch_node_degree:1.3773913\n",
      "Logits [23.12800407409668, 2.23740816116333, 53.3393440246582]\n",
      "Epoch duration: 2.142184257507324\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_163\n",
      "Epoch: 164\n",
      "FGW torch.Size([29508, 5]) 9.50262910919264e-05\n",
      "Penalty params: tau=0.93164 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=164 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 164, train\n",
      " fgw:0.3040275\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3040275\n",
      "Measure Epoch 164, train\n",
      " similarity:0.0402397\n",
      " penlog:-15.9246920\n",
      "Metrics Epoch 164, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.3913043\n",
      " batch_molecular_disconnected_validity:84.0000000\n",
      " batch_connected_components:12.4200000\n",
      " batch_invalid_valency_nodes:27.5652174\n",
      " batch_nodes_0degree:6.1800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.4200000\n",
      " batch_node_degree:1.4486957\n",
      "Logits [23.143781661987305, 2.2392117977142334, 53.442718505859375]\n",
      "Epoch duration: 2.0129032135009766\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_164\n",
      "Epoch: 165\n",
      "FGW torch.Size([29508, 5]) 9.321929974248633e-05\n",
      "Penalty params: tau=0.93124 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=165 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 165, train\n",
      " fgw:0.3002571\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3002571\n",
      "Measure Epoch 165, train\n",
      " similarity:0.0422508\n",
      " penlog:-9.8940748\n",
      "Metrics Epoch 165, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.3043478\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:12.2400000\n",
      " batch_invalid_valency_nodes:26.4347826\n",
      " batch_nodes_0degree:5.9800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.2400000\n",
      " batch_node_degree:1.4678261\n",
      "Logits [23.293588638305664, 2.257718086242676, 53.52141571044922]\n",
      "Epoch duration: 2.1174352169036865\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_165\n",
      "Epoch: 166\n",
      "FGW torch.Size([29508, 5]) 9.21514438232407e-05\n",
      "Penalty params: tau=0.93084 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=166 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 166, train\n",
      " fgw:0.2970350\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2970350\n",
      "Measure Epoch 166, train\n",
      " similarity:0.0440672\n",
      " penlog:-5.8073160\n",
      "Metrics Epoch 166, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.3043478\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:12.5200000\n",
      " batch_invalid_valency_nodes:26.1739130\n",
      " batch_nodes_0degree:5.9600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.5200000\n",
      " batch_node_degree:1.4400000\n",
      "Logits [23.381973266601562, 2.2636654376983643, 53.6937141418457]\n",
      "Epoch duration: 1.9588661193847656\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_166\n",
      "Epoch: 167\n",
      "FGW torch.Size([29508, 5]) 9.10144517547451e-05\n",
      "Penalty params: tau=0.93043 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=167 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 167, train\n",
      " fgw:0.2966521\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2966521\n",
      "Measure Epoch 167, train\n",
      " similarity:0.0471890\n",
      " penlog:-5.7101436\n",
      "Metrics Epoch 167, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.3043478\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:12.5400000\n",
      " batch_invalid_valency_nodes:24.6956522\n",
      " batch_nodes_0degree:5.6200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.5400000\n",
      " batch_node_degree:1.4521739\n",
      "Logits [23.59540557861328, 2.276522636413574, 53.8244743347168]\n",
      "Epoch duration: 2.012458324432373\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_167\n",
      "Epoch: 168\n",
      "FGW torch.Size([29508, 5]) 9.21622195164673e-05\n",
      "Penalty params: tau=0.93003 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=168 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 168, train\n",
      " fgw:0.2956810\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2956810\n",
      "Measure Epoch 168, train\n",
      " similarity:0.0456438\n",
      " penlog:-5.6788454\n",
      "Metrics Epoch 168, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.6521739\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:12.0200000\n",
      " batch_invalid_valency_nodes:21.0434783\n",
      " batch_nodes_0degree:4.7800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.0200000\n",
      " batch_node_degree:1.5391304\n",
      "Logits [23.679351806640625, 2.3014256954193115, 53.772159576416016]\n",
      "Epoch duration: 2.1050076484680176\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_168\n",
      "Epoch: 169\n",
      "FGW torch.Size([29508, 5]) 9.447295451536775e-05\n",
      "Penalty params: tau=0.92963 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=169 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 169, train\n",
      " fgw:0.2993113\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2993113\n",
      "Measure Epoch 169, train\n",
      " similarity:0.0442621\n",
      " penlog:-7.5645943\n",
      "Metrics Epoch 169, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.6521739\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:11.8400000\n",
      " batch_invalid_valency_nodes:19.3043478\n",
      " batch_nodes_0degree:4.3600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.8400000\n",
      " batch_node_degree:1.5600000\n",
      "Logits [23.85708999633789, 2.312397003173828, 53.89768600463867]\n",
      "Epoch duration: 1.9318954944610596\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_169\n",
      "Epoch: 170\n",
      "FGW torch.Size([29508, 5]) 9.259207581635565e-05\n",
      "Penalty params: tau=0.92923 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=170 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 170, train\n",
      " fgw:0.2955067\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2955067\n",
      "Measure Epoch 170, train\n",
      " similarity:0.0407627\n",
      " penlog:-5.4065217\n",
      "Metrics Epoch 170, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.6521739\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:12.9800000\n",
      " batch_invalid_valency_nodes:22.0869565\n",
      " batch_nodes_0degree:5.0200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.9800000\n",
      " batch_node_degree:1.4486957\n",
      "Logits [23.952768325805664, 2.3236870765686035, 53.983211517333984]\n",
      "Epoch duration: 1.9548561573028564\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_170\n",
      "Epoch: 171\n",
      "FGW torch.Size([29508, 5]) 9.130821854341775e-05\n",
      "Penalty params: tau=0.92883 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=171 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 171, train\n",
      " fgw:0.2954628\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2954628\n",
      "Measure Epoch 171, train\n",
      " similarity:0.0413530\n",
      " penlog:-3.2279476\n",
      "Metrics Epoch 171, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.3043478\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.4600000\n",
      " batch_invalid_valency_nodes:23.1304348\n",
      " batch_nodes_0degree:5.2800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.4600000\n",
      " batch_node_degree:1.4052174\n",
      "Logits [24.192121505737305, 2.339179039001465, 54.0741081237793]\n",
      "Epoch duration: 1.942906141281128\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 172\n",
      "FGW torch.Size([29508, 5]) 9.248105197912082e-05\n",
      "Penalty params: tau=0.92843 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=172 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 172, train\n",
      " fgw:0.2961667\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2961667\n",
      "Measure Epoch 172, train\n",
      " similarity:0.0412120\n",
      " penlog:-3.1461949\n",
      "Metrics Epoch 172, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.5652174\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.9200000\n",
      " batch_invalid_valency_nodes:25.9130435\n",
      " batch_nodes_0degree:5.9200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.9200000\n",
      " batch_node_degree:1.3408696\n",
      "Logits [24.12163734436035, 2.3445770740509033, 54.09394073486328]\n",
      "Epoch duration: 2.2867352962493896\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_172\n",
      "Epoch: 173\n",
      "FGW torch.Size([29508, 5]) 9.361032425658777e-05\n",
      "Penalty params: tau=0.92803 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=173 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 173, train\n",
      " fgw:0.2965067\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2965067\n",
      "Measure Epoch 173, train\n",
      " similarity:0.0405883\n",
      " penlog:-3.0664502\n",
      "Metrics Epoch 173, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.3913043\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.9200000\n",
      " batch_invalid_valency_nodes:26.1739130\n",
      " batch_nodes_0degree:5.9800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.9200000\n",
      " batch_node_degree:1.3339130\n",
      "Logits [24.40285301208496, 2.3584039211273193, 54.12577819824219]\n",
      "Epoch duration: 1.91300630569458\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_173\n",
      "Epoch: 174\n",
      "FGW torch.Size([29508, 5]) 9.409091580891982e-05\n",
      "Penalty params: tau=0.92763 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=174 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 174, train\n",
      " fgw:0.3003976\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3003976\n",
      "Measure Epoch 174, train\n",
      " similarity:0.0392626\n",
      " penlog:-1.2158354\n",
      "Metrics Epoch 174, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.0434783\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:13.8600000\n",
      " batch_invalid_valency_nodes:26.3478261\n",
      " batch_nodes_0degree:6.0400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.8600000\n",
      " batch_node_degree:1.3600000\n",
      "Logits [24.154216766357422, 2.369574546813965, 53.96173858642578]\n",
      "Epoch duration: 1.9641203880310059\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_174\n",
      "Epoch: 175\n",
      "FGW torch.Size([29508, 5]) 9.20797319849953e-05\n",
      "Penalty params: tau=0.92723 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=175 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 175, train\n",
      " fgw:0.2988660\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2988660\n",
      "Measure Epoch 175, train\n",
      " similarity:0.0388503\n",
      " penlog:-3.1744774\n",
      "Metrics Epoch 175, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:96.7826087\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.8200000\n",
      " batch_invalid_valency_nodes:24.4347826\n",
      " batch_nodes_0degree:5.5600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.8200000\n",
      " batch_node_degree:1.4034783\n",
      "Logits [24.554941177368164, 2.3619961738586426, 54.196231842041016]\n",
      "Epoch duration: 2.1733856201171875\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_175\n",
      "Epoch: 176\n",
      "FGW torch.Size([29508, 5]) 9.129907266469672e-05\n",
      "Penalty params: tau=0.92683 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=176 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 176, train\n",
      " fgw:0.2980974\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2980974\n",
      "Measure Epoch 176, train\n",
      " similarity:0.0355301\n",
      " penlog:-1.4417320\n",
      "Metrics Epoch 176, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.0434783\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:13.6600000\n",
      " batch_invalid_valency_nodes:24.9565217\n",
      " batch_nodes_0degree:5.7000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.6600000\n",
      " batch_node_degree:1.4295652\n",
      "Logits [24.277591705322266, 2.3764283657073975, 54.05666732788086]\n",
      "Epoch duration: 2.0371930599212646\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_176\n",
      "Epoch: 177\n",
      "FGW torch.Size([29508, 5]) 9.127220255322754e-05\n",
      "Penalty params: tau=0.92643 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=177 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 177, train\n",
      " fgw:0.2906542\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2906542\n",
      "Measure Epoch 177, train\n",
      " similarity:0.0371940\n",
      " penlog:-5.4410301\n",
      "Metrics Epoch 177, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.5652174\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:13.1400000\n",
      " batch_invalid_valency_nodes:22.6086957\n",
      " batch_nodes_0degree:5.1400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.1400000\n",
      " batch_node_degree:1.4678261\n",
      "Logits [24.58761215209961, 2.380481004714966, 54.26249313354492]\n",
      "Epoch duration: 2.2550246715545654\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_177\n",
      "Epoch: 178\n",
      "FGW torch.Size([29508, 5]) 9.287769353250042e-05\n",
      "Penalty params: tau=0.92603 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=178 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 178, train\n",
      " fgw:0.2919944\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2919944\n",
      "Measure Epoch 178, train\n",
      " similarity:0.0419891\n",
      " penlog:-5.6182537\n",
      "Metrics Epoch 178, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.6521739\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:12.3800000\n",
      " batch_invalid_valency_nodes:22.3478261\n",
      " batch_nodes_0degree:5.0600000\n",
      " batch_nodes_7plus_degree:0.0200000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.3800000\n",
      " batch_node_degree:1.5200000\n",
      "Logits [24.62989616394043, 2.3796958923339844, 54.4041862487793]\n",
      "Epoch duration: 2.110823154449463\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_178\n",
      "Epoch: 179\n",
      "FGW torch.Size([29508, 5]) 9.263773245038465e-05\n",
      "Penalty params: tau=0.92563 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=179 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 179, train\n",
      " fgw:0.2981336\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2981336\n",
      "Measure Epoch 179, train\n",
      " similarity:0.0364505\n",
      " penlog:-1.3784546\n",
      "Metrics Epoch 179, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.0434783\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:13.3200000\n",
      " batch_invalid_valency_nodes:23.5652174\n",
      " batch_nodes_0degree:5.4000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.3200000\n",
      " batch_node_degree:1.4313043\n",
      "Logits [24.47471809387207, 2.404543161392212, 54.21382522583008]\n",
      "Epoch duration: 1.9421188831329346\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_179\n",
      "Epoch: 180\n",
      "FGW torch.Size([29508, 5]) 9.106750803766772e-05\n",
      "Penalty params: tau=0.92523 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=180 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 180, train\n",
      " fgw:0.2908762\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2908762\n",
      "Measure Epoch 180, train\n",
      " similarity:0.0379014\n",
      " penlog:-3.2213130\n",
      "Metrics Epoch 180, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.2173913\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.5800000\n",
      " batch_invalid_valency_nodes:23.3043478\n",
      " batch_nodes_0degree:5.3200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.5800000\n",
      " batch_node_degree:1.4156522\n",
      "Logits [24.991613388061523, 2.410885810852051, 54.40983581542969]\n",
      "Epoch duration: 1.9714891910552979\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 181\n",
      "FGW torch.Size([29508, 5]) 9.258079808205366e-05\n",
      "Penalty params: tau=0.92483 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=181 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 181, train\n",
      " fgw:0.2945798\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2945798\n",
      "Measure Epoch 181, train\n",
      " similarity:0.0441614\n",
      " penlog:0.6438953\n",
      "Metrics Epoch 181, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.3913043\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:13.7800000\n",
      " batch_invalid_valency_nodes:24.6086957\n",
      " batch_nodes_0degree:5.6600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.7800000\n",
      " batch_node_degree:1.3547826\n",
      "Logits [24.82741928100586, 2.403449773788452, 54.49025344848633]\n",
      "Epoch duration: 1.8669843673706055\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_181\n",
      "Epoch: 182\n",
      "FGW torch.Size([29508, 5]) 9.26992142922245e-05\n",
      "Penalty params: tau=0.92443 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=182 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 182, train\n",
      " fgw:0.2886880\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2886880\n",
      "Measure Epoch 182, train\n",
      " similarity:0.0392150\n",
      " penlog:-1.2190683\n",
      "Metrics Epoch 182, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.6521739\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:13.7000000\n",
      " batch_invalid_valency_nodes:24.0000000\n",
      " batch_nodes_0degree:5.5000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.7000000\n",
      " batch_node_degree:1.3565217\n",
      "Logits [24.951967239379883, 2.424431085586548, 54.49843978881836]\n",
      "Epoch duration: 1.9293618202209473\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_182\n",
      "Epoch: 183\n",
      "FGW torch.Size([29508, 5]) 9.227787813870236e-05\n",
      "Penalty params: tau=0.92403 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=183 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 183, train\n",
      " fgw:0.2963284\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2963284\n",
      "Measure Epoch 183, train\n",
      " similarity:0.0419685\n",
      " penlog:-0.9274619\n",
      "Metrics Epoch 183, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.3043478\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:13.9200000\n",
      " batch_invalid_valency_nodes:24.6956522\n",
      " batch_nodes_0degree:5.6600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.9200000\n",
      " batch_node_degree:1.3234783\n",
      "Logits [25.218778610229492, 2.4205145835876465, 54.740577697753906]\n",
      "Epoch duration: 2.3607616424560547\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_183\n",
      "Epoch: 184\n",
      "FGW torch.Size([29508, 5]) 9.144403884420171e-05\n",
      "Penalty params: tau=0.92363 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=184 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 184, train\n",
      " fgw:0.2937074\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2937074\n",
      "Measure Epoch 184, train\n",
      " similarity:0.0439862\n",
      " penlog:0.7642608\n",
      "Metrics Epoch 184, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.4782609\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:14.1000000\n",
      " batch_invalid_valency_nodes:26.4347826\n",
      " batch_nodes_0degree:6.0800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-14.1000000\n",
      " batch_node_degree:1.3008696\n",
      "Logits [24.921566009521484, 2.4197919368743896, 54.645668029785156]\n",
      "Epoch duration: 2.1183273792266846\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_184\n",
      "Epoch: 185\n",
      "FGW torch.Size([29508, 5]) 9.036141273099929e-05\n",
      "Penalty params: tau=0.92323 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=185 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 185, train\n",
      " fgw:0.2867043\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2867043\n",
      "Measure Epoch 185, train\n",
      " similarity:0.0391768\n",
      " penlog:-3.1992625\n",
      "Metrics Epoch 185, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.5652174\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.7000000\n",
      " batch_invalid_valency_nodes:23.2173913\n",
      " batch_nodes_0degree:5.3000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.7000000\n",
      " batch_node_degree:1.4069565\n",
      "Logits [25.363689422607422, 2.449294328689575, 54.564903259277344]\n",
      "Epoch duration: 1.9582927227020264\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_185\n",
      "Epoch: 186\n",
      "FGW torch.Size([29508, 5]) 9.181276254821569e-05\n",
      "Penalty params: tau=0.92283 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=186 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 186, train\n",
      " fgw:0.2884060\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2884060\n",
      "Measure Epoch 186, train\n",
      " similarity:0.0439709\n",
      " penlog:-3.3535135\n",
      "Metrics Epoch 186, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.3043478\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.3200000\n",
      " batch_invalid_valency_nodes:21.6521739\n",
      " batch_nodes_0degree:4.9400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.3200000\n",
      " batch_node_degree:1.4591304\n",
      "Logits [25.518638610839844, 2.456435441970825, 54.58027648925781]\n",
      "Epoch duration: 1.99867582321167\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_186\n",
      "Epoch: 187\n",
      "FGW torch.Size([29508, 5]) 9.265677363146096e-05\n",
      "Penalty params: tau=0.92243 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=187 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 187, train\n",
      " fgw:0.2902394\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2902394\n",
      "Measure Epoch 187, train\n",
      " similarity:0.0451942\n",
      " penlog:-3.5682364\n",
      "Metrics Epoch 187, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.3043478\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.1600000\n",
      " batch_invalid_valency_nodes:21.5652174\n",
      " batch_nodes_0degree:4.9200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.1600000\n",
      " batch_node_degree:1.4365217\n",
      "Logits [25.322147369384766, 2.4440090656280518, 54.653804779052734]\n",
      "Epoch duration: 1.9924354553222656\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_187\n",
      "Epoch: 188\n",
      "FGW torch.Size([29508, 5]) 9.11485985852778e-05\n",
      "Penalty params: tau=0.92204 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=188 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 188, train\n",
      " fgw:0.2875052\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2875052\n",
      "Measure Epoch 188, train\n",
      " similarity:0.0445618\n",
      " penlog:-1.2250781\n",
      "Metrics Epoch 188, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.9130435\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:13.5000000\n",
      " batch_invalid_valency_nodes:23.5652174\n",
      " batch_nodes_0degree:5.4000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.5000000\n",
      " batch_node_degree:1.3826087\n",
      "Logits [25.68632698059082, 2.464723825454712, 54.703155517578125]\n",
      "Epoch duration: 1.9537668228149414\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_188\n",
      "Epoch: 189\n",
      "FGW torch.Size([29508, 5]) 9.155457519227639e-05\n",
      "Penalty params: tau=0.92164 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=189 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 189, train\n",
      " fgw:0.2900369\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2900369\n",
      "Measure Epoch 189, train\n",
      " similarity:0.0421481\n",
      " penlog:-1.2461470\n",
      "Metrics Epoch 189, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.0000000\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:13.4800000\n",
      " batch_invalid_valency_nodes:23.5652174\n",
      " batch_nodes_0degree:5.4000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.4800000\n",
      " batch_node_degree:1.3756522\n",
      "Logits [25.7226619720459, 2.474421739578247, 54.74214172363281]\n",
      "Epoch duration: 1.987610101699829\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 190\n",
      "FGW torch.Size([29508, 5]) 9.217990736942738e-05\n",
      "Penalty params: tau=0.92124 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=190 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 190, train\n",
      " fgw:0.2910043\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2910043\n",
      "Measure Epoch 190, train\n",
      " similarity:0.0464898\n",
      " penlog:0.4743786\n",
      "Metrics Epoch 190, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.6521739\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:13.3400000\n",
      " batch_invalid_valency_nodes:23.5652174\n",
      " batch_nodes_0degree:5.4200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.3400000\n",
      " batch_node_degree:1.3739130\n",
      "Logits [25.619516372680664, 2.4660301208496094, 54.87764358520508]\n",
      "Epoch duration: 2.013584852218628\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_190\n",
      "Epoch: 191\n",
      "FGW torch.Size([29508, 5]) 9.135380241787061e-05\n",
      "Penalty params: tau=0.92084 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=191 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 191, train\n",
      " fgw:0.2854212\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2854212\n",
      "Measure Epoch 191, train\n",
      " similarity:0.0407487\n",
      " penlog:0.7300641\n",
      "Metrics Epoch 191, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.6521739\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:13.4800000\n",
      " batch_invalid_valency_nodes:23.5652174\n",
      " batch_nodes_0degree:5.4200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.4800000\n",
      " batch_node_degree:1.3826087\n",
      "Logits [25.893598556518555, 2.4909207820892334, 54.873023986816406]\n",
      "Epoch duration: 2.064911127090454\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_191\n",
      "Epoch: 192\n",
      "FGW torch.Size([29508, 5]) 9.157143358606845e-05\n",
      "Penalty params: tau=0.92044 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=192 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 192, train\n",
      " fgw:0.2839388\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2839388\n",
      "Measure Epoch 192, train\n",
      " similarity:0.0367900\n",
      " penlog:-1.4157570\n",
      "Metrics Epoch 192, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.9130435\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:13.6200000\n",
      " batch_invalid_valency_nodes:23.9130435\n",
      " batch_nodes_0degree:5.4800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.6200000\n",
      " batch_node_degree:1.4104348\n",
      "Logits [25.75198745727539, 2.5056095123291016, 54.813011169433594]\n",
      "Epoch duration: 1.8942081928253174\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_192\n",
      "Epoch: 193\n",
      "FGW torch.Size([29508, 5]) 9.108903759624809e-05\n",
      "Penalty params: tau=0.92005 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=193 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 193, train\n",
      " fgw:0.2810319\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2810319\n",
      "Measure Epoch 193, train\n",
      " similarity:0.0319475\n",
      " penlog:-3.4512064\n",
      "Metrics Epoch 193, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.7391304\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.9600000\n",
      " batch_invalid_valency_nodes:25.2173913\n",
      " batch_nodes_0degree:5.7600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.9600000\n",
      " batch_node_degree:1.3773913\n",
      "Logits [25.766706466674805, 2.5007715225219727, 54.93743896484375]\n",
      "Epoch duration: 2.034705877304077\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_193\n",
      "Epoch: 194\n",
      "FGW torch.Size([29508, 5]) 9.107245568884537e-05\n",
      "Penalty params: tau=0.91965 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=194 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 194, train\n",
      " fgw:0.2802204\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2802204\n",
      "Measure Epoch 194, train\n",
      " similarity:0.0334456\n",
      " penlog:-5.3450083\n",
      "Metrics Epoch 194, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.6521739\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:13.7200000\n",
      " batch_invalid_valency_nodes:24.2608696\n",
      " batch_nodes_0degree:5.5200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.7200000\n",
      " batch_node_degree:1.4208696\n",
      "Logits [26.068639755249023, 2.5170838832855225, 55.043785095214844]\n",
      "Epoch duration: 2.0320916175842285\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_194\n",
      "Epoch: 195\n",
      "FGW torch.Size([29508, 5]) 9.15270356927067e-05\n",
      "Penalty params: tau=0.91925 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=195 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 195, train\n",
      " fgw:0.2812741\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2812741\n",
      "Measure Epoch 195, train\n",
      " similarity:0.0360596\n",
      " penlog:-3.2845430\n",
      "Metrics Epoch 195, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.9130435\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.9600000\n",
      " batch_invalid_valency_nodes:26.1739130\n",
      " batch_nodes_0degree:5.9800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.9600000\n",
      " batch_node_degree:1.3495652\n",
      "Logits [26.054731369018555, 2.5243980884552, 55.14692687988281]\n",
      "Epoch duration: 2.0076072216033936\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_195\n",
      "Epoch: 196\n",
      "FGW torch.Size([29508, 5]) 9.116011642618105e-05\n",
      "Penalty params: tau=0.91886 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=196 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 196, train\n",
      " fgw:0.2821454\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2821454\n",
      "Measure Epoch 196, train\n",
      " similarity:0.0359775\n",
      " penlog:-7.4386069\n",
      "Metrics Epoch 196, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.0000000\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:13.6600000\n",
      " batch_invalid_valency_nodes:25.3043478\n",
      " batch_nodes_0degree:5.7400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.6600000\n",
      " batch_node_degree:1.3826087\n",
      "Logits [26.405288696289062, 2.52909255027771, 55.32395553588867]\n",
      "Epoch duration: 2.106884717941284\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_196\n",
      "Epoch: 197\n",
      "FGW torch.Size([29508, 5]) 8.982133294921368e-05\n",
      "Penalty params: tau=0.91846 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=197 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 197, train\n",
      " fgw:0.2833680\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2833680\n",
      "Measure Epoch 197, train\n",
      " similarity:0.0357701\n",
      " penlog:-9.5936466\n",
      "Metrics Epoch 197, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.1739130\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:13.4400000\n",
      " batch_invalid_valency_nodes:24.6956522\n",
      " batch_nodes_0degree:5.5800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.4400000\n",
      " batch_node_degree:1.4208696\n",
      "Logits [26.46031379699707, 2.5446066856384277, 55.304710388183594]\n",
      "Epoch duration: 2.258223056793213\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_197\n",
      "Epoch: 198\n",
      "FGW torch.Size([29508, 5]) 9.04792541405186e-05\n",
      "Penalty params: tau=0.91806 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=198 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 198, train\n",
      " fgw:0.2768146\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2768146\n",
      "Measure Epoch 198, train\n",
      " similarity:0.0396029\n",
      " penlog:-5.5521517\n",
      "Metrics Epoch 198, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.0000000\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:13.3200000\n",
      " batch_invalid_valency_nodes:23.7391304\n",
      " batch_nodes_0degree:5.4000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.3200000\n",
      " batch_node_degree:1.4365217\n",
      "Logits [26.44359016418457, 2.5531039237976074, 55.36043167114258]\n",
      "Epoch duration: 1.9532959461212158\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 199\n",
      "FGW torch.Size([29508, 5]) 9.09582813619636e-05\n",
      "Penalty params: tau=0.91767 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=199 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 199, train\n",
      " fgw:0.2836077\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2836077\n",
      "Measure Epoch 199, train\n",
      " similarity:0.0393386\n",
      " penlog:-3.5013712\n",
      "Metrics Epoch 199, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.6521739\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.3400000\n",
      " batch_invalid_valency_nodes:23.9130435\n",
      " batch_nodes_0degree:5.4600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.3400000\n",
      " batch_node_degree:1.4260870\n",
      "Logits [26.686946868896484, 2.565722942352295, 55.49915313720703]\n",
      "Epoch duration: 2.11788272857666\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_199\n",
      "Epoch: 200\n",
      "FGW torch.Size([29508, 5]) 8.900073589757085e-05\n",
      "Penalty params: tau=0.91727 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=200 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 200, train\n",
      " fgw:0.2802287\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2802287\n",
      "Measure Epoch 200, train\n",
      " similarity:0.0394000\n",
      " penlog:-1.5414518\n",
      "Metrics Epoch 200, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.8260870\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:13.7200000\n",
      " batch_invalid_valency_nodes:26.0000000\n",
      " batch_nodes_0degree:5.9400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.7200000\n",
      " batch_node_degree:1.3895652\n",
      "Logits [26.691905975341797, 2.5754337310791016, 55.551307678222656]\n",
      "Epoch duration: 1.928272008895874\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_200\n",
      "Epoch: 201\n",
      "FGW torch.Size([29508, 5]) 8.912837074603885e-05\n",
      "Penalty params: tau=0.91688 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=201 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 201, train\n",
      " fgw:0.2826830\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2826830\n",
      "Measure Epoch 201, train\n",
      " similarity:0.0386058\n",
      " penlog:-3.3723502\n",
      "Metrics Epoch 201, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.8260870\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.5800000\n",
      " batch_invalid_valency_nodes:24.4347826\n",
      " batch_nodes_0degree:5.5800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.5800000\n",
      " batch_node_degree:1.4104348\n",
      "Logits [27.117717742919922, 2.6000173091888428, 55.61936950683594]\n",
      "Epoch duration: 2.031221866607666\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_201\n",
      "Epoch: 202\n",
      "FGW torch.Size([29508, 5]) 9.160251647699624e-05\n",
      "Penalty params: tau=0.91648 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=202 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 202, train\n",
      " fgw:0.2793394\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2793394\n",
      "Measure Epoch 202, train\n",
      " similarity:0.0407981\n",
      " penlog:-3.4469424\n",
      "Metrics Epoch 202, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.0869565\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.4000000\n",
      " batch_invalid_valency_nodes:24.0869565\n",
      " batch_nodes_0degree:5.4600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.4000000\n",
      " batch_node_degree:1.3895652\n",
      "Logits [27.149545669555664, 2.596571922302246, 55.766937255859375]\n",
      "Epoch duration: 2.099828004837036\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_202\n",
      "Epoch: 203\n",
      "FGW torch.Size([29508, 5]) 9.082497126655653e-05\n",
      "Penalty params: tau=0.91608 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=203 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 203, train\n",
      " fgw:0.2778608\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2778608\n",
      "Measure Epoch 203, train\n",
      " similarity:0.0392352\n",
      " penlog:-3.2099690\n",
      "Metrics Epoch 203, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.2608696\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.7000000\n",
      " batch_invalid_valency_nodes:24.6956522\n",
      " batch_nodes_0degree:5.6200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.7000000\n",
      " batch_node_degree:1.3513043\n",
      "Logits [27.276294708251953, 2.610250473022461, 55.83820724487305]\n",
      "Epoch duration: 2.1014842987060547\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_203\n",
      "Epoch: 204\n",
      "FGW torch.Size([29508, 5]) 8.916795195546001e-05\n",
      "Penalty params: tau=0.91569 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=204 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 204, train\n",
      " fgw:0.2768701\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2768701\n",
      "Measure Epoch 204, train\n",
      " similarity:0.0352072\n",
      " penlog:-5.0658160\n",
      "Metrics Epoch 204, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.1739130\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:13.9800000\n",
      " batch_invalid_valency_nodes:24.8695652\n",
      " batch_nodes_0degree:5.6600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.9800000\n",
      " batch_node_degree:1.3478261\n",
      "Logits [27.415464401245117, 2.616103410720825, 55.9576416015625]\n",
      "Epoch duration: 1.9825162887573242\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_204\n",
      "Epoch: 205\n",
      "FGW torch.Size([29508, 5]) 8.890098979463801e-05\n",
      "Penalty params: tau=0.91529 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=205 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 205, train\n",
      " fgw:0.2763468\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2763468\n",
      "Measure Epoch 205, train\n",
      " similarity:0.0383621\n",
      " penlog:-3.2782869\n",
      "Metrics Epoch 205, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.0000000\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.7600000\n",
      " batch_invalid_valency_nodes:24.1739130\n",
      " batch_nodes_0degree:5.5000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.7600000\n",
      " batch_node_degree:1.3878261\n",
      "Logits [27.23343276977539, 2.617805004119873, 55.935890197753906]\n",
      "Epoch duration: 2.1173789501190186\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_205\n",
      "Epoch: 206\n",
      "FGW torch.Size([29508, 5]) 8.999419515021145e-05\n",
      "Penalty params: tau=0.91490 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=206 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 206, train\n",
      " fgw:0.2780453\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2780453\n",
      "Measure Epoch 206, train\n",
      " similarity:0.0385071\n",
      " penlog:-7.5063234\n",
      "Metrics Epoch 206, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.8260870\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.8200000\n",
      " batch_invalid_valency_nodes:19.7391304\n",
      " batch_nodes_0degree:4.4600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.8200000\n",
      " batch_node_degree:1.5234783\n",
      "Logits [27.570959091186523, 2.646106719970703, 55.93236541748047]\n",
      "Epoch duration: 1.9919037818908691\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_206\n",
      "Epoch: 207\n",
      "FGW torch.Size([29508, 5]) 9.051591041497886e-05\n",
      "Penalty params: tau=0.91450 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=207 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 207, train\n",
      " fgw:0.2760813\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2760813\n",
      "Measure Epoch 207, train\n",
      " similarity:0.0476909\n",
      " penlog:-1.5460181\n",
      "Metrics Epoch 207, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.3478261\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:12.4600000\n",
      " batch_invalid_valency_nodes:18.6086957\n",
      " batch_nodes_0degree:4.2600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.4600000\n",
      " batch_node_degree:1.5217391\n",
      "Logits [27.47435188293457, 2.644617795944214, 56.027584075927734]\n",
      "Epoch duration: 1.927100419998169\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 208\n",
      "FGW torch.Size([29508, 5]) 8.82576423464343e-05\n",
      "Penalty params: tau=0.91411 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=208 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 208, train\n",
      " fgw:0.2692442\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2692442\n",
      "Measure Epoch 208, train\n",
      " similarity:0.0461855\n",
      " penlog:-1.4980289\n",
      "Metrics Epoch 208, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.1739130\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:12.9600000\n",
      " batch_invalid_valency_nodes:20.9565217\n",
      " batch_nodes_0degree:4.8000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.9600000\n",
      " batch_node_degree:1.4626087\n",
      "Logits [27.80570411682129, 2.6583306789398193, 56.18742370605469]\n",
      "Epoch duration: 2.080634832382202\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_208\n",
      "Epoch: 209\n",
      "FGW torch.Size([29508, 5]) 8.745185186853632e-05\n",
      "Penalty params: tau=0.91371 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=209 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 209, train\n",
      " fgw:0.2675168\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2675168\n",
      "Measure Epoch 209, train\n",
      " similarity:0.0438316\n",
      " penlog:0.5508826\n",
      "Metrics Epoch 209, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.4347826\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:13.2800000\n",
      " batch_invalid_valency_nodes:22.8695652\n",
      " batch_nodes_0degree:5.2600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.2800000\n",
      " batch_node_degree:1.4243478\n",
      "Logits [27.97764015197754, 2.669306755065918, 56.31605529785156]\n",
      "Epoch duration: 1.906850814819336\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_209\n",
      "Epoch: 210\n",
      "FGW torch.Size([29508, 5]) 8.894616621546447e-05\n",
      "Penalty params: tau=0.91332 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=210 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 210, train\n",
      " fgw:0.2699485\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2699485\n",
      "Measure Epoch 210, train\n",
      " similarity:0.0454871\n",
      " penlog:0.5607669\n",
      "Metrics Epoch 210, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.1739130\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:13.2600000\n",
      " batch_invalid_valency_nodes:22.9565217\n",
      " batch_nodes_0degree:5.2800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.2600000\n",
      " batch_node_degree:1.4226087\n",
      "Logits [27.938692092895508, 2.669642925262451, 56.45241165161133]\n",
      "Epoch duration: 2.2121288776397705\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_210\n",
      "Epoch: 211\n",
      "FGW torch.Size([29508, 5]) 8.874294144334272e-05\n",
      "Penalty params: tau=0.91292 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=211 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 211, train\n",
      " fgw:0.2707236\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2707236\n",
      "Measure Epoch 211, train\n",
      " similarity:0.0393464\n",
      " penlog:-3.3091213\n",
      "Metrics Epoch 211, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.8260870\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.3600000\n",
      " batch_invalid_valency_nodes:22.6086957\n",
      " batch_nodes_0degree:5.1600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.3600000\n",
      " batch_node_degree:1.4556522\n",
      "Logits [28.254074096679688, 2.6912240982055664, 56.459228515625]\n",
      "Epoch duration: 2.0379281044006348\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_211\n",
      "Epoch: 212\n",
      "FGW torch.Size([29508, 5]) 8.813881868263707e-05\n",
      "Penalty params: tau=0.91253 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=212 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 212, train\n",
      " fgw:0.2730278\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2730278\n",
      "Measure Epoch 212, train\n",
      " similarity:0.0345774\n",
      " penlog:-3.5471792\n",
      "Metrics Epoch 212, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.8260870\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.1600000\n",
      " batch_invalid_valency_nodes:22.1739130\n",
      " batch_nodes_0degree:5.0400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.1600000\n",
      " batch_node_degree:1.4886957\n",
      "Logits [28.00707244873047, 2.6975433826446533, 56.378604888916016]\n",
      "Epoch duration: 2.019690752029419\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_212\n",
      "Epoch: 213\n",
      "FGW torch.Size([29508, 5]) 8.859046647557989e-05\n",
      "Penalty params: tau=0.91214 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=213 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 213, train\n",
      " fgw:0.2716173\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2716173\n",
      "Measure Epoch 213, train\n",
      " similarity:0.0385476\n",
      " penlog:-5.5155365\n",
      "Metrics Epoch 213, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.6521739\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:12.7600000\n",
      " batch_invalid_valency_nodes:19.8260870\n",
      " batch_nodes_0degree:4.4800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.7600000\n",
      " batch_node_degree:1.5304348\n",
      "Logits [28.534236907958984, 2.7079429626464844, 56.62044143676758]\n",
      "Epoch duration: 1.9610106945037842\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_213\n",
      "Epoch: 214\n",
      "FGW torch.Size([29508, 5]) 8.983100269688293e-05\n",
      "Penalty params: tau=0.91174 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=214 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 214, train\n",
      " fgw:0.2715750\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2715750\n",
      "Measure Epoch 214, train\n",
      " similarity:0.0428568\n",
      " penlog:-1.6003615\n",
      "Metrics Epoch 214, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.9130435\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:12.6800000\n",
      " batch_invalid_valency_nodes:19.4782609\n",
      " batch_nodes_0degree:4.4600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.6600000\n",
      " batch_node_degree:1.4886957\n",
      "Logits [28.30071258544922, 2.7118980884552, 56.607276916503906]\n",
      "Epoch duration: 2.08976674079895\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_214\n",
      "Epoch: 215\n",
      "FGW torch.Size([29508, 5]) 8.802757656667382e-05\n",
      "Penalty params: tau=0.91135 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=215 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 215, train\n",
      " fgw:0.2668498\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2668498\n",
      "Measure Epoch 215, train\n",
      " similarity:0.0420496\n",
      " penlog:-3.4036610\n",
      "Metrics Epoch 215, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.4347826\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:12.7800000\n",
      " batch_invalid_valency_nodes:18.9565217\n",
      " batch_nodes_0degree:4.3200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.7800000\n",
      " batch_node_degree:1.4730435\n",
      "Logits [28.653118133544922, 2.72731351852417, 56.705177307128906]\n",
      "Epoch duration: 1.9949798583984375\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_215\n",
      "Epoch: 216\n",
      "FGW torch.Size([29508, 5]) 8.828967111185193e-05\n",
      "Penalty params: tau=0.91096 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=216 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 216, train\n",
      " fgw:0.2701785\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2701785\n",
      "Measure Epoch 216, train\n",
      " similarity:0.0387001\n",
      " penlog:-3.2664306\n",
      "Metrics Epoch 216, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.2608696\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.2000000\n",
      " batch_invalid_valency_nodes:20.8695652\n",
      " batch_nodes_0degree:4.7600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.2000000\n",
      " batch_node_degree:1.4243478\n",
      "Logits [28.83568000793457, 2.7463793754577637, 56.736175537109375]\n",
      "Epoch duration: 2.039112091064453\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 217\n",
      "FGW torch.Size([29508, 5]) 8.961200364865363e-05\n",
      "Penalty params: tau=0.91056 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=217 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 217, train\n",
      " fgw:0.2718981\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2718981\n",
      "Measure Epoch 217, train\n",
      " similarity:0.0377245\n",
      " penlog:-5.3631444\n",
      "Metrics Epoch 217, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.1739130\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:13.3000000\n",
      " batch_invalid_valency_nodes:21.5652174\n",
      " batch_nodes_0degree:4.9000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.3000000\n",
      " batch_node_degree:1.3965217\n",
      "Logits [28.67604637145996, 2.7572576999664307, 56.75315856933594]\n",
      "Epoch duration: 2.1592836380004883\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_217\n",
      "Epoch: 218\n",
      "FGW torch.Size([29508, 5]) 8.76234844326973e-05\n",
      "Penalty params: tau=0.91017 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=218 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 218, train\n",
      " fgw:0.2675742\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2675742\n",
      "Measure Epoch 218, train\n",
      " similarity:0.0346327\n",
      " penlog:-9.4120208\n",
      "Metrics Epoch 218, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.9130435\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:13.2000000\n",
      " batch_invalid_valency_nodes:21.0434783\n",
      " batch_nodes_0degree:4.7400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.2000000\n",
      " batch_node_degree:1.4660870\n",
      "Logits [29.0806941986084, 2.7800705432891846, 56.88080978393555]\n",
      "Epoch duration: 2.1049375534057617\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_218\n",
      "Epoch: 219\n",
      "FGW torch.Size([29508, 5]) 8.694251300767064e-05\n",
      "Penalty params: tau=0.90978 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=219 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 219, train\n",
      " fgw:0.2645655\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2645655\n",
      "Measure Epoch 219, train\n",
      " similarity:0.0376354\n",
      " penlog:-9.5919713\n",
      "Metrics Epoch 219, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.2608696\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:13.1200000\n",
      " batch_invalid_valency_nodes:21.4782609\n",
      " batch_nodes_0degree:4.8000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.1200000\n",
      " batch_node_degree:1.4765217\n",
      "Logits [28.838411331176758, 2.779810905456543, 56.93589401245117]\n",
      "Epoch duration: 2.371375799179077\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_219\n",
      "Epoch: 220\n",
      "FGW torch.Size([29508, 5]) 8.730010449653491e-05\n",
      "Penalty params: tau=0.90938 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=220 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 220, train\n",
      " fgw:0.2684550\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2684550\n",
      "Measure Epoch 220, train\n",
      " similarity:0.0365897\n",
      " penlog:-7.5062990\n",
      "Metrics Epoch 220, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.5217391\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.9400000\n",
      " batch_invalid_valency_nodes:20.6956522\n",
      " batch_nodes_0degree:4.6600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.9400000\n",
      " batch_node_degree:1.4921739\n",
      "Logits [29.037960052490234, 2.7905118465423584, 57.073787689208984]\n",
      "Epoch duration: 2.3072752952575684\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_220\n",
      "Epoch: 221\n",
      "FGW torch.Size([29508, 5]) 8.884560520527884e-05\n",
      "Penalty params: tau=0.90899 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=221 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 221, train\n",
      " fgw:0.2653350\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2653350\n",
      "Measure Epoch 221, train\n",
      " similarity:0.0408802\n",
      " penlog:-3.3169186\n",
      "Metrics Epoch 221, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.5217391\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:12.7800000\n",
      " batch_invalid_valency_nodes:20.2608696\n",
      " batch_nodes_0degree:4.5800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.7800000\n",
      " batch_node_degree:1.5026087\n",
      "Logits [29.360761642456055, 2.805267572402954, 57.22336196899414]\n",
      "Epoch duration: 2.111255407333374\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_221\n",
      "Epoch: 222\n",
      "FGW torch.Size([29508, 5]) 8.770409476710483e-05\n",
      "Penalty params: tau=0.90860 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=222 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 222, train\n",
      " fgw:0.2647831\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2647831\n",
      "Measure Epoch 222, train\n",
      " similarity:0.0367724\n",
      " penlog:-3.3806433\n",
      "Metrics Epoch 222, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.4347826\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.2600000\n",
      " batch_invalid_valency_nodes:22.1739130\n",
      " batch_nodes_0degree:5.0400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.2600000\n",
      " batch_node_degree:1.4208696\n",
      "Logits [29.293556213378906, 2.8004565238952637, 57.3754768371582]\n",
      "Epoch duration: 1.9201571941375732\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_222\n",
      "Epoch: 223\n",
      "FGW torch.Size([29508, 5]) 8.679513848619536e-05\n",
      "Penalty params: tau=0.90821 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=223 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 223, train\n",
      " fgw:0.2675375\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2675375\n",
      "Measure Epoch 223, train\n",
      " similarity:0.0322936\n",
      " penlog:-7.2450490\n",
      "Metrics Epoch 223, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.3478261\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:13.6200000\n",
      " batch_invalid_valency_nodes:23.3043478\n",
      " batch_nodes_0degree:5.2800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.6200000\n",
      " batch_node_degree:1.4173913\n",
      "Logits [29.72197151184082, 2.825376033782959, 57.454593658447266]\n",
      "Epoch duration: 2.047370672225952\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_223\n",
      "Epoch: 224\n",
      "FGW torch.Size([29508, 5]) 8.94123877515085e-05\n",
      "Penalty params: tau=0.90782 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=224 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 224, train\n",
      " fgw:0.2665535\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2665535\n",
      "Measure Epoch 224, train\n",
      " similarity:0.0515991\n",
      " penlog:-5.3519230\n",
      "Metrics Epoch 224, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.2608696\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:13.3200000\n",
      " batch_invalid_valency_nodes:21.6521739\n",
      " batch_nodes_0degree:4.9000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.3200000\n",
      " batch_node_degree:1.4452174\n",
      "Logits [29.554372787475586, 2.82641863822937, 57.46775817871094]\n",
      "Epoch duration: 2.19795560836792\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_224\n",
      "Epoch: 225\n",
      "FGW torch.Size([29508, 5]) 8.944460569182411e-05\n",
      "Penalty params: tau=0.90742 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=225 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 225, train\n",
      " fgw:0.2597916\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2597916\n",
      "Measure Epoch 225, train\n",
      " similarity:0.0558225\n",
      " penlog:-3.3187157\n",
      "Metrics Epoch 225, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.6956522\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.1600000\n",
      " batch_invalid_valency_nodes:20.8695652\n",
      " batch_nodes_0degree:4.7600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.1600000\n",
      " batch_node_degree:1.4678261\n",
      "Logits [29.789541244506836, 2.841590166091919, 57.55729675292969]\n",
      "Epoch duration: 2.0529346466064453\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 226\n",
      "FGW torch.Size([29508, 5]) 8.606755727669224e-05\n",
      "Penalty params: tau=0.90703 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=226 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 226, train\n",
      " fgw:0.2624882\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2624882\n",
      "Measure Epoch 226, train\n",
      " similarity:0.0533395\n",
      " penlog:-9.4094242\n",
      "Metrics Epoch 226, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.5217391\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:13.3400000\n",
      " batch_invalid_valency_nodes:21.8260870\n",
      " batch_nodes_0degree:4.9200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.3400000\n",
      " batch_node_degree:1.4643478\n",
      "Logits [29.96044921875, 2.8521018028259277, 57.64061737060547]\n",
      "Epoch duration: 1.8838779926300049\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_226\n",
      "Epoch: 227\n",
      "FGW torch.Size([29508, 5]) 8.609358337707818e-05\n",
      "Penalty params: tau=0.90664 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=227 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 227, train\n",
      " fgw:0.2609594\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2609594\n",
      "Measure Epoch 227, train\n",
      " similarity:0.0560915\n",
      " penlog:-5.4206706\n",
      "Metrics Epoch 227, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.4347826\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:13.0600000\n",
      " batch_invalid_valency_nodes:20.8695652\n",
      " batch_nodes_0degree:4.7400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.0400000\n",
      " batch_node_degree:1.4626087\n",
      "Logits [29.9605712890625, 2.8467893600463867, 57.776702880859375]\n",
      "Epoch duration: 2.0763750076293945\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_227\n",
      "Epoch: 228\n",
      "FGW torch.Size([29508, 5]) 8.818647620500997e-05\n",
      "Penalty params: tau=0.90625 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=228 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 228, train\n",
      " fgw:0.2623075\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2623075\n",
      "Measure Epoch 228, train\n",
      " similarity:0.0574941\n",
      " penlog:-7.4437068\n",
      "Metrics Epoch 228, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.6086957\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.7400000\n",
      " batch_invalid_valency_nodes:20.1739130\n",
      " batch_nodes_0degree:4.5200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.7000000\n",
      " batch_node_degree:1.4869565\n",
      "Logits [30.283153533935547, 2.862886905670166, 57.873512268066406]\n",
      "Epoch duration: 2.0751404762268066\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_228\n",
      "Epoch: 229\n",
      "FGW torch.Size([29508, 5]) 8.793221059022471e-05\n",
      "Penalty params: tau=0.90586 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=229 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 229, train\n",
      " fgw:0.2597669\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2597669\n",
      "Measure Epoch 229, train\n",
      " similarity:0.0567968\n",
      " penlog:-3.3841059\n",
      "Metrics Epoch 229, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.4347826\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:12.9800000\n",
      " batch_invalid_valency_nodes:20.6956522\n",
      " batch_nodes_0degree:4.7200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.9800000\n",
      " batch_node_degree:1.4452174\n",
      "Logits [30.135038375854492, 2.8598999977111816, 57.861270904541016]\n",
      "Epoch duration: 2.014106035232544\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_229\n",
      "Epoch: 230\n",
      "FGW torch.Size([29508, 5]) 8.520734991179779e-05\n",
      "Penalty params: tau=0.90547 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=230 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 230, train\n",
      " fgw:0.2579079\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2579079\n",
      "Measure Epoch 230, train\n",
      " similarity:0.0389476\n",
      " penlog:-1.1348263\n",
      "Metrics Epoch 230, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.4347826\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:13.6600000\n",
      " batch_invalid_valency_nodes:23.1304348\n",
      " batch_nodes_0degree:5.3000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.6600000\n",
      " batch_node_degree:1.4295652\n",
      "Logits [30.477615356445312, 2.8808865547180176, 57.8397102355957]\n",
      "Epoch duration: 1.8987162113189697\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_230\n",
      "Epoch: 231\n",
      "FGW torch.Size([29508, 5]) 8.788423292571679e-05\n",
      "Penalty params: tau=0.90508 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=231 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 231, train\n",
      " fgw:0.2593368\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2593368\n",
      "Measure Epoch 231, train\n",
      " similarity:0.0377821\n",
      " penlog:-3.3045848\n",
      "Metrics Epoch 231, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.5217391\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.5000000\n",
      " batch_invalid_valency_nodes:23.3913043\n",
      " batch_nodes_0degree:5.3400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.5000000\n",
      " batch_node_degree:1.4486957\n",
      "Logits [30.324905395507812, 2.8857126235961914, 57.86962127685547]\n",
      "Epoch duration: 2.0458061695098877\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_231\n",
      "Epoch: 232\n",
      "FGW torch.Size([29508, 5]) 8.8467713794671e-05\n",
      "Penalty params: tau=0.90468 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=232 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 232, train\n",
      " fgw:0.2636452\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2636452\n",
      "Measure Epoch 232, train\n",
      " similarity:0.0568492\n",
      " penlog:0.7434556\n",
      "Metrics Epoch 232, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.5217391\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:13.1400000\n",
      " batch_invalid_valency_nodes:21.2173913\n",
      " batch_nodes_0degree:4.8800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.1400000\n",
      " batch_node_degree:1.4730435\n",
      "Logits [30.588090896606445, 2.888857841491699, 58.048255920410156]\n",
      "Epoch duration: 2.24577260017395\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_232\n",
      "Epoch: 233\n",
      "FGW torch.Size([29508, 5]) 8.517542300978675e-05\n",
      "Penalty params: tau=0.90429 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=233 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 233, train\n",
      " fgw:0.2597901\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2597901\n",
      "Measure Epoch 233, train\n",
      " similarity:0.0567069\n",
      " penlog:0.7325491\n",
      "Metrics Epoch 233, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.6956522\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:13.2800000\n",
      " batch_invalid_valency_nodes:22.7826087\n",
      " batch_nodes_0degree:5.2400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.2800000\n",
      " batch_node_degree:1.4573913\n",
      "Logits [30.499494552612305, 2.903107166290283, 57.99419021606445]\n",
      "Epoch duration: 2.0684235095977783\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_233\n",
      "Epoch: 234\n",
      "FGW torch.Size([29508, 5]) 8.634840196464211e-05\n",
      "Penalty params: tau=0.90390 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=234 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 234, train\n",
      " fgw:0.2559960\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2559960\n",
      "Measure Epoch 234, train\n",
      " similarity:0.0552052\n",
      " penlog:-1.3781285\n",
      "Metrics Epoch 234, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.6086957\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:13.1800000\n",
      " batch_invalid_valency_nodes:21.9130435\n",
      " batch_nodes_0degree:5.0200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.1800000\n",
      " batch_node_degree:1.4486957\n",
      "Logits [30.791824340820312, 2.901716947555542, 58.19032669067383]\n",
      "Epoch duration: 2.0343058109283447\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 235\n",
      "FGW torch.Size([29508, 5]) 8.768662519287318e-05\n",
      "Penalty params: tau=0.90351 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=235 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 235, train\n",
      " fgw:0.2574838\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2574838\n",
      "Measure Epoch 235, train\n",
      " similarity:0.0580117\n",
      " penlog:-3.5141759\n",
      "Metrics Epoch 235, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.7826087\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:12.8000000\n",
      " batch_invalid_valency_nodes:21.1304348\n",
      " batch_nodes_0degree:4.8200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.7800000\n",
      " batch_node_degree:1.4713043\n",
      "Logits [30.74173927307129, 2.9242193698883057, 58.11402893066406]\n",
      "Epoch duration: 2.098147392272949\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_235\n",
      "Epoch: 236\n",
      "FGW torch.Size([29508, 5]) 8.562345465179533e-05\n",
      "Penalty params: tau=0.90312 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=236 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 236, train\n",
      " fgw:0.2569646\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2569646\n",
      "Measure Epoch 236, train\n",
      " similarity:0.0513085\n",
      " penlog:-5.4668812\n",
      "Metrics Epoch 236, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.6086957\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:13.2800000\n",
      " batch_invalid_valency_nodes:22.2608696\n",
      " batch_nodes_0degree:5.0600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.2800000\n",
      " batch_node_degree:1.4313043\n",
      "Logits [31.088241577148438, 2.9293572902679443, 58.22046661376953]\n",
      "Epoch duration: 1.9886841773986816\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_236\n",
      "Epoch: 237\n",
      "FGW torch.Size([29508, 5]) 8.637690189061686e-05\n",
      "Penalty params: tau=0.90273 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=237 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 237, train\n",
      " fgw:0.2528639\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2528639\n",
      "Measure Epoch 237, train\n",
      " similarity:0.0593889\n",
      " penlog:-3.3856549\n",
      "Metrics Epoch 237, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.5217391\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.0800000\n",
      " batch_invalid_valency_nodes:21.2173913\n",
      " batch_nodes_0degree:4.8400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.0800000\n",
      " batch_node_degree:1.4504348\n",
      "Logits [31.013248443603516, 2.937812089920044, 58.242977142333984]\n",
      "Epoch duration: 2.0236003398895264\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_237\n",
      "Epoch: 238\n",
      "FGW torch.Size([29508, 5]) 8.762448123889044e-05\n",
      "Penalty params: tau=0.90234 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=238 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 238, train\n",
      " fgw:0.2578949\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2578949\n",
      "Measure Epoch 238, train\n",
      " similarity:0.0621963\n",
      " penlog:-1.3429468\n",
      "Metrics Epoch 238, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.7826087\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:12.9600000\n",
      " batch_invalid_valency_nodes:20.1739130\n",
      " batch_nodes_0degree:4.6200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.9400000\n",
      " batch_node_degree:1.4765217\n",
      "Logits [31.375669479370117, 2.956472158432007, 58.370365142822266]\n",
      "Epoch duration: 2.06750226020813\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_238\n",
      "Epoch: 239\n",
      "FGW torch.Size([29508, 5]) 8.643282490083948e-05\n",
      "Penalty params: tau=0.90195 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=239 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 239, train\n",
      " fgw:0.2590328\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2590328\n",
      "Measure Epoch 239, train\n",
      " similarity:0.0572505\n",
      " penlog:-3.2677045\n",
      "Metrics Epoch 239, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.8695652\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.1800000\n",
      " batch_invalid_valency_nodes:21.4782609\n",
      " batch_nodes_0degree:4.8800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.1800000\n",
      " batch_node_degree:1.4382609\n",
      "Logits [31.343935012817383, 2.9496445655822754, 58.55384826660156]\n",
      "Epoch duration: 1.9992213249206543\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_239\n",
      "Epoch: 240\n",
      "FGW torch.Size([29508, 5]) 8.61408407217823e-05\n",
      "Penalty params: tau=0.90157 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=240 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 240, train\n",
      " fgw:0.2609842\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2609842\n",
      "Measure Epoch 240, train\n",
      " similarity:0.0578068\n",
      " penlog:-3.2605564\n",
      "Metrics Epoch 240, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.9565217\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.2000000\n",
      " batch_invalid_valency_nodes:21.3043478\n",
      " batch_nodes_0degree:4.8600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.2000000\n",
      " batch_node_degree:1.4573913\n",
      "Logits [31.708377838134766, 2.986511468887329, 58.54900360107422]\n",
      "Epoch duration: 2.138737201690674\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_240\n",
      "Epoch: 241\n",
      "FGW torch.Size([29508, 5]) 8.843681280268356e-05\n",
      "Penalty params: tau=0.90118 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=241 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 241, train\n",
      " fgw:0.2594880\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2594880\n",
      "Measure Epoch 241, train\n",
      " similarity:0.0561051\n",
      " penlog:-5.3760503\n",
      "Metrics Epoch 241, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.7826087\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:13.0200000\n",
      " batch_invalid_valency_nodes:21.4782609\n",
      " batch_nodes_0degree:4.8800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.0000000\n",
      " batch_node_degree:1.4591304\n",
      "Logits [31.691896438598633, 2.9846670627593994, 58.71356201171875]\n",
      "Epoch duration: 2.381237506866455\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_241\n",
      "Epoch: 242\n",
      "FGW torch.Size([29508, 5]) 8.812532178126276e-05\n",
      "Penalty params: tau=0.90079 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=242 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 242, train\n",
      " fgw:0.2592879\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2592879\n",
      "Measure Epoch 242, train\n",
      " similarity:0.0584702\n",
      " penlog:-5.2741593\n",
      "Metrics Epoch 242, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.7826087\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:13.0600000\n",
      " batch_invalid_valency_nodes:21.7391304\n",
      " batch_nodes_0degree:4.9400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.0600000\n",
      " batch_node_degree:1.4591304\n",
      "Logits [31.997303009033203, 3.00333833694458, 58.83259963989258]\n",
      "Epoch duration: 1.9955315589904785\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_242\n",
      "Epoch: 243\n",
      "FGW torch.Size([29508, 5]) 8.497483941027895e-05\n",
      "Penalty params: tau=0.90040 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=243 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 243, train\n",
      " fgw:0.2522977\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2522977\n",
      "Measure Epoch 243, train\n",
      " similarity:0.0576460\n",
      " penlog:-1.1617666\n",
      "Metrics Epoch 243, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.9565217\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:13.5000000\n",
      " batch_invalid_valency_nodes:24.0869565\n",
      " batch_nodes_0degree:5.5200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.5000000\n",
      " batch_node_degree:1.4069565\n",
      "Logits [32.022220611572266, 3.014305353164673, 58.90274429321289]\n",
      "Epoch duration: 2.0110561847686768\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 244\n",
      "FGW torch.Size([29508, 5]) 8.687025547260419e-05\n",
      "Penalty params: tau=0.90001 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=244 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 244, train\n",
      " fgw:0.2566171\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2566171\n",
      "Measure Epoch 244, train\n",
      " similarity:0.0623903\n",
      " penlog:-1.1653514\n",
      "Metrics Epoch 244, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.9565217\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:13.1600000\n",
      " batch_invalid_valency_nodes:21.8260870\n",
      " batch_nodes_0degree:5.0000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.1600000\n",
      " batch_node_degree:1.4486957\n",
      "Logits [32.30207824707031, 3.013568639755249, 59.06036376953125]\n",
      "Epoch duration: 2.016706705093384\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_244\n",
      "Epoch: 245\n",
      "FGW torch.Size([29508, 5]) 8.774754678597674e-05\n",
      "Penalty params: tau=0.89962 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=245 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 245, train\n",
      " fgw:0.2553749\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2553749\n",
      "Measure Epoch 245, train\n",
      " similarity:0.0603941\n",
      " penlog:-5.2983529\n",
      "Metrics Epoch 245, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.7826087\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:13.0400000\n",
      " batch_invalid_valency_nodes:21.9130435\n",
      " batch_nodes_0degree:4.9800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.0400000\n",
      " batch_node_degree:1.4713043\n",
      "Logits [32.259056091308594, 3.0399937629699707, 58.89013671875]\n",
      "Epoch duration: 2.2537436485290527\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_245\n",
      "Epoch: 246\n",
      "FGW torch.Size([29508, 5]) 8.645839989185333e-05\n",
      "Penalty params: tau=0.89923 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=246 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 246, train\n",
      " fgw:0.2558474\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2558474\n",
      "Measure Epoch 246, train\n",
      " similarity:0.0611026\n",
      " penlog:-1.1545926\n",
      "Metrics Epoch 246, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.1739130\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:13.4200000\n",
      " batch_invalid_valency_nodes:22.5217391\n",
      " batch_nodes_0degree:5.1600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.4200000\n",
      " batch_node_degree:1.4295652\n",
      "Logits [32.642372131347656, 3.029824733734131, 59.1688346862793]\n",
      "Epoch duration: 1.996166706085205\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_246\n",
      "Epoch: 247\n",
      "FGW torch.Size([29508, 5]) 8.440250530838966e-05\n",
      "Penalty params: tau=0.89884 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=247 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 247, train\n",
      " fgw:0.2507985\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2507985\n",
      "Measure Epoch 247, train\n",
      " similarity:0.0607481\n",
      " penlog:-1.2417681\n",
      "Metrics Epoch 247, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.6086957\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:13.3000000\n",
      " batch_invalid_valency_nodes:21.6521739\n",
      " batch_nodes_0degree:4.9600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.3000000\n",
      " batch_node_degree:1.4573913\n",
      "Logits [32.42071533203125, 3.0506949424743652, 59.029075622558594]\n",
      "Epoch duration: 1.9673664569854736\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_247\n",
      "Epoch: 248\n",
      "FGW torch.Size([29508, 5]) 8.66496775415726e-05\n",
      "Penalty params: tau=0.89846 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=248 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 248, train\n",
      " fgw:0.2533945\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2533945\n",
      "Measure Epoch 248, train\n",
      " similarity:0.0605778\n",
      " penlog:-5.2831746\n",
      "Metrics Epoch 248, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.6956522\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:12.9800000\n",
      " batch_invalid_valency_nodes:20.6956522\n",
      " batch_nodes_0degree:4.6800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.9800000\n",
      " batch_node_degree:1.4747826\n",
      "Logits [32.87913131713867, 3.05670166015625, 59.27910232543945]\n",
      "Epoch duration: 2.078857660293579\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_248\n",
      "Epoch: 249\n",
      "FGW torch.Size([29508, 5]) 8.561047434341162e-05\n",
      "Penalty params: tau=0.89807 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=249 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 249, train\n",
      " fgw:0.2477070\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2477070\n",
      "Measure Epoch 249, train\n",
      " similarity:0.0601993\n",
      " penlog:-5.3938608\n",
      "Metrics Epoch 249, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.0434783\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:12.9000000\n",
      " batch_invalid_valency_nodes:20.5217391\n",
      " batch_nodes_0degree:4.6600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.8800000\n",
      " batch_node_degree:1.4852174\n",
      "Logits [32.729164123535156, 3.065795660018921, 59.239784240722656]\n",
      "Epoch duration: 2.042587995529175\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_249\n",
      "Epoch: 250\n",
      "FGW torch.Size([29508, 5]) 8.52155135362409e-05\n",
      "Penalty params: tau=0.89768 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=250 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 250, train\n",
      " fgw:0.2532149\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2532149\n",
      "Measure Epoch 250, train\n",
      " similarity:0.0600301\n",
      " penlog:-5.3674831\n",
      "Metrics Epoch 250, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.1304348\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:12.6800000\n",
      " batch_invalid_valency_nodes:19.6521739\n",
      " batch_nodes_0degree:4.4600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.6600000\n",
      " batch_node_degree:1.5026087\n",
      "Logits [32.95954132080078, 3.0843372344970703, 59.33970260620117]\n",
      "Epoch duration: 2.1448395252227783\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_250\n",
      "Epoch: 251\n",
      "FGW torch.Size([29508, 5]) 8.493944187648594e-05\n",
      "Penalty params: tau=0.89729 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=251 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 251, train\n",
      " fgw:0.2523790\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2523790\n",
      "Measure Epoch 251, train\n",
      " similarity:0.0603388\n",
      " penlog:-3.4237109\n",
      "Metrics Epoch 251, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.7826087\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:12.7000000\n",
      " batch_invalid_valency_nodes:19.8260870\n",
      " batch_nodes_0degree:4.5000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.6800000\n",
      " batch_node_degree:1.4782609\n",
      "Logits [33.162689208984375, 3.0802247524261475, 59.54448699951172]\n",
      "Epoch duration: 2.2440426349639893\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_251\n",
      "Epoch: 252\n",
      "FGW torch.Size([29508, 5]) 8.557656110497192e-05\n",
      "Penalty params: tau=0.89691 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=252 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 252, train\n",
      " fgw:0.2514603\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2514603\n",
      "Measure Epoch 252, train\n",
      " similarity:0.0564011\n",
      " penlog:-7.4892374\n",
      "Metrics Epoch 252, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.6956522\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.4800000\n",
      " batch_invalid_valency_nodes:19.1304348\n",
      " batch_nodes_0degree:4.2600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.4600000\n",
      " batch_node_degree:1.5234783\n",
      "Logits [33.01814651489258, 3.1046605110168457, 59.4611701965332]\n",
      "Epoch duration: 2.0802459716796875\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 253\n",
      "FGW torch.Size([29508, 5]) 8.502551645506173e-05\n",
      "Penalty params: tau=0.89652 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=253 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 253, train\n",
      " fgw:0.2519779\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2519779\n",
      "Measure Epoch 253, train\n",
      " similarity:0.0551686\n",
      " penlog:-9.4870457\n",
      "Metrics Epoch 253, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.9130435\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:12.5800000\n",
      " batch_invalid_valency_nodes:19.6521739\n",
      " batch_nodes_0degree:4.4200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.5600000\n",
      " batch_node_degree:1.5147826\n",
      "Logits [33.3299446105957, 3.0859739780426025, 59.79304504394531]\n",
      "Epoch duration: 2.1524674892425537\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_253\n",
      "Epoch: 254\n",
      "FGW torch.Size([29508, 5]) 8.547948527848348e-05\n",
      "Penalty params: tau=0.89613 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=254 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 254, train\n",
      " fgw:0.2570322\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2570322\n",
      "Measure Epoch 254, train\n",
      " similarity:0.0579825\n",
      " penlog:-3.6014050\n",
      "Metrics Epoch 254, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.2608696\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:12.3800000\n",
      " batch_invalid_valency_nodes:18.0869565\n",
      " batch_nodes_0degree:4.0800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.3600000\n",
      " batch_node_degree:1.5391304\n",
      "Logits [32.97594451904297, 3.0968739986419678, 59.61754608154297]\n",
      "Epoch duration: 2.3378450870513916\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_254\n",
      "Epoch: 255\n",
      "FGW torch.Size([29508, 5]) 8.511051419191062e-05\n",
      "Penalty params: tau=0.89575 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=255 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 255, train\n",
      " fgw:0.2517548\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2517548\n",
      "Measure Epoch 255, train\n",
      " similarity:0.0552321\n",
      " penlog:-15.7005719\n",
      "Metrics Epoch 255, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.5217391\n",
      " batch_molecular_disconnected_validity:84.0000000\n",
      " batch_connected_components:12.0200000\n",
      " batch_invalid_valency_nodes:18.1739130\n",
      " batch_nodes_0degree:3.9000000\n",
      " batch_nodes_7plus_degree:0.0200000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.0200000\n",
      " batch_node_degree:1.6208696\n",
      "Logits [33.763587951660156, 3.117853879928589, 59.672176361083984]\n",
      "Epoch duration: 2.043325901031494\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_255\n",
      "Epoch: 256\n",
      "FGW torch.Size([29508, 5]) 8.61042135511525e-05\n",
      "Penalty params: tau=0.89536 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=256 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 256, train\n",
      " fgw:0.2513584\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2513584\n",
      "Measure Epoch 256, train\n",
      " similarity:0.0578495\n",
      " penlog:-9.6948584\n",
      "Metrics Epoch 256, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.2173913\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:11.8600000\n",
      " batch_invalid_valency_nodes:17.2173913\n",
      " batch_nodes_0degree:3.7800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.8400000\n",
      " batch_node_degree:1.6243478\n",
      "Logits [33.55466079711914, 3.1240713596343994, 59.59278106689453]\n",
      "Epoch duration: 2.1406824588775635\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_256\n",
      "Epoch: 257\n",
      "FGW torch.Size([29508, 5]) 8.554982196073979e-05\n",
      "Penalty params: tau=0.89497 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=257 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 257, train\n",
      " fgw:0.2490489\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2490489\n",
      "Measure Epoch 257, train\n",
      " similarity:0.0606133\n",
      " penlog:-7.4480084\n",
      "Metrics Epoch 257, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.9565217\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.3200000\n",
      " batch_invalid_valency_nodes:19.3913043\n",
      " batch_nodes_0degree:4.3400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.3200000\n",
      " batch_node_degree:1.5408696\n",
      "Logits [33.5303955078125, 3.1274101734161377, 59.75175476074219]\n",
      "Epoch duration: 2.120267868041992\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_257\n",
      "Epoch: 258\n",
      "FGW torch.Size([29508, 5]) 8.499086106894538e-05\n",
      "Penalty params: tau=0.89459 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=258 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 258, train\n",
      " fgw:0.2514910\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2514910\n",
      "Measure Epoch 258, train\n",
      " similarity:0.0608074\n",
      " penlog:-7.2763933\n",
      "Metrics Epoch 258, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.3478261\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.6200000\n",
      " batch_invalid_valency_nodes:20.3478261\n",
      " batch_nodes_0degree:4.5400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.6200000\n",
      " batch_node_degree:1.5200000\n",
      "Logits [33.87177276611328, 3.135469436645508, 59.97389221191406]\n",
      "Epoch duration: 2.0986921787261963\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_258\n",
      "Epoch: 259\n",
      "FGW torch.Size([29508, 5]) 8.647593494970351e-05\n",
      "Penalty params: tau=0.89420 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=259 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 259, train\n",
      " fgw:0.2523805\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2523805\n",
      "Measure Epoch 259, train\n",
      " similarity:0.0627634\n",
      " penlog:-7.5511900\n",
      "Metrics Epoch 259, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.4347826\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.3800000\n",
      " batch_invalid_valency_nodes:19.7391304\n",
      " batch_nodes_0degree:4.4000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.3600000\n",
      " batch_node_degree:1.5339130\n",
      "Logits [33.511173248291016, 3.141920328140259, 59.90148162841797]\n",
      "Epoch duration: 1.962251901626587\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_259\n",
      "Epoch: 260\n",
      "FGW torch.Size([29508, 5]) 8.502721902914345e-05\n",
      "Penalty params: tau=0.89381 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=260 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 260, train\n",
      " fgw:0.2472223\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2472223\n",
      "Measure Epoch 260, train\n",
      " similarity:0.0543725\n",
      " penlog:-15.6314072\n",
      "Metrics Epoch 260, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.8695652\n",
      " batch_molecular_disconnected_validity:84.0000000\n",
      " batch_connected_components:12.4600000\n",
      " batch_invalid_valency_nodes:19.5652174\n",
      " batch_nodes_0degree:4.3400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.4600000\n",
      " batch_node_degree:1.5495652\n",
      "Logits [34.02506637573242, 3.153477907180786, 59.9676399230957]\n",
      "Epoch duration: 1.902648687362671\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_260\n",
      "Epoch: 261\n",
      "FGW torch.Size([29508, 5]) 8.561122376704589e-05\n",
      "Penalty params: tau=0.89343 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=261 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 261, train\n",
      " fgw:0.2483763\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2483763\n",
      "Measure Epoch 261, train\n",
      " similarity:0.0619315\n",
      " penlog:-7.4962039\n",
      "Metrics Epoch 261, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.2173913\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.3000000\n",
      " batch_invalid_valency_nodes:18.6086957\n",
      " batch_nodes_0degree:4.2000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.2800000\n",
      " batch_node_degree:1.5460870\n",
      "Logits [33.922386169433594, 3.150853157043457, 60.007381439208984]\n",
      "Epoch duration: 2.29879093170166\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 262\n",
      "FGW torch.Size([29508, 5]) 8.505498408339918e-05\n",
      "Penalty params: tau=0.89304 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=262 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 262, train\n",
      " fgw:0.2492368\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2492368\n",
      "Measure Epoch 262, train\n",
      " similarity:0.0578616\n",
      " penlog:-7.4723387\n",
      "Metrics Epoch 262, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.1304348\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.6400000\n",
      " batch_invalid_valency_nodes:20.2608696\n",
      " batch_nodes_0degree:4.5400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.6200000\n",
      " batch_node_degree:1.4886957\n",
      "Logits [33.93218994140625, 3.158395528793335, 60.149559020996094]\n",
      "Epoch duration: 2.150602102279663\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_262\n",
      "Epoch: 263\n",
      "FGW torch.Size([29508, 5]) 8.409294241573662e-05\n",
      "Penalty params: tau=0.89266 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=263 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 263, train\n",
      " fgw:0.2513198\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2513198\n",
      "Measure Epoch 263, train\n",
      " similarity:0.0584719\n",
      " penlog:-7.3134438\n",
      "Metrics Epoch 263, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.9565217\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:13.0400000\n",
      " batch_invalid_valency_nodes:21.2173913\n",
      " batch_nodes_0degree:4.7800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.0200000\n",
      " batch_node_degree:1.4678261\n",
      "Logits [34.33230209350586, 3.167820453643799, 60.34086990356445]\n",
      "Epoch duration: 2.077482223510742\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_263\n",
      "Epoch: 264\n",
      "FGW torch.Size([29508, 5]) 8.45562171889469e-05\n",
      "Penalty params: tau=0.89227 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=264 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 264, train\n",
      " fgw:0.2439577\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2439577\n",
      "Measure Epoch 264, train\n",
      " similarity:0.0575986\n",
      " penlog:-7.5499734\n",
      "Metrics Epoch 264, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.1304348\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.4400000\n",
      " batch_invalid_valency_nodes:18.9565217\n",
      " batch_nodes_0degree:4.2400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.4400000\n",
      " batch_node_degree:1.5304348\n",
      "Logits [33.96916580200195, 3.166141986846924, 60.244869232177734]\n",
      "Epoch duration: 1.963374376296997\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_264\n",
      "Epoch: 265\n",
      "FGW torch.Size([29508, 5]) 8.678471931489184e-05\n",
      "Penalty params: tau=0.89189 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=265 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 265, train\n",
      " fgw:0.2496422\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2496422\n",
      "Measure Epoch 265, train\n",
      " similarity:0.0560955\n",
      " penlog:-9.5299659\n",
      "Metrics Epoch 265, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.2173913\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:12.0000000\n",
      " batch_invalid_valency_nodes:17.9130435\n",
      " batch_nodes_0degree:3.9400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.0000000\n",
      " batch_node_degree:1.5895652\n",
      "Logits [34.26601791381836, 3.1833388805389404, 60.237979888916016]\n",
      "Epoch duration: 2.245283365249634\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_265\n",
      "Epoch: 266\n",
      "FGW torch.Size([29508, 5]) 8.38949708850123e-05\n",
      "Penalty params: tau=0.89150 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=266 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 266, train\n",
      " fgw:0.2426392\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2426392\n",
      "Measure Epoch 266, train\n",
      " similarity:0.0562671\n",
      " penlog:-9.5622910\n",
      "Metrics Epoch 266, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.3913043\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:11.8400000\n",
      " batch_invalid_valency_nodes:17.6521739\n",
      " batch_nodes_0degree:3.9200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.8400000\n",
      " batch_node_degree:1.6156522\n",
      "Logits [34.462493896484375, 3.182224750518799, 60.34416961669922]\n",
      "Epoch duration: 1.9844329357147217\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_266\n",
      "Epoch: 267\n",
      "FGW torch.Size([29508, 5]) 8.438377699349076e-05\n",
      "Penalty params: tau=0.89112 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=267 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 267, train\n",
      " fgw:0.2463705\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2463705\n",
      "Measure Epoch 267, train\n",
      " similarity:0.0603157\n",
      " penlog:-7.6440479\n",
      "Metrics Epoch 267, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.1304348\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.2400000\n",
      " batch_invalid_valency_nodes:20.2608696\n",
      " batch_nodes_0degree:4.5000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.2000000\n",
      " batch_node_degree:1.5495652\n",
      "Logits [34.28180694580078, 3.177288055419922, 60.432437896728516]\n",
      "Epoch duration: 2.0794568061828613\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_267\n",
      "Epoch: 268\n",
      "FGW torch.Size([29508, 5]) 8.470196917187423e-05\n",
      "Penalty params: tau=0.89073 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=268 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 268, train\n",
      " fgw:0.2457825\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2457825\n",
      "Measure Epoch 268, train\n",
      " similarity:0.0581073\n",
      " penlog:-5.2628113\n",
      "Metrics Epoch 268, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.2173913\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:12.7600000\n",
      " batch_invalid_valency_nodes:20.7826087\n",
      " batch_nodes_0degree:4.7000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.7600000\n",
      " batch_node_degree:1.4956522\n",
      "Logits [34.65277099609375, 3.187749147415161, 60.5620231628418]\n",
      "Epoch duration: 2.14438796043396\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_268\n",
      "Epoch: 269\n",
      "FGW torch.Size([29508, 5]) 8.423922554356977e-05\n",
      "Penalty params: tau=0.89035 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=269 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 269, train\n",
      " fgw:0.2394791\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2394791\n",
      "Measure Epoch 269, train\n",
      " similarity:0.0601791\n",
      " penlog:-3.2205148\n",
      "Metrics Epoch 269, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.8695652\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.0200000\n",
      " batch_invalid_valency_nodes:21.3043478\n",
      " batch_nodes_0degree:4.8600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.0200000\n",
      " batch_node_degree:1.4591304\n",
      "Logits [34.411415100097656, 3.174382448196411, 60.58990478515625]\n",
      "Epoch duration: 2.0136611461639404\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_269\n",
      "Epoch: 270\n",
      "FGW torch.Size([29508, 5]) 8.489733590977266e-05\n",
      "Penalty params: tau=0.88996 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=270 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 270, train\n",
      " fgw:0.2423116\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2423116\n",
      "Measure Epoch 270, train\n",
      " similarity:0.0565282\n",
      " penlog:-11.4385656\n",
      "Metrics Epoch 270, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.3913043\n",
      " batch_molecular_disconnected_validity:88.0000000\n",
      " batch_connected_components:12.5000000\n",
      " batch_invalid_valency_nodes:19.3043478\n",
      " batch_nodes_0degree:4.3000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.5000000\n",
      " batch_node_degree:1.5391304\n",
      "Logits [34.711891174316406, 3.2032790184020996, 60.47893142700195]\n",
      "Epoch duration: 1.9630563259124756\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 271\n",
      "FGW torch.Size([29508, 5]) 8.4935367340222e-05\n",
      "Penalty params: tau=0.88958 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=271 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 271, train\n",
      " fgw:0.2437219\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2437219\n",
      "Measure Epoch 271, train\n",
      " similarity:0.0592315\n",
      " penlog:-13.5326717\n",
      "Metrics Epoch 271, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.3913043\n",
      " batch_molecular_disconnected_validity:86.0000000\n",
      " batch_connected_components:12.1000000\n",
      " batch_invalid_valency_nodes:18.0869565\n",
      " batch_nodes_0degree:3.9400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.0800000\n",
      " batch_node_degree:1.5791304\n",
      "Logits [34.819725036621094, 3.2105977535247803, 60.5558967590332]\n",
      "Epoch duration: 1.9860942363739014\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_271\n",
      "Epoch: 272\n",
      "FGW torch.Size([29508, 5]) 8.332858851645142e-05\n",
      "Penalty params: tau=0.88919 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=272 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 272, train\n",
      " fgw:0.2450249\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2450249\n",
      "Measure Epoch 272, train\n",
      " similarity:0.0616238\n",
      " penlog:-7.4933389\n",
      "Metrics Epoch 272, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.3043478\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.4000000\n",
      " batch_invalid_valency_nodes:19.3913043\n",
      " batch_nodes_0degree:4.3400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.3800000\n",
      " batch_node_degree:1.5234783\n",
      "Logits [34.71818161010742, 3.195260763168335, 60.73725509643555]\n",
      "Epoch duration: 2.0141091346740723\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_272\n",
      "Epoch: 273\n",
      "FGW torch.Size([29508, 5]) 8.41403889353387e-05\n",
      "Penalty params: tau=0.88881 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=273 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 273, train\n",
      " fgw:0.2373514\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2373514\n",
      "Measure Epoch 273, train\n",
      " similarity:0.0615830\n",
      " penlog:-7.4231747\n",
      "Metrics Epoch 273, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.3043478\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.1800000\n",
      " batch_invalid_valency_nodes:18.7826087\n",
      " batch_nodes_0degree:4.1800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.1600000\n",
      " batch_node_degree:1.5443478\n",
      "Logits [34.900047302246094, 3.2248294353485107, 60.706111907958984]\n",
      "Epoch duration: 2.095893144607544\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_273\n",
      "Epoch: 274\n",
      "FGW torch.Size([29508, 5]) 8.506842277711257e-05\n",
      "Penalty params: tau=0.88843 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=274 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 274, train\n",
      " fgw:0.2420637\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2420637\n",
      "Measure Epoch 274, train\n",
      " similarity:0.0584924\n",
      " penlog:-9.5082559\n",
      "Metrics Epoch 274, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.5652174\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:12.2600000\n",
      " batch_invalid_valency_nodes:19.1304348\n",
      " batch_nodes_0degree:4.2200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.2600000\n",
      " batch_node_degree:1.5391304\n",
      "Logits [35.138309478759766, 3.2199041843414307, 60.87126922607422]\n",
      "Epoch duration: 2.0020134449005127\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_274\n",
      "Epoch: 275\n",
      "FGW torch.Size([29508, 5]) 8.310074190376326e-05\n",
      "Penalty params: tau=0.88804 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=275 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 275, train\n",
      " fgw:0.2429417\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2429417\n",
      "Measure Epoch 275, train\n",
      " similarity:0.0578237\n",
      " penlog:-7.5194050\n",
      "Metrics Epoch 275, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.5652174\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.3400000\n",
      " batch_invalid_valency_nodes:19.0434783\n",
      " batch_nodes_0degree:4.2200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.3400000\n",
      " batch_node_degree:1.5443478\n",
      "Logits [35.17285919189453, 3.233152151107788, 60.82856750488281]\n",
      "Epoch duration: 2.2318458557128906\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_275\n",
      "Epoch: 276\n",
      "FGW torch.Size([29508, 5]) 8.58394123497419e-05\n",
      "Penalty params: tau=0.88766 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=276 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 276, train\n",
      " fgw:0.2385983\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2385983\n",
      "Measure Epoch 276, train\n",
      " similarity:0.0614803\n",
      " penlog:-7.5509280\n",
      "Metrics Epoch 276, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.5652174\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:11.8800000\n",
      " batch_invalid_valency_nodes:17.3913043\n",
      " batch_nodes_0degree:3.8200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.8600000\n",
      " batch_node_degree:1.5947826\n",
      "Logits [35.353668212890625, 3.240128993988037, 60.95996856689453]\n",
      "Epoch duration: 2.0792508125305176\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_276\n",
      "Epoch: 277\n",
      "FGW torch.Size([29508, 5]) 8.221241296269e-05\n",
      "Penalty params: tau=0.88728 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=277 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 277, train\n",
      " fgw:0.2389411\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2389411\n",
      "Measure Epoch 277, train\n",
      " similarity:0.0591962\n",
      " penlog:-7.3664479\n",
      "Metrics Epoch 277, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.5652174\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.6400000\n",
      " batch_invalid_valency_nodes:19.8260870\n",
      " batch_nodes_0degree:4.4600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.6200000\n",
      " batch_node_degree:1.5078261\n",
      "Logits [35.4511604309082, 3.2301511764526367, 61.170562744140625]\n",
      "Epoch duration: 2.1661977767944336\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_277\n",
      "Epoch: 278\n",
      "FGW torch.Size([29508, 5]) 8.363838423974812e-05\n",
      "Penalty params: tau=0.88689 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=278 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 278, train\n",
      " fgw:0.2433596\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2433596\n",
      "Measure Epoch 278, train\n",
      " similarity:0.0630762\n",
      " penlog:-5.3505901\n",
      "Metrics Epoch 278, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.3043478\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:12.6800000\n",
      " batch_invalid_valency_nodes:19.8260870\n",
      " batch_nodes_0degree:4.5000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.6600000\n",
      " batch_node_degree:1.5008696\n",
      "Logits [35.40934371948242, 3.252103090286255, 61.13867950439453]\n",
      "Epoch duration: 2.119860887527466\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_278\n",
      "Epoch: 279\n",
      "FGW torch.Size([29508, 5]) 8.53067176649347e-05\n",
      "Penalty params: tau=0.88651 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=279 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 279, train\n",
      " fgw:0.2385040\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2385040\n",
      "Measure Epoch 279, train\n",
      " similarity:0.0628869\n",
      " penlog:-5.4709143\n",
      "Metrics Epoch 279, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.5652174\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:12.5800000\n",
      " batch_invalid_valency_nodes:19.5652174\n",
      " batch_nodes_0degree:4.4200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.5800000\n",
      " batch_node_degree:1.5113043\n",
      "Logits [35.64080047607422, 3.2463321685791016, 61.340293884277344]\n",
      "Epoch duration: 2.157017707824707\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 280\n",
      "FGW torch.Size([29508, 5]) 8.070722833508626e-05\n",
      "Penalty params: tau=0.88613 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=280 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 280, train\n",
      " fgw:0.2335365\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2335365\n",
      "Measure Epoch 280, train\n",
      " similarity:0.0558801\n",
      " penlog:-7.3532051\n",
      "Metrics Epoch 280, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.5652174\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.8600000\n",
      " batch_invalid_valency_nodes:20.6086957\n",
      " batch_nodes_0degree:4.6200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.8600000\n",
      " batch_node_degree:1.4973913\n",
      "Logits [35.71820068359375, 3.2764883041381836, 61.20579528808594]\n",
      "Epoch duration: 2.0409390926361084\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_280\n",
      "Epoch: 281\n",
      "FGW torch.Size([29508, 5]) 8.65503097884357e-05\n",
      "Penalty params: tau=0.88575 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=281 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 281, train\n",
      " fgw:0.2480104\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2480104\n",
      "Measure Epoch 281, train\n",
      " similarity:0.0613454\n",
      " penlog:-5.4019729\n",
      "Metrics Epoch 281, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.5652174\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:12.3200000\n",
      " batch_invalid_valency_nodes:19.2173913\n",
      " batch_nodes_0degree:4.3000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.3000000\n",
      " batch_node_degree:1.5356522\n",
      "Logits [35.79852294921875, 3.2760562896728516, 61.322364807128906]\n",
      "Epoch duration: 2.09252667427063\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_281\n",
      "Epoch: 282\n",
      "FGW torch.Size([29508, 5]) 8.38766572996974e-05\n",
      "Penalty params: tau=0.88536 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=282 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 282, train\n",
      " fgw:0.2394006\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2394006\n",
      "Measure Epoch 282, train\n",
      " similarity:0.0613036\n",
      " penlog:-5.4183016\n",
      "Metrics Epoch 282, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.7391304\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:12.3400000\n",
      " batch_invalid_valency_nodes:18.3478261\n",
      " batch_nodes_0degree:4.1200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.3200000\n",
      " batch_node_degree:1.5460870\n",
      "Logits [36.105613708496094, 3.2887182235717773, 61.375545501708984]\n",
      "Epoch duration: 2.0534932613372803\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_282\n",
      "Epoch: 283\n",
      "FGW torch.Size([29508, 5]) 8.118442929117009e-05\n",
      "Penalty params: tau=0.88498 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=283 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 283, train\n",
      " fgw:0.2353582\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2353582\n",
      "Measure Epoch 283, train\n",
      " similarity:0.0562359\n",
      " penlog:-7.3198696\n",
      "Metrics Epoch 283, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.6521739\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.6400000\n",
      " batch_invalid_valency_nodes:19.5652174\n",
      " batch_nodes_0degree:4.3400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.6200000\n",
      " batch_node_degree:1.5269565\n",
      "Logits [36.16599655151367, 3.3009352684020996, 61.30534744262695]\n",
      "Epoch duration: 1.9258220195770264\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_283\n",
      "Epoch: 284\n",
      "FGW torch.Size([29508, 5]) 8.659572631586343e-05\n",
      "Penalty params: tau=0.88460 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=284 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 284, train\n",
      " fgw:0.2366110\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2366110\n",
      "Measure Epoch 284, train\n",
      " similarity:0.0576480\n",
      " penlog:-5.3797832\n",
      "Metrics Epoch 284, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.6521739\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:12.4000000\n",
      " batch_invalid_valency_nodes:18.5217391\n",
      " batch_nodes_0degree:4.1200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.3600000\n",
      " batch_node_degree:1.5426087\n",
      "Logits [36.238365173339844, 3.2900736331939697, 61.480072021484375]\n",
      "Epoch duration: 2.155189037322998\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_284\n",
      "Epoch: 285\n",
      "FGW torch.Size([29508, 5]) 8.096382225630805e-05\n",
      "Penalty params: tau=0.88422 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=285 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 285, train\n",
      " fgw:0.2420263\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2420263\n",
      "Measure Epoch 285, train\n",
      " similarity:0.0507834\n",
      " penlog:-7.2385896\n",
      "Metrics Epoch 285, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.4782609\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:13.0000000\n",
      " batch_invalid_valency_nodes:20.5217391\n",
      " batch_nodes_0degree:4.5800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.0000000\n",
      " batch_node_degree:1.5026087\n",
      "Logits [36.309452056884766, 3.3171727657318115, 61.3624153137207]\n",
      "Epoch duration: 1.9627132415771484\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_285\n",
      "Epoch: 286\n",
      "FGW torch.Size([29508, 5]) 8.615477418061346e-05\n",
      "Penalty params: tau=0.88384 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=286 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 286, train\n",
      " fgw:0.2366133\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2366133\n",
      "Measure Epoch 286, train\n",
      " similarity:0.0612387\n",
      " penlog:-5.6661030\n",
      "Metrics Epoch 286, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.4782609\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:12.2600000\n",
      " batch_invalid_valency_nodes:17.7391304\n",
      " batch_nodes_0degree:4.0200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.2400000\n",
      " batch_node_degree:1.5495652\n",
      "Logits [36.320255279541016, 3.2905025482177734, 61.597259521484375]\n",
      "Epoch duration: 1.9904847145080566\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_286\n",
      "Epoch: 287\n",
      "FGW torch.Size([29508, 5]) 8.108944166451693e-05\n",
      "Penalty params: tau=0.88345 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=287 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 287, train\n",
      " fgw:0.2332181\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2332181\n",
      "Measure Epoch 287, train\n",
      " similarity:0.0573385\n",
      " penlog:-7.6804141\n",
      "Metrics Epoch 287, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.4782609\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.3400000\n",
      " batch_invalid_valency_nodes:18.3478261\n",
      " batch_nodes_0degree:4.1400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.3200000\n",
      " batch_node_degree:1.5565217\n",
      "Logits [36.29602813720703, 3.3179006576538086, 61.41868591308594]\n",
      "Epoch duration: 2.253418445587158\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_287\n",
      "Epoch: 288\n",
      "FGW torch.Size([29508, 5]) 8.464124402962625e-05\n",
      "Penalty params: tau=0.88307 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=288 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 288, train\n",
      " fgw:0.2480206\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2480206\n",
      "Measure Epoch 288, train\n",
      " similarity:0.0614783\n",
      " penlog:-5.6403651\n",
      "Metrics Epoch 288, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.4782609\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:12.1400000\n",
      " batch_invalid_valency_nodes:18.1739130\n",
      " batch_nodes_0degree:4.1000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.1200000\n",
      " batch_node_degree:1.5513043\n",
      "Logits [36.43593215942383, 3.2933218479156494, 61.671775817871094]\n",
      "Epoch duration: 2.268831729888916\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 289\n",
      "FGW torch.Size([29508, 5]) 8.241548493970186e-05\n",
      "Penalty params: tau=0.88269 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=289 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 289, train\n",
      " fgw:0.2322695\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2322695\n",
      "Measure Epoch 289, train\n",
      " similarity:0.0596006\n",
      " penlog:-7.5372603\n",
      "Metrics Epoch 289, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.6521739\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.4000000\n",
      " batch_invalid_valency_nodes:19.3043478\n",
      " batch_nodes_0degree:4.3600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.4000000\n",
      " batch_node_degree:1.5530435\n",
      "Logits [36.43384552001953, 3.317556858062744, 61.43669509887695]\n",
      "Epoch duration: 2.0555543899536133\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_289\n",
      "Epoch: 290\n",
      "FGW torch.Size([29508, 5]) 8.422700193477795e-05\n",
      "Penalty params: tau=0.88231 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=290 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 290, train\n",
      " fgw:0.2372340\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2372340\n",
      "Measure Epoch 290, train\n",
      " similarity:0.0593792\n",
      " penlog:-7.3586910\n",
      "Metrics Epoch 290, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.8260870\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.6200000\n",
      " batch_invalid_valency_nodes:21.3043478\n",
      " batch_nodes_0degree:4.7200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.6200000\n",
      " batch_node_degree:1.5217391\n",
      "Logits [36.57652282714844, 3.31843638420105, 61.540950775146484]\n",
      "Epoch duration: 2.053208112716675\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_290\n",
      "Epoch: 291\n",
      "FGW torch.Size([29508, 5]) 8.529780461685732e-05\n",
      "Penalty params: tau=0.88193 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=291 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 291, train\n",
      " fgw:0.2410776\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2410776\n",
      "Measure Epoch 291, train\n",
      " similarity:0.0568971\n",
      " penlog:-7.3367639\n",
      "Metrics Epoch 291, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.7391304\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.6400000\n",
      " batch_invalid_valency_nodes:20.1739130\n",
      " batch_nodes_0degree:4.4800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.6200000\n",
      " batch_node_degree:1.5217391\n",
      "Logits [36.57419967651367, 3.312685251235962, 61.780208587646484]\n",
      "Epoch duration: 2.098560094833374\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_291\n",
      "Epoch: 292\n",
      "FGW torch.Size([29508, 5]) 8.141055150190368e-05\n",
      "Penalty params: tau=0.88155 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=292 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 292, train\n",
      " fgw:0.2339294\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2339294\n",
      "Measure Epoch 292, train\n",
      " similarity:0.0560161\n",
      " penlog:-7.3026247\n",
      "Metrics Epoch 292, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.7391304\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.6400000\n",
      " batch_invalid_valency_nodes:18.5217391\n",
      " batch_nodes_0degree:4.1200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.6200000\n",
      " batch_node_degree:1.5321739\n",
      "Logits [36.765594482421875, 3.3367233276367188, 61.79069900512695]\n",
      "Epoch duration: 2.0993547439575195\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_292\n",
      "Epoch: 293\n",
      "FGW torch.Size([29508, 5]) 8.596904808655381e-05\n",
      "Penalty params: tau=0.88117 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=293 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 293, train\n",
      " fgw:0.2353387\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2353387\n",
      "Measure Epoch 293, train\n",
      " similarity:0.0619622\n",
      " penlog:-5.5133207\n",
      "Metrics Epoch 293, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.7391304\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:12.1000000\n",
      " batch_invalid_valency_nodes:16.5217391\n",
      " batch_nodes_0degree:3.7400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.0800000\n",
      " batch_node_degree:1.5547826\n",
      "Logits [36.83095169067383, 3.335169792175293, 62.02116394042969]\n",
      "Epoch duration: 2.1082117557525635\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_293\n",
      "Epoch: 294\n",
      "FGW torch.Size([29508, 5]) 8.048675226746127e-05\n",
      "Penalty params: tau=0.88079 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=294 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 294, train\n",
      " fgw:0.2317591\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2317591\n",
      "Measure Epoch 294, train\n",
      " similarity:0.0599567\n",
      " penlog:-7.5880660\n",
      "Metrics Epoch 294, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.6521739\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.1600000\n",
      " batch_invalid_valency_nodes:17.5652174\n",
      " batch_nodes_0degree:3.9600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.1400000\n",
      " batch_node_degree:1.5721739\n",
      "Logits [37.01606750488281, 3.367079973220825, 61.895694732666016]\n",
      "Epoch duration: 1.9830803871154785\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_294\n",
      "Epoch: 295\n",
      "FGW torch.Size([29508, 5]) 8.440430974587798e-05\n",
      "Penalty params: tau=0.88041 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=295 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 295, train\n",
      " fgw:0.2312552\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2312552\n",
      "Measure Epoch 295, train\n",
      " similarity:0.0633061\n",
      " penlog:-3.7457685\n",
      "Metrics Epoch 295, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.7391304\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:12.0600000\n",
      " batch_invalid_valency_nodes:17.8260870\n",
      " batch_nodes_0degree:4.0600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.0400000\n",
      " batch_node_degree:1.5530435\n",
      "Logits [37.087791442871094, 3.37384295463562, 61.991153717041016]\n",
      "Epoch duration: 1.9370307922363281\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_295\n",
      "Epoch: 296\n",
      "FGW torch.Size([29508, 5]) 8.254412387032062e-05\n",
      "Penalty params: tau=0.88003 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=296 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 296, train\n",
      " fgw:0.2318143\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2318143\n",
      "Measure Epoch 296, train\n",
      " similarity:0.0602511\n",
      " penlog:-3.5130522\n",
      "Metrics Epoch 296, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.8260870\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:12.5600000\n",
      " batch_invalid_valency_nodes:19.6521739\n",
      " batch_nodes_0degree:4.4600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.5600000\n",
      " batch_node_degree:1.5095652\n",
      "Logits [37.250823974609375, 3.389404058456421, 62.004520416259766]\n",
      "Epoch duration: 1.9190294742584229\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_296\n",
      "Epoch: 297\n",
      "FGW torch.Size([29508, 5]) 8.20514906081371e-05\n",
      "Penalty params: tau=0.87965 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=297 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 297, train\n",
      " fgw:0.2331349\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2331349\n",
      "Measure Epoch 297, train\n",
      " similarity:0.0573609\n",
      " penlog:-7.3693679\n",
      "Metrics Epoch 297, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.7391304\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.5000000\n",
      " batch_invalid_valency_nodes:19.8260870\n",
      " batch_nodes_0degree:4.4400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.5000000\n",
      " batch_node_degree:1.5304348\n",
      "Logits [37.37734603881836, 3.3960392475128174, 62.0713005065918]\n",
      "Epoch duration: 2.3095216751098633\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 298\n",
      "FGW torch.Size([29508, 5]) 8.47340197651647e-05\n",
      "Penalty params: tau=0.87927 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=298 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 298, train\n",
      " fgw:0.2357043\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2357043\n",
      "Measure Epoch 298, train\n",
      " similarity:0.0589399\n",
      " penlog:-9.4957234\n",
      "Metrics Epoch 298, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.7391304\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:11.6600000\n",
      " batch_invalid_valency_nodes:16.2608696\n",
      " batch_nodes_0degree:3.5600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.6200000\n",
      " batch_node_degree:1.6417391\n",
      "Logits [37.52230453491211, 3.4020566940307617, 62.07019805908203]\n",
      "Epoch duration: 2.2274816036224365\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_298\n",
      "Epoch: 299\n",
      "FGW torch.Size([29508, 5]) 8.116594108287245e-05\n",
      "Penalty params: tau=0.87889 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=299 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 299, train\n",
      " fgw:0.2389393\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2389393\n",
      "Measure Epoch 299, train\n",
      " similarity:0.0619905\n",
      " penlog:-9.4768704\n",
      "Metrics Epoch 299, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:11.9400000\n",
      " batch_invalid_valency_nodes:17.2173913\n",
      " batch_nodes_0degree:3.7800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.8800000\n",
      " batch_node_degree:1.6104348\n",
      "Logits [37.64043045043945, 3.403620481491089, 62.098148345947266]\n",
      "Epoch duration: 2.2189908027648926\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_299\n",
      "Epoch: 300\n",
      "FGW torch.Size([29508, 5]) 8.27716794447042e-05\n",
      "Penalty params: tau=0.87851 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=300 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 300, train\n",
      " fgw:0.2264959\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2264959\n",
      "Measure Epoch 300, train\n",
      " similarity:0.0645124\n",
      " penlog:-11.5279794\n",
      "Metrics Epoch 300, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.7391304\n",
      " batch_molecular_disconnected_validity:88.0000000\n",
      " batch_connected_components:11.8800000\n",
      " batch_invalid_valency_nodes:18.0000000\n",
      " batch_nodes_0degree:3.9600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.8400000\n",
      " batch_node_degree:1.5843478\n",
      "Logits [37.69489288330078, 3.4123897552490234, 62.183048248291016]\n",
      "Epoch duration: 2.181041955947876\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_300\n",
      "Epoch: 301\n",
      "FGW torch.Size([29508, 5]) 8.122620783979073e-05\n",
      "Penalty params: tau=0.87813 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=301 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 301, train\n",
      " fgw:0.2279755\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2279755\n",
      "Measure Epoch 301, train\n",
      " similarity:0.0638312\n",
      " penlog:-7.3783312\n",
      "Metrics Epoch 301, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.7391304\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.4400000\n",
      " batch_invalid_valency_nodes:19.2173913\n",
      " batch_nodes_0degree:4.3200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.4000000\n",
      " batch_node_degree:1.5304348\n",
      "Logits [37.70647048950195, 3.4172275066375732, 62.23439025878906]\n",
      "Epoch duration: 2.0695953369140625\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_301\n",
      "Epoch: 302\n",
      "FGW torch.Size([29508, 5]) 8.260042523033917e-05\n",
      "Penalty params: tau=0.87775 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=302 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 302, train\n",
      " fgw:0.2267735\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2267735\n",
      "Measure Epoch 302, train\n",
      " similarity:0.0675812\n",
      " penlog:-9.3758477\n",
      "Metrics Epoch 302, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.6521739\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:12.4600000\n",
      " batch_invalid_valency_nodes:19.3913043\n",
      " batch_nodes_0degree:4.3400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.4400000\n",
      " batch_node_degree:1.5304348\n",
      "Logits [37.83230972290039, 3.420478105545044, 62.29450988769531]\n",
      "Epoch duration: 1.9404082298278809\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_302\n",
      "Epoch: 303\n",
      "FGW torch.Size([29508, 5]) 8.146536129061133e-05\n",
      "Penalty params: tau=0.87737 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=303 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 303, train\n",
      " fgw:0.2263124\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2263124\n",
      "Measure Epoch 303, train\n",
      " similarity:0.0647104\n",
      " penlog:-11.4664758\n",
      "Metrics Epoch 303, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.7391304\n",
      " batch_molecular_disconnected_validity:88.0000000\n",
      " batch_connected_components:12.3000000\n",
      " batch_invalid_valency_nodes:19.3913043\n",
      " batch_nodes_0degree:4.2800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.3000000\n",
      " batch_node_degree:1.5530435\n",
      "Logits [38.14084243774414, 3.4432811737060547, 62.264400482177734]\n",
      "Epoch duration: 2.2767109870910645\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_303\n",
      "Epoch: 304\n",
      "FGW torch.Size([29508, 5]) 8.2579099398572e-05\n",
      "Penalty params: tau=0.87699 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=304 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 304, train\n",
      " fgw:0.2314629\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2314629\n",
      "Measure Epoch 304, train\n",
      " similarity:0.0663748\n",
      " penlog:-7.3832047\n",
      "Metrics Epoch 304, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.1800000\n",
      " batch_invalid_valency_nodes:19.1304348\n",
      " batch_nodes_0degree:4.2800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.1600000\n",
      " batch_node_degree:1.5443478\n",
      "Logits [38.292930603027344, 3.4417431354522705, 62.51932907104492]\n",
      "Epoch duration: 2.36555814743042\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_304\n",
      "Epoch: 305\n",
      "FGW torch.Size([29508, 5]) 8.002819231478497e-05\n",
      "Penalty params: tau=0.87662 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=305 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 305, train\n",
      " fgw:0.2250971\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2250971\n",
      "Measure Epoch 305, train\n",
      " similarity:0.0644234\n",
      " penlog:-7.4117017\n",
      "Metrics Epoch 305, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.1200000\n",
      " batch_invalid_valency_nodes:18.6956522\n",
      " batch_nodes_0degree:4.1400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.0800000\n",
      " batch_node_degree:1.5634783\n",
      "Logits [38.419376373291016, 3.4513909816741943, 62.60200500488281]\n",
      "Epoch duration: 1.9241857528686523\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_305\n",
      "Epoch: 306\n",
      "FGW torch.Size([29508, 5]) 8.241791510954499e-05\n",
      "Penalty params: tau=0.87624 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=306 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 306, train\n",
      " fgw:0.2237884\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2237884\n",
      "Measure Epoch 306, train\n",
      " similarity:0.0697695\n",
      " penlog:-7.5533848\n",
      "Metrics Epoch 306, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.7391304\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:11.6600000\n",
      " batch_invalid_valency_nodes:16.8695652\n",
      " batch_nodes_0degree:3.7400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.6000000\n",
      " batch_node_degree:1.6191304\n",
      "Logits [38.486061096191406, 3.453946113586426, 62.67286682128906]\n",
      "Epoch duration: 2.0353736877441406\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 307\n",
      "FGW torch.Size([29508, 5]) 8.027646981645375e-05\n",
      "Penalty params: tau=0.87586 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=307 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 307, train\n",
      " fgw:0.2286048\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2286048\n",
      "Measure Epoch 307, train\n",
      " similarity:0.0634314\n",
      " penlog:-7.4922690\n",
      "Metrics Epoch 307, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.7391304\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.1600000\n",
      " batch_invalid_valency_nodes:17.6521739\n",
      " batch_nodes_0degree:3.9600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.1200000\n",
      " batch_node_degree:1.5756522\n",
      "Logits [38.52641677856445, 3.4640121459960938, 62.679473876953125]\n",
      "Epoch duration: 2.382368803024292\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_307\n",
      "Epoch: 308\n",
      "FGW torch.Size([29508, 5]) 8.250162500189617e-05\n",
      "Penalty params: tau=0.87548 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=308 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 308, train\n",
      " fgw:0.2257329\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2257329\n",
      "Measure Epoch 308, train\n",
      " similarity:0.0661698\n",
      " penlog:-9.4270045\n",
      "Metrics Epoch 308, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.8260870\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:12.1400000\n",
      " batch_invalid_valency_nodes:18.0869565\n",
      " batch_nodes_0degree:4.0400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.1200000\n",
      " batch_node_degree:1.5547826\n",
      "Logits [38.68488693237305, 3.4666802883148193, 62.89448165893555]\n",
      "Epoch duration: 2.2731902599334717\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_308\n",
      "Epoch: 309\n",
      "FGW torch.Size([29508, 5]) 7.964904943946749e-05\n",
      "Penalty params: tau=0.87510 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=309 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 309, train\n",
      " fgw:0.2239303\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2239303\n",
      "Measure Epoch 309, train\n",
      " similarity:0.0639526\n",
      " penlog:-5.2920034\n",
      "Metrics Epoch 309, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:12.6600000\n",
      " batch_invalid_valency_nodes:20.0869565\n",
      " batch_nodes_0degree:4.5400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.6600000\n",
      " batch_node_degree:1.5078261\n",
      "Logits [38.85600662231445, 3.491020679473877, 62.810367584228516]\n",
      "Epoch duration: 2.0943853855133057\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_309\n",
      "Epoch: 310\n",
      "FGW torch.Size([29508, 5]) 8.390068978769705e-05\n",
      "Penalty params: tau=0.87472 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=310 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 310, train\n",
      " fgw:0.2279438\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2279438\n",
      "Measure Epoch 310, train\n",
      " similarity:0.0669163\n",
      " penlog:-5.3665244\n",
      "Metrics Epoch 310, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.8260870\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:12.1200000\n",
      " batch_invalid_valency_nodes:17.7391304\n",
      " batch_nodes_0degree:4.0200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.0800000\n",
      " batch_node_degree:1.5652174\n",
      "Logits [38.94809341430664, 3.4916837215423584, 62.9880485534668]\n",
      "Epoch duration: 2.1285388469696045\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_310\n",
      "Epoch: 311\n",
      "FGW torch.Size([29508, 5]) 7.823973282938823e-05\n",
      "Penalty params: tau=0.87435 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=311 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 311, train\n",
      " fgw:0.2242005\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2242005\n",
      "Measure Epoch 311, train\n",
      " similarity:0.0643954\n",
      " penlog:-3.3403231\n",
      "Metrics Epoch 311, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.7391304\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:12.4400000\n",
      " batch_invalid_valency_nodes:18.6956522\n",
      " batch_nodes_0degree:4.2200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.4000000\n",
      " batch_node_degree:1.5547826\n",
      "Logits [39.20289993286133, 3.5324177742004395, 63.01063919067383]\n",
      "Epoch duration: 2.1598868370056152\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_311\n",
      "Epoch: 312\n",
      "FGW torch.Size([29508, 5]) 8.372163574676961e-05\n",
      "Penalty params: tau=0.87397 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=312 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 312, train\n",
      " fgw:0.2265192\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2265192\n",
      "Measure Epoch 312, train\n",
      " similarity:0.0698813\n",
      " penlog:-5.5021341\n",
      "Metrics Epoch 312, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.7391304\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:11.8000000\n",
      " batch_invalid_valency_nodes:17.2173913\n",
      " batch_nodes_0degree:3.8200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.7600000\n",
      " batch_node_degree:1.6017391\n",
      "Logits [39.311622619628906, 3.520643472671509, 63.313636779785156]\n",
      "Epoch duration: 2.179438829421997\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_312\n",
      "Epoch: 313\n",
      "FGW torch.Size([29508, 5]) 7.674356311326846e-05\n",
      "Penalty params: tau=0.87359 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=313 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 313, train\n",
      " fgw:0.2299838\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2299838\n",
      "Measure Epoch 313, train\n",
      " similarity:0.0624496\n",
      " penlog:-3.3767599\n",
      "Metrics Epoch 313, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.5652174\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:12.4200000\n",
      " batch_invalid_valency_nodes:19.3913043\n",
      " batch_nodes_0degree:4.3600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.3800000\n",
      " batch_node_degree:1.5513043\n",
      "Logits [39.287418365478516, 3.5493836402893066, 63.16436004638672]\n",
      "Epoch duration: 2.0794453620910645\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_313\n",
      "Epoch: 314\n",
      "FGW torch.Size([29508, 5]) 8.867477299645543e-05\n",
      "Penalty params: tau=0.87322 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=314 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 314, train\n",
      " fgw:0.2378462\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2378462\n",
      "Measure Epoch 314, train\n",
      " similarity:0.0649567\n",
      " penlog:-13.7796683\n",
      "Metrics Epoch 314, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.1304348\n",
      " batch_molecular_disconnected_validity:86.0000000\n",
      " batch_connected_components:11.1400000\n",
      " batch_invalid_valency_nodes:16.5217391\n",
      " batch_nodes_0degree:3.5800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.1000000\n",
      " batch_node_degree:1.6400000\n",
      "Logits [39.743125915527344, 3.5203075408935547, 63.49593734741211]\n",
      "Epoch duration: 1.979363203048706\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_314\n",
      "Epoch: 315\n",
      "FGW torch.Size([29508, 5]) 7.233349606394768e-05\n",
      "Penalty params: tau=0.87284 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=315 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 315, train\n",
      " fgw:0.2451370\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2451370\n",
      "Measure Epoch 315, train\n",
      " similarity:0.0526081\n",
      " penlog:-7.3009656\n",
      "Metrics Epoch 315, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.4347826\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.9200000\n",
      " batch_invalid_valency_nodes:21.3913043\n",
      " batch_nodes_0degree:4.7600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.9200000\n",
      " batch_node_degree:1.5339130\n",
      "Logits [39.44401550292969, 3.5761330127716064, 62.9742431640625]\n",
      "Epoch duration: 2.092705488204956\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 316\n",
      "FGW torch.Size([29508, 5]) 9.559815225657076e-05\n",
      "Penalty params: tau=0.87246 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=316 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 316, train\n",
      " fgw:0.2474484\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2474484\n",
      "Measure Epoch 316, train\n",
      " similarity:0.0751976\n",
      " penlog:-11.6517176\n",
      "Metrics Epoch 316, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.5217391\n",
      " batch_molecular_disconnected_validity:88.0000000\n",
      " batch_connected_components:10.6200000\n",
      " batch_invalid_valency_nodes:15.7391304\n",
      " batch_nodes_0degree:3.3400000\n",
      " batch_nodes_7plus_degree:0.0200000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.4800000\n",
      " batch_node_degree:1.6573913\n",
      "Logits [39.654258728027344, 3.4913766384124756, 63.96059799194336]\n",
      "Epoch duration: 2.068561315536499\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_316\n",
      "Epoch: 317\n",
      "FGW torch.Size([29508, 5]) 6.675338954664767e-05\n",
      "Penalty params: tau=0.87208 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=317 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 317, train\n",
      " fgw:0.2472734\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2472734\n",
      "Measure Epoch 317, train\n",
      " similarity:0.0477747\n",
      " penlog:-3.1768432\n",
      "Metrics Epoch 317, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.1304348\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.4600000\n",
      " batch_invalid_valency_nodes:22.0000000\n",
      " batch_nodes_0degree:5.0000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.4600000\n",
      " batch_node_degree:1.5043478\n",
      "Logits [39.548038482666016, 3.5691027641296387, 62.9598388671875]\n",
      "Epoch duration: 2.0357463359832764\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_317\n",
      "Epoch: 318\n",
      "FGW torch.Size([29508, 5]) 8.849878940964118e-05\n",
      "Penalty params: tau=0.87171 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=318 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 318, train\n",
      " fgw:0.2299785\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2299785\n",
      "Measure Epoch 318, train\n",
      " similarity:0.0682519\n",
      " penlog:-15.7074026\n",
      "Metrics Epoch 318, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.5652174\n",
      " batch_molecular_disconnected_validity:84.0000000\n",
      " batch_connected_components:11.0000000\n",
      " batch_invalid_valency_nodes:14.8695652\n",
      " batch_nodes_0degree:3.2000000\n",
      " batch_nodes_7plus_degree:0.0200000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.9600000\n",
      " batch_node_degree:1.6834783\n",
      "Logits [39.426143646240234, 3.5121262073516846, 63.588382720947266]\n",
      "Epoch duration: 2.2090885639190674\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_318\n",
      "Epoch: 319\n",
      "FGW torch.Size([29508, 5]) 8.689668902661651e-05\n",
      "Penalty params: tau=0.87133 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=319 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 319, train\n",
      " fgw:0.2275414\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2275414\n",
      "Measure Epoch 319, train\n",
      " similarity:0.0692733\n",
      " penlog:-9.5072042\n",
      "Metrics Epoch 319, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.5652174\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:11.6600000\n",
      " batch_invalid_valency_nodes:17.3913043\n",
      " batch_nodes_0degree:3.8800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.6200000\n",
      " batch_node_degree:1.5565217\n",
      "Logits [39.43608856201172, 3.4935643672943115, 63.88319778442383]\n",
      "Epoch duration: 2.290133476257324\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_319\n",
      "Epoch: 320\n",
      "FGW torch.Size([29508, 5]) 6.925038178451359e-05\n",
      "Penalty params: tau=0.87096 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=320 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 320, train\n",
      " fgw:0.2369320\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2369320\n",
      "Measure Epoch 320, train\n",
      " similarity:0.0486890\n",
      " penlog:-3.1982593\n",
      "Metrics Epoch 320, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.2173913\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.5000000\n",
      " batch_invalid_valency_nodes:23.3913043\n",
      " batch_nodes_0degree:5.3200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.5000000\n",
      " batch_node_degree:1.4330435\n",
      "Logits [39.27978515625, 3.523592948913574, 63.469635009765625]\n",
      "Epoch duration: 2.0389275550842285\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_320\n",
      "Epoch: 321\n",
      "FGW torch.Size([29508, 5]) 9.187828254653141e-05\n",
      "Penalty params: tau=0.87058 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=321 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 321, train\n",
      " fgw:0.2306469\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2306469\n",
      "Measure Epoch 321, train\n",
      " similarity:0.0684369\n",
      " penlog:-15.8855428\n",
      "Metrics Epoch 321, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.3913043\n",
      " batch_molecular_disconnected_validity:84.0000000\n",
      " batch_connected_components:10.6200000\n",
      " batch_invalid_valency_nodes:14.0869565\n",
      " batch_nodes_0degree:2.9400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.5600000\n",
      " batch_node_degree:1.6991304\n",
      "Logits [39.21799850463867, 3.482492685317993, 63.54803466796875]\n",
      "Epoch duration: 2.0360851287841797\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_321\n",
      "Epoch: 322\n",
      "FGW torch.Size([29508, 5]) 7.471125718438998e-05\n",
      "Penalty params: tau=0.87020 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=322 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 322, train\n",
      " fgw:0.2282566\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2282566\n",
      "Measure Epoch 322, train\n",
      " similarity:0.0567157\n",
      " penlog:-13.5821780\n",
      "Metrics Epoch 322, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.7391304\n",
      " batch_molecular_disconnected_validity:86.0000000\n",
      " batch_connected_components:12.4000000\n",
      " batch_invalid_valency_nodes:18.4347826\n",
      " batch_nodes_0degree:4.0600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.4000000\n",
      " batch_node_degree:1.5686957\n",
      "Logits [39.3913688659668, 3.516162872314453, 63.205543518066406]\n",
      "Epoch duration: 2.110157012939453\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_322\n",
      "Epoch: 323\n",
      "FGW torch.Size([29508, 5]) 8.235314453486353e-05\n",
      "Penalty params: tau=0.86983 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=323 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 323, train\n",
      " fgw:0.2254841\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2254841\n",
      "Measure Epoch 323, train\n",
      " similarity:0.0672964\n",
      " penlog:-7.5577521\n",
      "Metrics Epoch 323, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.4782609\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:11.7600000\n",
      " batch_invalid_valency_nodes:17.0434783\n",
      " batch_nodes_0degree:3.8000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.7600000\n",
      " batch_node_degree:1.5721739\n",
      "Logits [38.90766906738281, 3.509028434753418, 63.20064926147461]\n",
      "Epoch duration: 2.116743326187134\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_323\n",
      "Epoch: 324\n",
      "FGW torch.Size([29508, 5]) 8.41719374875538e-05\n",
      "Penalty params: tau=0.86945 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=324 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 324, train\n",
      " fgw:0.2319444\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2319444\n",
      "Measure Epoch 324, train\n",
      " similarity:0.0669730\n",
      " penlog:-7.5365396\n",
      "Metrics Epoch 324, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.4782609\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:11.8200000\n",
      " batch_invalid_valency_nodes:17.1304348\n",
      " batch_nodes_0degree:3.8600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.8200000\n",
      " batch_node_degree:1.5373913\n",
      "Logits [38.79540252685547, 3.458000659942627, 63.38863754272461]\n",
      "Epoch duration: 2.133273124694824\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 325\n",
      "FGW torch.Size([29508, 5]) 7.452944555552676e-05\n",
      "Penalty params: tau=0.86908 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=325 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 325, train\n",
      " fgw:0.2244245\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2244245\n",
      "Measure Epoch 325, train\n",
      " similarity:0.0569180\n",
      " penlog:-5.3563440\n",
      "Metrics Epoch 325, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.4782609\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:12.5600000\n",
      " batch_invalid_valency_nodes:18.9565217\n",
      " batch_nodes_0degree:4.2800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.5600000\n",
      " batch_node_degree:1.5373913\n",
      "Logits [39.06173324584961, 3.50517201423645, 62.89835739135742]\n",
      "Epoch duration: 1.9219110012054443\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_325\n",
      "Epoch: 326\n",
      "FGW torch.Size([29508, 5]) 8.607211202615872e-05\n",
      "Penalty params: tau=0.86870 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=326 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 326, train\n",
      " fgw:0.2240444\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2240444\n",
      "Measure Epoch 326, train\n",
      " similarity:0.0648297\n",
      " penlog:-13.8521556\n",
      "Metrics Epoch 326, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.7391304\n",
      " batch_molecular_disconnected_validity:86.0000000\n",
      " batch_connected_components:10.8400000\n",
      " batch_invalid_valency_nodes:13.7391304\n",
      " batch_nodes_0degree:2.9400000\n",
      " batch_nodes_7plus_degree:0.0200000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.7400000\n",
      " batch_node_degree:1.7200000\n",
      "Logits [38.92273712158203, 3.4871160984039307, 62.964359283447266]\n",
      "Epoch duration: 2.053760528564453\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_326\n",
      "Epoch: 327\n",
      "FGW torch.Size([29508, 5]) 7.656559319002554e-05\n",
      "Penalty params: tau=0.86833 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=327 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 327, train\n",
      " fgw:0.2237577\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2237577\n",
      "Measure Epoch 327, train\n",
      " similarity:0.0600953\n",
      " penlog:-5.4027206\n",
      "Metrics Epoch 327, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.5652174\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:12.6400000\n",
      " batch_invalid_valency_nodes:19.1304348\n",
      " batch_nodes_0degree:4.3400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.6400000\n",
      " batch_node_degree:1.5147826\n",
      "Logits [38.85091018676758, 3.4638872146606445, 63.170143127441406]\n",
      "Epoch duration: 1.961902141571045\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_327\n",
      "Epoch: 328\n",
      "FGW torch.Size([29508, 5]) 8.166790212271735e-05\n",
      "Penalty params: tau=0.86795 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=328 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 328, train\n",
      " fgw:0.2261065\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2261065\n",
      "Measure Epoch 328, train\n",
      " similarity:0.0668350\n",
      " penlog:-5.5656785\n",
      "Metrics Epoch 328, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.7391304\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:11.8400000\n",
      " batch_invalid_valency_nodes:16.6956522\n",
      " batch_nodes_0degree:3.7400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.8400000\n",
      " batch_node_degree:1.5826087\n",
      "Logits [39.102996826171875, 3.5084269046783447, 62.981685638427734]\n",
      "Epoch duration: 2.0362439155578613\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_328\n",
      "Epoch: 329\n",
      "FGW torch.Size([29508, 5]) 8.42261579236947e-05\n",
      "Penalty params: tau=0.86758 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=329 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 329, train\n",
      " fgw:0.2194236\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2194236\n",
      "Measure Epoch 329, train\n",
      " similarity:0.0706289\n",
      " penlog:-7.7209670\n",
      "Metrics Epoch 329, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.7391304\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:11.3600000\n",
      " batch_invalid_valency_nodes:15.2173913\n",
      " batch_nodes_0degree:3.4000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.3600000\n",
      " batch_node_degree:1.6278261\n",
      "Logits [39.13737869262695, 3.4825901985168457, 63.158145904541016]\n",
      "Epoch duration: 1.9453339576721191\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_329\n",
      "Epoch: 330\n",
      "FGW torch.Size([29508, 5]) 7.377626752713695e-05\n",
      "Penalty params: tau=0.86720 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=330 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 330, train\n",
      " fgw:0.2209719\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2209719\n",
      "Measure Epoch 330, train\n",
      " similarity:0.0593750\n",
      " penlog:-5.4268620\n",
      "Metrics Epoch 330, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.7391304\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:12.9400000\n",
      " batch_invalid_valency_nodes:20.6956522\n",
      " batch_nodes_0degree:4.6800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.9400000\n",
      " batch_node_degree:1.4904348\n",
      "Logits [39.07259750366211, 3.481660842895508, 63.1767578125]\n",
      "Epoch duration: 1.9748525619506836\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_330\n",
      "Epoch: 331\n",
      "FGW torch.Size([29508, 5]) 8.753937436267734e-05\n",
      "Penalty params: tau=0.86683 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=331 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 331, train\n",
      " fgw:0.2243501\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2243501\n",
      "Measure Epoch 331, train\n",
      " similarity:0.0728236\n",
      " penlog:-9.6590851\n",
      "Metrics Epoch 331, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.5652174\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:10.7600000\n",
      " batch_invalid_valency_nodes:14.3478261\n",
      " batch_nodes_0degree:3.1400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.6800000\n",
      " batch_node_degree:1.6852174\n",
      "Logits [39.13975143432617, 3.5189802646636963, 63.26831817626953]\n",
      "Epoch duration: 2.045543670654297\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_331\n",
      "Epoch: 332\n",
      "FGW torch.Size([29508, 5]) 7.279995770659298e-05\n",
      "Penalty params: tau=0.86646 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=332 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 332, train\n",
      " fgw:0.2237546\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2237546\n",
      "Measure Epoch 332, train\n",
      " similarity:0.0594030\n",
      " penlog:-7.3866041\n",
      "Metrics Epoch 332, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.8260870\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.6200000\n",
      " batch_invalid_valency_nodes:19.2173913\n",
      " batch_nodes_0degree:4.3400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.6200000\n",
      " batch_node_degree:1.5356522\n",
      "Logits [39.372249603271484, 3.5094618797302246, 63.39976119995117]\n",
      "Epoch duration: 2.114607810974121\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_332\n",
      "Epoch: 333\n",
      "FGW torch.Size([29508, 5]) 8.387592242797837e-05\n",
      "Penalty params: tau=0.86608 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=333 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 333, train\n",
      " fgw:0.2195063\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2195063\n",
      "Measure Epoch 333, train\n",
      " similarity:0.0689357\n",
      " penlog:-17.7884112\n",
      "Metrics Epoch 333, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.8260870\n",
      " batch_molecular_disconnected_validity:82.0000000\n",
      " batch_connected_components:10.7000000\n",
      " batch_invalid_valency_nodes:13.3913043\n",
      " batch_nodes_0degree:2.8200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.6200000\n",
      " batch_node_degree:1.7200000\n",
      "Logits [39.0918083190918, 3.4943525791168213, 63.49904251098633]\n",
      "Epoch duration: 2.239809274673462\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 334\n",
      "FGW torch.Size([29508, 5]) 7.823020860087126e-05\n",
      "Penalty params: tau=0.86571 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=334 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 334, train\n",
      " fgw:0.2218117\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2218117\n",
      "Measure Epoch 334, train\n",
      " similarity:0.0613450\n",
      " penlog:-11.5913798\n",
      "Metrics Epoch 334, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.7391304\n",
      " batch_molecular_disconnected_validity:88.0000000\n",
      " batch_connected_components:11.3600000\n",
      " batch_invalid_valency_nodes:15.4782609\n",
      " batch_nodes_0degree:3.3400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.3600000\n",
      " batch_node_degree:1.6521739\n",
      "Logits [39.2652473449707, 3.5291290283203125, 63.312347412109375]\n",
      "Epoch duration: 2.1060385704040527\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_334\n",
      "Epoch: 335\n",
      "FGW torch.Size([29508, 5]) 7.86375385359861e-05\n",
      "Penalty params: tau=0.86533 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=335 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 335, train\n",
      " fgw:0.2286982\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2286982\n",
      "Measure Epoch 335, train\n",
      " similarity:0.0595081\n",
      " penlog:-7.5786047\n",
      "Metrics Epoch 335, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.1800000\n",
      " batch_invalid_valency_nodes:17.6521739\n",
      " batch_nodes_0degree:3.9600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.1800000\n",
      " batch_node_degree:1.5547826\n",
      "Logits [39.57904815673828, 3.5210824012756348, 63.42588424682617]\n",
      "Epoch duration: 2.0316662788391113\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_335\n",
      "Epoch: 336\n",
      "FGW torch.Size([29508, 5]) 8.692780829733238e-05\n",
      "Penalty params: tau=0.86496 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=336 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 336, train\n",
      " fgw:0.2271492\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2271492\n",
      "Measure Epoch 336, train\n",
      " similarity:0.0716296\n",
      " penlog:-7.7743291\n",
      "Metrics Epoch 336, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:11.0400000\n",
      " batch_invalid_valency_nodes:15.3043478\n",
      " batch_nodes_0degree:3.4000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.9800000\n",
      " batch_node_degree:1.6469565\n",
      "Logits [39.48693084716797, 3.5085067749023438, 63.56232833862305]\n",
      "Epoch duration: 2.041595220565796\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_336\n",
      "Epoch: 337\n",
      "FGW torch.Size([29508, 5]) 7.118623034330085e-05\n",
      "Penalty params: tau=0.86459 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=337 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 337, train\n",
      " fgw:0.2244285\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2244285\n",
      "Measure Epoch 337, train\n",
      " similarity:0.0573044\n",
      " penlog:-3.2848517\n",
      "Metrics Epoch 337, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.6521739\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.0600000\n",
      " batch_invalid_valency_nodes:21.1304348\n",
      " batch_nodes_0degree:4.8200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.0600000\n",
      " batch_node_degree:1.4817391\n",
      "Logits [39.39814376831055, 3.536555528640747, 63.2147331237793]\n",
      "Epoch duration: 2.0181450843811035\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_337\n",
      "Epoch: 338\n",
      "FGW torch.Size([29508, 5]) 8.72735254233703e-05\n",
      "Penalty params: tau=0.86421 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=338 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 338, train\n",
      " fgw:0.2260816\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2260816\n",
      "Measure Epoch 338, train\n",
      " similarity:0.0782265\n",
      " penlog:-9.8207233\n",
      "Metrics Epoch 338, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.8260870\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:10.4600000\n",
      " batch_invalid_valency_nodes:13.6521739\n",
      " batch_nodes_0degree:2.9400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.3800000\n",
      " batch_node_degree:1.7095652\n",
      "Logits [39.481937408447266, 3.501042127609253, 63.468448638916016]\n",
      "Epoch duration: 2.1139121055603027\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_338\n",
      "Epoch: 339\n",
      "FGW torch.Size([29508, 5]) 7.418241148116067e-05\n",
      "Penalty params: tau=0.86384 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=339 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 339, train\n",
      " fgw:0.2185004\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2185004\n",
      "Measure Epoch 339, train\n",
      " similarity:0.0634961\n",
      " penlog:-3.4645799\n",
      "Metrics Epoch 339, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:12.4200000\n",
      " batch_invalid_valency_nodes:17.9130435\n",
      " batch_nodes_0degree:4.0600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.4200000\n",
      " batch_node_degree:1.5547826\n",
      "Logits [39.58268356323242, 3.5057942867279053, 63.36223220825195]\n",
      "Epoch duration: 2.1605138778686523\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_339\n",
      "Epoch: 340\n",
      "FGW torch.Size([29508, 5]) 8.058318053372204e-05\n",
      "Penalty params: tau=0.86347 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=340 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 340, train\n",
      " fgw:0.2164854\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2164854\n",
      "Measure Epoch 340, train\n",
      " similarity:0.0698640\n",
      " penlog:-7.3996331\n",
      "Metrics Epoch 340, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.6521739\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:11.6800000\n",
      " batch_invalid_valency_nodes:16.0869565\n",
      " batch_nodes_0degree:3.6000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.6400000\n",
      " batch_node_degree:1.5982609\n",
      "Logits [39.270790100097656, 3.504469394683838, 63.40140151977539]\n",
      "Epoch duration: 2.1948599815368652\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_340\n",
      "Epoch: 341\n",
      "FGW torch.Size([29508, 5]) 8.387848356505856e-05\n",
      "Penalty params: tau=0.86309 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=341 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 341, train\n",
      " fgw:0.2240664\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2240664\n",
      "Measure Epoch 341, train\n",
      " similarity:0.0689883\n",
      " penlog:-11.5614997\n",
      "Metrics Epoch 341, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:88.0000000\n",
      " batch_connected_components:11.5000000\n",
      " batch_invalid_valency_nodes:15.3043478\n",
      " batch_nodes_0degree:3.3800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.4200000\n",
      " batch_node_degree:1.6278261\n",
      "Logits [39.501346588134766, 3.4998068809509277, 63.549251556396484]\n",
      "Epoch duration: 1.9485299587249756\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_341\n",
      "Epoch: 342\n",
      "FGW torch.Size([29508, 5]) 7.405735232168809e-05\n",
      "Penalty params: tau=0.86272 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=342 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 342, train\n",
      " fgw:0.2175696\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2175696\n",
      "Measure Epoch 342, train\n",
      " similarity:0.0609862\n",
      " penlog:-7.4621180\n",
      "Metrics Epoch 342, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.3800000\n",
      " batch_invalid_valency_nodes:17.3913043\n",
      " batch_nodes_0degree:3.9000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.3800000\n",
      " batch_node_degree:1.5634783\n",
      "Logits [39.86156463623047, 3.533730983734131, 63.383296966552734]\n",
      "Epoch duration: 2.0001683235168457\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 343\n",
      "FGW torch.Size([29508, 5]) 8.612577221356332e-05\n",
      "Penalty params: tau=0.86235 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=343 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 343, train\n",
      " fgw:0.2246759\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2246759\n",
      "Measure Epoch 343, train\n",
      " similarity:0.0715990\n",
      " penlog:-13.8016100\n",
      "Metrics Epoch 343, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.8260870\n",
      " batch_molecular_disconnected_validity:86.0000000\n",
      " batch_connected_components:10.8400000\n",
      " batch_invalid_valency_nodes:12.7826087\n",
      " batch_nodes_0degree:2.7600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.7600000\n",
      " batch_node_degree:1.6921739\n",
      "Logits [39.70420455932617, 3.545984983444214, 63.42901611328125]\n",
      "Epoch duration: 1.9466373920440674\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_343\n",
      "Epoch: 344\n",
      "FGW torch.Size([29508, 5]) 7.400453614536673e-05\n",
      "Penalty params: tau=0.86198 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=344 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 344, train\n",
      " fgw:0.2219264\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2219264\n",
      "Measure Epoch 344, train\n",
      " similarity:0.0623701\n",
      " penlog:-5.4944627\n",
      "Metrics Epoch 344, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:12.5800000\n",
      " batch_invalid_valency_nodes:19.0434783\n",
      " batch_nodes_0degree:4.3000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.5800000\n",
      " batch_node_degree:1.5060870\n",
      "Logits [39.8284797668457, 3.553220748901367, 63.453155517578125]\n",
      "Epoch duration: 2.0028767585754395\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_344\n",
      "Epoch: 345\n",
      "FGW torch.Size([29508, 5]) 8.277299639303237e-05\n",
      "Penalty params: tau=0.86161 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=345 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 345, train\n",
      " fgw:0.2160755\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2160755\n",
      "Measure Epoch 345, train\n",
      " similarity:0.0689935\n",
      " penlog:-9.7355954\n",
      "Metrics Epoch 345, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:11.3000000\n",
      " batch_invalid_valency_nodes:15.3043478\n",
      " batch_nodes_0degree:3.3400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.3000000\n",
      " batch_node_degree:1.6173913\n",
      "Logits [39.992862701416016, 3.55961275100708, 63.50687789916992]\n",
      "Epoch duration: 2.171435832977295\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_345\n",
      "Epoch: 346\n",
      "FGW torch.Size([29508, 5]) 8.066464943112805e-05\n",
      "Penalty params: tau=0.86123 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=346 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 346, train\n",
      " fgw:0.2143952\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2143952\n",
      "Measure Epoch 346, train\n",
      " similarity:0.0648962\n",
      " penlog:-9.6275680\n",
      "Metrics Epoch 346, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.8260870\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:11.6000000\n",
      " batch_invalid_valency_nodes:16.0000000\n",
      " batch_nodes_0degree:3.5000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.6000000\n",
      " batch_node_degree:1.6104348\n",
      "Logits [40.14977264404297, 3.579374313354492, 63.45698165893555]\n",
      "Epoch duration: 2.0242555141448975\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_346\n",
      "Epoch: 347\n",
      "FGW torch.Size([29508, 5]) 7.727872434770688e-05\n",
      "Penalty params: tau=0.86086 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=347 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 347, train\n",
      " fgw:0.2237145\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2237145\n",
      "Measure Epoch 347, train\n",
      " similarity:0.0633019\n",
      " penlog:-5.4205821\n",
      "Metrics Epoch 347, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.7391304\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:12.3000000\n",
      " batch_invalid_valency_nodes:17.3913043\n",
      " batch_nodes_0degree:3.9000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.3000000\n",
      " batch_node_degree:1.5495652\n",
      "Logits [40.22246170043945, 3.569627046585083, 63.66086959838867]\n",
      "Epoch duration: 2.1350061893463135\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_347\n",
      "Epoch: 348\n",
      "FGW torch.Size([29508, 5]) 8.490178151987493e-05\n",
      "Penalty params: tau=0.86049 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=348 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 348, train\n",
      " fgw:0.2204061\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2204061\n",
      "Measure Epoch 348, train\n",
      " similarity:0.0678124\n",
      " penlog:-7.5501873\n",
      "Metrics Epoch 348, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:11.0400000\n",
      " batch_invalid_valency_nodes:14.1739130\n",
      " batch_nodes_0degree:3.0800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.0000000\n",
      " batch_node_degree:1.6660870\n",
      "Logits [40.293155670166016, 3.570831537246704, 63.80884552001953]\n",
      "Epoch duration: 2.0747971534729004\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_348\n",
      "Epoch: 349\n",
      "FGW torch.Size([29508, 5]) 7.444390212185681e-05\n",
      "Penalty params: tau=0.86012 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=349 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 349, train\n",
      " fgw:0.2212190\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2212190\n",
      "Measure Epoch 349, train\n",
      " similarity:0.0576502\n",
      " penlog:-5.3493373\n",
      "Metrics Epoch 349, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:12.4000000\n",
      " batch_invalid_valency_nodes:17.8260870\n",
      " batch_nodes_0degree:4.0000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.4000000\n",
      " batch_node_degree:1.5669565\n",
      "Logits [40.45269012451172, 3.6044225692749023, 63.6036262512207]\n",
      "Epoch duration: 2.0487403869628906\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_349\n",
      "Epoch: 350\n",
      "FGW torch.Size([29508, 5]) 8.446355059277266e-05\n",
      "Penalty params: tau=0.85975 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=350 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 350, train\n",
      " fgw:0.2181381\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2181381\n",
      "Measure Epoch 350, train\n",
      " similarity:0.0730278\n",
      " penlog:-7.8453739\n",
      "Metrics Epoch 350, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:10.7000000\n",
      " batch_invalid_valency_nodes:13.8260870\n",
      " batch_nodes_0degree:3.0400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.6200000\n",
      " batch_node_degree:1.6886957\n",
      "Logits [40.28020477294922, 3.5831298828125, 63.88176727294922]\n",
      "Epoch duration: 1.9964728355407715\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_350\n",
      "Epoch: 351\n",
      "FGW torch.Size([29508, 5]) 7.47014200896956e-05\n",
      "Penalty params: tau=0.85938 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=351 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 351, train\n",
      " fgw:0.2178836\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2178836\n",
      "Measure Epoch 351, train\n",
      " similarity:0.0660881\n",
      " penlog:-3.4549460\n",
      "Metrics Epoch 351, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:12.3000000\n",
      " batch_invalid_valency_nodes:18.0000000\n",
      " batch_nodes_0degree:4.0800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.3000000\n",
      " batch_node_degree:1.5373913\n",
      "Logits [40.328372955322266, 3.591296672821045, 63.823822021484375]\n",
      "Epoch duration: 2.2946906089782715\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 352\n",
      "FGW torch.Size([29508, 5]) 8.146036270773038e-05\n",
      "Penalty params: tau=0.85901 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=352 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 352, train\n",
      " fgw:0.2127165\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2127165\n",
      "Measure Epoch 352, train\n",
      " similarity:0.0739471\n",
      " penlog:-3.5965641\n",
      "Metrics Epoch 352, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.8260870\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:11.2400000\n",
      " batch_invalid_valency_nodes:14.3478261\n",
      " batch_nodes_0degree:3.2400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.2200000\n",
      " batch_node_degree:1.6486957\n",
      "Logits [40.46385192871094, 3.6088273525238037, 63.77201843261719]\n",
      "Epoch duration: 2.1003408432006836\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_352\n",
      "Epoch: 353\n",
      "FGW torch.Size([29508, 5]) 7.823995838407427e-05\n",
      "Penalty params: tau=0.85863 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=353 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 353, train\n",
      " fgw:0.2173584\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2173584\n",
      "Measure Epoch 353, train\n",
      " similarity:0.0677101\n",
      " penlog:-3.4422111\n",
      "Metrics Epoch 353, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:12.1000000\n",
      " batch_invalid_valency_nodes:16.4347826\n",
      " batch_nodes_0degree:3.7200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.1000000\n",
      " batch_node_degree:1.5739130\n",
      "Logits [40.70149612426758, 3.6101839542388916, 63.82155227661133]\n",
      "Epoch duration: 2.0298800468444824\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_353\n",
      "Epoch: 354\n",
      "FGW torch.Size([29508, 5]) 7.996566273504868e-05\n",
      "Penalty params: tau=0.85826 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=354 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 354, train\n",
      " fgw:0.2188974\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2188974\n",
      "Measure Epoch 354, train\n",
      " similarity:0.0686614\n",
      " penlog:-3.4118566\n",
      "Metrics Epoch 354, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:11.7600000\n",
      " batch_invalid_valency_nodes:14.9565217\n",
      " batch_nodes_0degree:3.3800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.7200000\n",
      " batch_node_degree:1.6121739\n",
      "Logits [40.748836517333984, 3.6170055866241455, 64.02289581298828]\n",
      "Epoch duration: 2.296694278717041\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_354\n",
      "Epoch: 355\n",
      "FGW torch.Size([29508, 5]) 8.066563896136358e-05\n",
      "Penalty params: tau=0.85789 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=355 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 355, train\n",
      " fgw:0.2180193\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2180193\n",
      "Measure Epoch 355, train\n",
      " similarity:0.0724883\n",
      " penlog:-9.5032826\n",
      "Metrics Epoch 355, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:11.5200000\n",
      " batch_invalid_valency_nodes:15.1304348\n",
      " batch_nodes_0degree:3.3200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.4600000\n",
      " batch_node_degree:1.6347826\n",
      "Logits [40.74788284301758, 3.6311709880828857, 64.18061065673828]\n",
      "Epoch duration: 2.091080904006958\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_355\n",
      "Epoch: 356\n",
      "FGW torch.Size([29508, 5]) 7.667457248317078e-05\n",
      "Penalty params: tau=0.85752 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=356 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 356, train\n",
      " fgw:0.2217286\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2217286\n",
      "Measure Epoch 356, train\n",
      " similarity:0.0722252\n",
      " penlog:-7.5171808\n",
      "Metrics Epoch 356, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:11.8600000\n",
      " batch_invalid_valency_nodes:15.5652174\n",
      " batch_nodes_0degree:3.4800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.8000000\n",
      " batch_node_degree:1.6243478\n",
      "Logits [40.953365325927734, 3.6409013271331787, 64.23139190673828]\n",
      "Epoch duration: 2.0445797443389893\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_356\n",
      "Epoch: 357\n",
      "FGW torch.Size([29508, 5]) 8.338740735780448e-05\n",
      "Penalty params: tau=0.85715 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=357 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 357, train\n",
      " fgw:0.2205666\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2205666\n",
      "Measure Epoch 357, train\n",
      " similarity:0.0836700\n",
      " penlog:-13.8261727\n",
      "Metrics Epoch 357, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:86.0000000\n",
      " batch_connected_components:10.5000000\n",
      " batch_invalid_valency_nodes:13.6521739\n",
      " batch_nodes_0degree:2.9400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:94.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.4200000\n",
      " batch_node_degree:1.7217391\n",
      "Logits [41.098304748535156, 3.666236639022827, 64.16387939453125]\n",
      "Epoch duration: 2.052536725997925\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_357\n",
      "Epoch: 358\n",
      "FGW torch.Size([29508, 5]) 7.387371442746371e-05\n",
      "Penalty params: tau=0.85678 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=358 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 358, train\n",
      " fgw:0.2183945\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2183945\n",
      "Measure Epoch 358, train\n",
      " similarity:0.0662589\n",
      " penlog:-3.4458386\n",
      "Metrics Epoch 358, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:12.5600000\n",
      " batch_invalid_valency_nodes:19.0434783\n",
      " batch_nodes_0degree:4.3400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.5600000\n",
      " batch_node_degree:1.4939130\n",
      "Logits [41.23556137084961, 3.6730847358703613, 64.2288589477539]\n",
      "Epoch duration: 2.166782855987549\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_358\n",
      "Epoch: 359\n",
      "FGW torch.Size([29508, 5]) 8.319782500620931e-05\n",
      "Penalty params: tau=0.85641 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=359 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 359, train\n",
      " fgw:0.2143386\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2143386\n",
      "Measure Epoch 359, train\n",
      " similarity:0.0744721\n",
      " penlog:-7.7019461\n",
      "Metrics Epoch 359, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:11.0000000\n",
      " batch_invalid_valency_nodes:15.5652174\n",
      " batch_nodes_0degree:3.4600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.9400000\n",
      " batch_node_degree:1.6208696\n",
      "Logits [41.3487434387207, 3.6567606925964355, 64.46438598632812]\n",
      "Epoch duration: 2.0449557304382324\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_359\n",
      "Epoch: 360\n",
      "FGW torch.Size([29508, 5]) 7.453443686245009e-05\n",
      "Penalty params: tau=0.85604 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=360 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 360, train\n",
      " fgw:0.2183524\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2183524\n",
      "Measure Epoch 360, train\n",
      " similarity:0.0615717\n",
      " penlog:-9.5066717\n",
      "Metrics Epoch 360, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:12.2200000\n",
      " batch_invalid_valency_nodes:17.3913043\n",
      " batch_nodes_0degree:3.8200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.2200000\n",
      " batch_node_degree:1.5878261\n",
      "Logits [41.53257751464844, 3.688007116317749, 64.26492309570312]\n",
      "Epoch duration: 2.0856423377990723\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 361\n",
      "FGW torch.Size([29508, 5]) 8.445949788438156e-05\n",
      "Penalty params: tau=0.85567 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=361 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 361, train\n",
      " fgw:0.2201486\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2201486\n",
      "Measure Epoch 361, train\n",
      " similarity:0.0720636\n",
      " penlog:-17.8830056\n",
      "Metrics Epoch 361, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:82.0000000\n",
      " batch_connected_components:10.4400000\n",
      " batch_invalid_valency_nodes:12.5217391\n",
      " batch_nodes_0degree:2.5800000\n",
      " batch_nodes_7plus_degree:0.0200000\n",
      " invalid_euler_toofew:94.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.3600000\n",
      " batch_node_degree:1.7652174\n",
      "Logits [41.39120864868164, 3.673870801925659, 64.56263732910156]\n",
      "Epoch duration: 2.208534002304077\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_361\n",
      "Epoch: 362\n",
      "FGW torch.Size([29508, 5]) 7.157091022236273e-05\n",
      "Penalty params: tau=0.85530 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=362 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 362, train\n",
      " fgw:0.2167307\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2167307\n",
      "Measure Epoch 362, train\n",
      " similarity:0.0597057\n",
      " penlog:-9.4431637\n",
      "Metrics Epoch 362, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:12.1400000\n",
      " batch_invalid_valency_nodes:16.6086957\n",
      " batch_nodes_0degree:3.6600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.1400000\n",
      " batch_node_degree:1.5860870\n",
      "Logits [41.39364242553711, 3.66263484954834, 64.67823791503906]\n",
      "Epoch duration: 2.1157095432281494\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_362\n",
      "Epoch: 363\n",
      "FGW torch.Size([29508, 5]) 8.531317871529609e-05\n",
      "Penalty params: tau=0.85494 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=363 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 363, train\n",
      " fgw:0.2127917\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2127917\n",
      "Measure Epoch 363, train\n",
      " similarity:0.0747022\n",
      " penlog:-13.8613718\n",
      "Metrics Epoch 363, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:86.0000000\n",
      " batch_connected_components:10.2000000\n",
      " batch_invalid_valency_nodes:12.6086957\n",
      " batch_nodes_0degree:2.6800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:94.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.0800000\n",
      " batch_node_degree:1.7443478\n",
      "Logits [41.420867919921875, 3.6680867671966553, 64.5936508178711]\n",
      "Epoch duration: 2.040613889694214\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_363\n",
      "Epoch: 364\n",
      "FGW torch.Size([29508, 5]) 7.12420151103288e-05\n",
      "Penalty params: tau=0.85457 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=364 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 364, train\n",
      " fgw:0.2160124\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2160124\n",
      "Measure Epoch 364, train\n",
      " similarity:0.0604689\n",
      " penlog:-7.4305145\n",
      "Metrics Epoch 364, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.4000000\n",
      " batch_invalid_valency_nodes:17.8260870\n",
      " batch_nodes_0degree:4.0000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.4000000\n",
      " batch_node_degree:1.5617391\n",
      "Logits [41.55183410644531, 3.682969570159912, 64.44474792480469]\n",
      "Epoch duration: 2.029348611831665\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_364\n",
      "Epoch: 365\n",
      "FGW torch.Size([29508, 5]) 8.80670195329003e-05\n",
      "Penalty params: tau=0.85420 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=365 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 365, train\n",
      " fgw:0.2185172\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2185172\n",
      "Measure Epoch 365, train\n",
      " similarity:0.0791341\n",
      " penlog:-7.8321261\n",
      "Metrics Epoch 365, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.7391304\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:10.7200000\n",
      " batch_invalid_valency_nodes:13.6521739\n",
      " batch_nodes_0degree:3.0400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:94.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.6000000\n",
      " batch_node_degree:1.6765217\n",
      "Logits [41.528316497802734, 3.651250123977661, 64.86564636230469]\n",
      "Epoch duration: 2.1527209281921387\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_365\n",
      "Epoch: 366\n",
      "FGW torch.Size([29508, 5]) 6.713216134812683e-05\n",
      "Penalty params: tau=0.85383 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=366 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 366, train\n",
      " fgw:0.2150443\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2150443\n",
      "Measure Epoch 366, train\n",
      " similarity:0.0591673\n",
      " penlog:-5.2396312\n",
      "Metrics Epoch 366, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.8260870\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:12.6800000\n",
      " batch_invalid_valency_nodes:18.7826087\n",
      " batch_nodes_0degree:4.2400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.6800000\n",
      " batch_node_degree:1.5373913\n",
      "Logits [41.70943832397461, 3.698611259460449, 64.53863525390625]\n",
      "Epoch duration: 2.0094165802001953\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_366\n",
      "Epoch: 367\n",
      "FGW torch.Size([29508, 5]) 8.696525765117258e-05\n",
      "Penalty params: tau=0.85346 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=367 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 367, train\n",
      " fgw:0.2155680\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2155680\n",
      "Measure Epoch 367, train\n",
      " similarity:0.0780033\n",
      " penlog:-11.9544295\n",
      "Metrics Epoch 367, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:88.0000000\n",
      " batch_connected_components:10.1400000\n",
      " batch_invalid_valency_nodes:12.3478261\n",
      " batch_nodes_0degree:2.5600000\n",
      " batch_nodes_7plus_degree:0.0200000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.0000000\n",
      " batch_node_degree:1.7582609\n",
      "Logits [41.72636795043945, 3.6619162559509277, 64.82176971435547]\n",
      "Epoch duration: 1.9768588542938232\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_367\n",
      "Epoch: 368\n",
      "FGW torch.Size([29508, 5]) 7.01707394910045e-05\n",
      "Penalty params: tau=0.85309 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=368 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 368, train\n",
      " fgw:0.2274562\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2274562\n",
      "Measure Epoch 368, train\n",
      " similarity:0.0592283\n",
      " penlog:-11.5746228\n",
      "Metrics Epoch 368, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:88.0000000\n",
      " batch_connected_components:11.9600000\n",
      " batch_invalid_valency_nodes:15.3913043\n",
      " batch_nodes_0degree:3.4000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.9600000\n",
      " batch_node_degree:1.6243478\n",
      "Logits [41.7918701171875, 3.6648783683776855, 64.68617248535156]\n",
      "Epoch duration: 2.4376089572906494\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_368\n",
      "Epoch: 369\n",
      "FGW torch.Size([29508, 5]) 8.578228880651295e-05\n",
      "Penalty params: tau=0.85272 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=369 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 369, train\n",
      " fgw:0.2168086\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2168086\n",
      "Measure Epoch 369, train\n",
      " similarity:0.0908645\n",
      " penlog:-13.8586557\n",
      "Metrics Epoch 369, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.6521739\n",
      " batch_molecular_disconnected_validity:86.0000000\n",
      " batch_connected_components:10.0000000\n",
      " batch_invalid_valency_nodes:11.2173913\n",
      " batch_nodes_0degree:2.3600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-9.9600000\n",
      " batch_node_degree:1.7634783\n",
      "Logits [41.602176666259766, 3.6913046836853027, 64.69689178466797]\n",
      "Epoch duration: 2.099911689758301\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 370\n",
      "FGW torch.Size([29508, 5]) 7.101716619217768e-05\n",
      "Penalty params: tau=0.85236 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=370 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 370, train\n",
      " fgw:0.2169713\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2169713\n",
      "Measure Epoch 370, train\n",
      " similarity:0.0614444\n",
      " penlog:-1.1945496\n",
      "Metrics Epoch 370, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:12.9200000\n",
      " batch_invalid_valency_nodes:20.4347826\n",
      " batch_nodes_0degree:4.6600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.9200000\n",
      " batch_node_degree:1.4573913\n",
      "Logits [41.80779266357422, 3.6623470783233643, 65.04972839355469]\n",
      "Epoch duration: 2.280289888381958\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_370\n",
      "Epoch: 371\n",
      "FGW torch.Size([29508, 5]) 8.32699442980811e-05\n",
      "Penalty params: tau=0.85199 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=371 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 371, train\n",
      " fgw:0.2109392\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2109392\n",
      "Measure Epoch 371, train\n",
      " similarity:0.0666887\n",
      " penlog:-13.7514563\n",
      "Metrics Epoch 371, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:86.0000000\n",
      " batch_connected_components:10.6000000\n",
      " batch_invalid_valency_nodes:12.6086957\n",
      " batch_nodes_0degree:2.6400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:94.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.5000000\n",
      " batch_node_degree:1.7182609\n",
      "Logits [41.89192581176758, 3.666883707046509, 64.83917236328125]\n",
      "Epoch duration: 2.06527042388916\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_371\n",
      "Epoch: 372\n",
      "FGW torch.Size([29508, 5]) 7.420121983159333e-05\n",
      "Penalty params: tau=0.85162 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=372 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 372, train\n",
      " fgw:0.2114913\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2114913\n",
      "Measure Epoch 372, train\n",
      " similarity:0.0659224\n",
      " penlog:-9.6730230\n",
      "Metrics Epoch 372, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:11.4600000\n",
      " batch_invalid_valency_nodes:14.0000000\n",
      " batch_nodes_0degree:3.0400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.4600000\n",
      " batch_node_degree:1.6747826\n",
      "Logits [41.92139434814453, 3.7047083377838135, 64.3335952758789]\n",
      "Epoch duration: 2.1125526428222656\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_372\n",
      "Epoch: 373\n",
      "FGW torch.Size([29508, 5]) 8.04971787147224e-05\n",
      "Penalty params: tau=0.85125 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=373 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 373, train\n",
      " fgw:0.2094808\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2094808\n",
      "Measure Epoch 373, train\n",
      " similarity:0.0743864\n",
      " penlog:-9.8636941\n",
      "Metrics Epoch 373, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:10.5800000\n",
      " batch_invalid_valency_nodes:12.8695652\n",
      " batch_nodes_0degree:2.7600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.4800000\n",
      " batch_node_degree:1.7095652\n",
      "Logits [41.8801155090332, 3.664388418197632, 64.84439849853516]\n",
      "Epoch duration: 2.0094289779663086\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_373\n",
      "Epoch: 374\n",
      "FGW torch.Size([29508, 5]) 7.484652451239526e-05\n",
      "Penalty params: tau=0.85088 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=374 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 374, train\n",
      " fgw:0.2148786\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2148786\n",
      "Measure Epoch 374, train\n",
      " similarity:0.0696252\n",
      " penlog:-5.5404598\n",
      "Metrics Epoch 374, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:11.9000000\n",
      " batch_invalid_valency_nodes:16.3478261\n",
      " batch_nodes_0degree:3.6200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.8800000\n",
      " batch_node_degree:1.5756522\n",
      "Logits [41.966224670410156, 3.666653871536255, 64.94815826416016]\n",
      "Epoch duration: 1.9448609352111816\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_374\n",
      "Epoch: 375\n",
      "FGW torch.Size([29508, 5]) 8.030443132156506e-05\n",
      "Penalty params: tau=0.85052 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=375 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 375, train\n",
      " fgw:0.2153870\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2153870\n",
      "Measure Epoch 375, train\n",
      " similarity:0.0753622\n",
      " penlog:-7.6116919\n",
      "Metrics Epoch 375, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.8260870\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:11.1600000\n",
      " batch_invalid_valency_nodes:13.6521739\n",
      " batch_nodes_0degree:2.9800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.1200000\n",
      " batch_node_degree:1.6817391\n",
      "Logits [41.92841720581055, 3.698340892791748, 64.57786560058594]\n",
      "Epoch duration: 2.2070088386535645\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_375\n",
      "Epoch: 376\n",
      "FGW torch.Size([29508, 5]) 7.744757022010162e-05\n",
      "Penalty params: tau=0.85015 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=376 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 376, train\n",
      " fgw:0.2135545\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2135545\n",
      "Measure Epoch 376, train\n",
      " similarity:0.0756872\n",
      " penlog:-3.4446751\n",
      "Metrics Epoch 376, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:11.6600000\n",
      " batch_invalid_valency_nodes:14.6086957\n",
      " batch_nodes_0degree:3.3000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.6000000\n",
      " batch_node_degree:1.6417391\n",
      "Logits [41.85784912109375, 3.6890881061553955, 64.5359878540039]\n",
      "Epoch duration: 2.1586079597473145\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_376\n",
      "Epoch: 377\n",
      "FGW torch.Size([29508, 5]) 7.694659143453464e-05\n",
      "Penalty params: tau=0.84978 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=377 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 377, train\n",
      " fgw:0.2051398\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2051398\n",
      "Measure Epoch 377, train\n",
      " similarity:0.0760140\n",
      " penlog:-1.5673223\n",
      "Metrics Epoch 377, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:11.6000000\n",
      " batch_invalid_valency_nodes:15.5652174\n",
      " batch_nodes_0degree:3.5400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.5800000\n",
      " batch_node_degree:1.6121739\n",
      "Logits [41.82795333862305, 3.6746928691864014, 64.79839324951172]\n",
      "Epoch duration: 1.9359636306762695\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_377\n",
      "Epoch: 378\n",
      "FGW torch.Size([29508, 5]) 7.710089266765863e-05\n",
      "Penalty params: tau=0.84942 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=378 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 378, train\n",
      " fgw:0.2101929\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2101929\n",
      "Measure Epoch 378, train\n",
      " similarity:0.0875255\n",
      " penlog:-3.6294890\n",
      "Metrics Epoch 378, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:11.3000000\n",
      " batch_invalid_valency_nodes:14.0869565\n",
      " batch_nodes_0degree:3.1800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:94.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.2600000\n",
      " batch_node_degree:1.6539130\n",
      "Logits [41.96868133544922, 3.7006616592407227, 64.73167419433594]\n",
      "Epoch duration: 2.0802528858184814\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 379\n",
      "FGW torch.Size([29508, 5]) 7.684250158490613e-05\n",
      "Penalty params: tau=0.84905 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=379 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 379, train\n",
      " fgw:0.2078557\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2078557\n",
      "Measure Epoch 379, train\n",
      " similarity:0.0719251\n",
      " penlog:-11.7619312\n",
      "Metrics Epoch 379, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:88.0000000\n",
      " batch_connected_components:11.0400000\n",
      " batch_invalid_valency_nodes:12.8695652\n",
      " batch_nodes_0degree:2.8000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.0200000\n",
      " batch_node_degree:1.6956522\n",
      "Logits [42.0651969909668, 3.7045738697052, 64.70555114746094]\n",
      "Epoch duration: 1.852461576461792\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_379\n",
      "Epoch: 380\n",
      "FGW torch.Size([29508, 5]) 7.734273094683886e-05\n",
      "Penalty params: tau=0.84868 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=380 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 380, train\n",
      " fgw:0.2065722\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2065722\n",
      "Measure Epoch 380, train\n",
      " similarity:0.0758078\n",
      " penlog:-9.6665969\n",
      "Metrics Epoch 380, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.6521739\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:11.0600000\n",
      " batch_invalid_valency_nodes:12.5217391\n",
      " batch_nodes_0degree:2.7800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.0200000\n",
      " batch_node_degree:1.6869565\n",
      "Logits [42.043540954589844, 3.6750338077545166, 64.97882080078125]\n",
      "Epoch duration: 2.023803234100342\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_380\n",
      "Epoch: 381\n",
      "FGW torch.Size([29508, 5]) 7.596120121888816e-05\n",
      "Penalty params: tau=0.84832 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=381 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 381, train\n",
      " fgw:0.2094871\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2094871\n",
      "Measure Epoch 381, train\n",
      " similarity:0.0660495\n",
      " penlog:-9.4987027\n",
      "Metrics Epoch 381, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.7391304\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:11.5400000\n",
      " batch_invalid_valency_nodes:13.9130435\n",
      " batch_nodes_0degree:3.0800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.5200000\n",
      " batch_node_degree:1.6417391\n",
      "Logits [42.164344787597656, 3.7217020988464355, 64.80940246582031]\n",
      "Epoch duration: 1.9441399574279785\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_381\n",
      "Epoch: 382\n",
      "FGW torch.Size([29508, 5]) 7.837067096261308e-05\n",
      "Penalty params: tau=0.84795 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=382 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 382, train\n",
      " fgw:0.2108747\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2108747\n",
      "Measure Epoch 382, train\n",
      " similarity:0.0739613\n",
      " penlog:-7.5462737\n",
      "Metrics Epoch 382, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:11.3800000\n",
      " batch_invalid_valency_nodes:13.7391304\n",
      " batch_nodes_0degree:3.0400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.3400000\n",
      " batch_node_degree:1.6382609\n",
      "Logits [42.22242736816406, 3.7056920528411865, 65.07506561279297]\n",
      "Epoch duration: 2.0862035751342773\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_382\n",
      "Epoch: 383\n",
      "FGW torch.Size([29508, 5]) 7.479187479475513e-05\n",
      "Penalty params: tau=0.84758 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=383 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 383, train\n",
      " fgw:0.2112327\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2112327\n",
      "Measure Epoch 383, train\n",
      " similarity:0.0706167\n",
      " penlog:-5.5328345\n",
      "Metrics Epoch 383, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:11.8800000\n",
      " batch_invalid_valency_nodes:15.3913043\n",
      " batch_nodes_0degree:3.4600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.8400000\n",
      " batch_node_degree:1.5965217\n",
      "Logits [42.383792877197266, 3.701834201812744, 65.17491912841797]\n",
      "Epoch duration: 2.0223119258880615\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_383\n",
      "Epoch: 384\n",
      "FGW torch.Size([29508, 5]) 8.060123946052045e-05\n",
      "Penalty params: tau=0.84722 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=384 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 384, train\n",
      " fgw:0.2138805\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2138805\n",
      "Measure Epoch 384, train\n",
      " similarity:0.0791406\n",
      " penlog:-9.7664720\n",
      "Metrics Epoch 384, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:10.5400000\n",
      " batch_invalid_valency_nodes:12.3478261\n",
      " batch_nodes_0degree:2.7000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.4600000\n",
      " batch_node_degree:1.7356522\n",
      "Logits [42.57188415527344, 3.7387583255767822, 65.05290985107422]\n",
      "Epoch duration: 2.037670373916626\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_384\n",
      "Epoch: 385\n",
      "FGW torch.Size([29508, 5]) 7.188716699602082e-05\n",
      "Penalty params: tau=0.84685 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=385 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 385, train\n",
      " fgw:0.2160667\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2160667\n",
      "Measure Epoch 385, train\n",
      " similarity:0.0695541\n",
      " penlog:-5.4307541\n",
      "Metrics Epoch 385, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:12.1000000\n",
      " batch_invalid_valency_nodes:16.3478261\n",
      " batch_nodes_0degree:3.6800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.0600000\n",
      " batch_node_degree:1.5860870\n",
      "Logits [42.615074157714844, 3.737222671508789, 65.22833251953125]\n",
      "Epoch duration: 2.167520046234131\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_385\n",
      "Epoch: 386\n",
      "FGW torch.Size([29508, 5]) 8.390958100790158e-05\n",
      "Penalty params: tau=0.84649 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=386 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 386, train\n",
      " fgw:0.2162875\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2162875\n",
      "Measure Epoch 386, train\n",
      " similarity:0.0845905\n",
      " penlog:-3.8228075\n",
      "Metrics Epoch 386, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:10.5600000\n",
      " batch_invalid_valency_nodes:11.7391304\n",
      " batch_nodes_0degree:2.6400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.4400000\n",
      " batch_node_degree:1.6991304\n",
      "Logits [42.5124397277832, 3.70805025100708, 65.50776672363281]\n",
      "Epoch duration: 2.1720845699310303\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_386\n",
      "Epoch: 387\n",
      "FGW torch.Size([29508, 5]) 6.742063851561397e-05\n",
      "Penalty params: tau=0.84612 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=387 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 387, train\n",
      " fgw:0.2176509\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2176509\n",
      "Measure Epoch 387, train\n",
      " similarity:0.0604812\n",
      " penlog:-9.4499080\n",
      "Metrics Epoch 387, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:12.4200000\n",
      " batch_invalid_valency_nodes:17.1304348\n",
      " batch_nodes_0degree:3.8200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.4000000\n",
      " batch_node_degree:1.5843478\n",
      "Logits [42.777435302734375, 3.743082046508789, 65.12516021728516]\n",
      "Epoch duration: 1.951005458831787\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 388\n",
      "FGW torch.Size([29508, 5]) 9.226761903846636e-05\n",
      "Penalty params: tau=0.84576 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=388 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 388, train\n",
      " fgw:0.2187520\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2187520\n",
      "Measure Epoch 388, train\n",
      " similarity:0.0651282\n",
      " penlog:-36.1252295\n",
      "Metrics Epoch 388, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:64.0000000\n",
      " batch_connected_components:8.1800000\n",
      " batch_invalid_valency_nodes:10.3478261\n",
      " batch_nodes_0degree:1.7200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:94.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-7.7600000\n",
      " batch_node_degree:1.9530435\n",
      "Logits [42.59724426269531, 3.7229819297790527, 65.36520385742188]\n",
      "Epoch duration: 2.0772321224212646\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_388\n",
      "Epoch: 389\n",
      "FGW torch.Size([29508, 5]) 6.112088885856792e-05\n",
      "Penalty params: tau=0.84539 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=389 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 389, train\n",
      " fgw:0.2311600\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2311600\n",
      "Measure Epoch 389, train\n",
      " similarity:0.0511780\n",
      " penlog:-3.1553452\n",
      "Metrics Epoch 389, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:14.0800000\n",
      " batch_invalid_valency_nodes:26.7826087\n",
      " batch_nodes_0degree:6.1200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-14.0800000\n",
      " batch_node_degree:1.3826087\n",
      "Logits [42.789913177490234, 3.72566294670105, 65.34329223632812]\n",
      "Epoch duration: 2.0045597553253174\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_389\n",
      "Epoch: 390\n",
      "FGW torch.Size([29508, 5]) 8.290835103252903e-05\n",
      "Penalty params: tau=0.84503 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=390 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 390, train\n",
      " fgw:0.2131348\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2131348\n",
      "Measure Epoch 390, train\n",
      " similarity:0.0824017\n",
      " penlog:-11.8427721\n",
      "Metrics Epoch 390, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:88.0000000\n",
      " batch_connected_components:10.5800000\n",
      " batch_invalid_valency_nodes:13.3043478\n",
      " batch_nodes_0degree:2.8400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.4800000\n",
      " batch_node_degree:1.7008696\n",
      "Logits [42.47258758544922, 3.7084455490112305, 65.37726593017578]\n",
      "Epoch duration: 2.0509753227233887\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_390\n",
      "Epoch: 391\n",
      "FGW torch.Size([29508, 5]) 9.162681089947e-05\n",
      "Penalty params: tau=0.84466 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=391 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 391, train\n",
      " fgw:0.2159287\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2159287\n",
      "Measure Epoch 391, train\n",
      " similarity:0.0922803\n",
      " penlog:-22.1649694\n",
      "Metrics Epoch 391, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:78.0000000\n",
      " batch_connected_components:8.8600000\n",
      " batch_invalid_valency_nodes:9.3913043\n",
      " batch_nodes_0degree:1.7200000\n",
      " batch_nodes_7plus_degree:0.0200000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-8.6600000\n",
      " batch_node_degree:1.9060870\n",
      "Logits [42.40891647338867, 3.72410249710083, 65.0435562133789]\n",
      "Epoch duration: 1.8754081726074219\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_391\n",
      "Epoch: 392\n",
      "FGW torch.Size([29508, 5]) 5.958863766863942e-05\n",
      "Penalty params: tau=0.84430 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=392 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 392, train\n",
      " fgw:0.2567110\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2567110\n",
      "Measure Epoch 392, train\n",
      " similarity:0.0482486\n",
      " penlog:-5.1731615\n",
      "Metrics Epoch 392, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:14.3800000\n",
      " batch_invalid_valency_nodes:29.0434783\n",
      " batch_nodes_0degree:6.6200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-14.3800000\n",
      " batch_node_degree:1.3791304\n",
      "Logits [42.53065490722656, 3.720675230026245, 64.94486999511719]\n",
      "Epoch duration: 1.932999610900879\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_392\n",
      "Epoch: 393\n",
      "FGW torch.Size([29508, 5]) 7.569184526801109e-05\n",
      "Penalty params: tau=0.84393 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=393 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 393, train\n",
      " fgw:0.2106091\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2106091\n",
      "Measure Epoch 393, train\n",
      " similarity:0.0761016\n",
      " penlog:-5.7047501\n",
      "Metrics Epoch 393, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:10.9800000\n",
      " batch_invalid_valency_nodes:14.0000000\n",
      " batch_nodes_0degree:3.1200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.9400000\n",
      " batch_node_degree:1.6417391\n",
      "Logits [42.144676208496094, 3.6626057624816895, 65.24879455566406]\n",
      "Epoch duration: 2.1706299781799316\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_393\n",
      "Epoch: 394\n",
      "FGW torch.Size([29508, 5]) 0.0001350938982795924\n",
      "Penalty params: tau=0.84357 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=394 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 394, train\n",
      " fgw:0.2861149\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2861149\n",
      "Measure Epoch 394, train\n",
      " similarity:0.0377076\n",
      " penlog:-84.1873940\n",
      "Metrics Epoch 394, train\n",
      " batch_molecular_validity:4.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.7391304\n",
      " batch_molecular_disconnected_validity:16.0000000\n",
      " batch_connected_components:3.3800000\n",
      " batch_invalid_valency_nodes:23.4782609\n",
      " batch_nodes_0degree:0.6200000\n",
      " batch_nodes_7plus_degree:1.5200000\n",
      " invalid_euler_toofew:40.0000000\n",
      " invalid_euler_toomany:28.0000000\n",
      " avg_euler_error:5.8600000\n",
      " batch_node_degree:3.1286957\n",
      "Logits [41.8472785949707, 3.6863789558410645, 64.8459701538086]\n",
      "Epoch duration: 1.9020400047302246\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_394\n",
      "Epoch: 395\n",
      "FGW torch.Size([29508, 5]) 5.749312913394533e-05\n",
      "Penalty params: tau=0.84320 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=395 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 395, train\n",
      " fgw:0.5095163\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.5095163\n",
      "Measure Epoch 395, train\n",
      " similarity:0.0191905\n",
      " penlog:1.1114175\n",
      "Metrics Epoch 395, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.7391304\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:17.1200000\n",
      " batch_invalid_valency_nodes:49.2173913\n",
      " batch_nodes_0degree:11.3200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-17.1200000\n",
      " batch_node_degree:1.0208696\n",
      "Logits [41.57311248779297, 3.6736621856689453, 64.01860046386719]\n",
      "Epoch duration: 2.035008668899536\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_395\n",
      "Epoch: 396\n",
      "FGW torch.Size([29508, 5]) 5.7979123084805906e-05\n",
      "Penalty params: tau=0.84284 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=396 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 396, train\n",
      " fgw:0.6008097\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.6008097\n",
      "Measure Epoch 396, train\n",
      " similarity:0.0211507\n",
      " penlog:0.9888513\n",
      "Metrics Epoch 396, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.6086957\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:17.3200000\n",
      " batch_invalid_valency_nodes:50.6956522\n",
      " batch_nodes_0degree:11.6600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-17.3200000\n",
      " batch_node_degree:0.9878261\n",
      "Logits [40.27692413330078, 3.458340883255005, 64.04310607910156]\n",
      "Epoch duration: 2.0833640098571777\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 397\n",
      "FGW torch.Size([29508, 5]) 6.068460061214864e-05\n",
      "Penalty params: tau=0.84248 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=397 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 397, train\n",
      " fgw:0.4332749\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.4332749\n",
      "Measure Epoch 397, train\n",
      " similarity:0.0164430\n",
      " penlog:-39.6484153\n",
      "Metrics Epoch 397, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.0000000\n",
      " batch_molecular_disconnected_validity:60.0000000\n",
      " batch_connected_components:14.2400000\n",
      " batch_invalid_valency_nodes:33.5652174\n",
      " batch_nodes_0degree:7.2400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-14.2400000\n",
      " batch_node_degree:1.5147826\n",
      "Logits [37.192413330078125, 3.2911860942840576, 61.976806640625]\n",
      "Epoch duration: 2.049708127975464\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_397\n",
      "Epoch: 398\n",
      "FGW torch.Size([29508, 5]) 0.0001319572184002027\n",
      "Penalty params: tau=0.84211 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=398 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 398, train\n",
      " fgw:0.3612788\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3612788\n",
      "Measure Epoch 398, train\n",
      " similarity:0.0219512\n",
      " penlog:-96.1520798\n",
      "Metrics Epoch 398, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.9130435\n",
      " batch_molecular_disconnected_validity:4.0000000\n",
      " batch_connected_components:2.2200000\n",
      " batch_invalid_valency_nodes:40.2608696\n",
      " batch_nodes_0degree:0.7200000\n",
      " batch_nodes_7plus_degree:3.3200000\n",
      " invalid_euler_toofew:6.0000000\n",
      " invalid_euler_toomany:52.0000000\n",
      " avg_euler_error:8.5200000\n",
      " batch_node_degree:3.8330435\n",
      "Logits [35.73460388183594, 3.008918285369873, 60.78725051879883]\n",
      "Epoch duration: 2.0013909339904785\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_398\n",
      "Epoch: 399\n",
      "FGW torch.Size([29508, 5]) 0.00013845287321601063\n",
      "Penalty params: tau=0.84175 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=399 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 399, train\n",
      " fgw:0.3600439\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3600439\n",
      "Measure Epoch 399, train\n",
      " similarity:0.0233503\n",
      " penlog:-92.2649913\n",
      "Metrics Epoch 399, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.7391304\n",
      " batch_molecular_disconnected_validity:8.0000000\n",
      " batch_connected_components:4.5800000\n",
      " batch_invalid_valency_nodes:31.1304348\n",
      " batch_nodes_0degree:2.7800000\n",
      " batch_nodes_7plus_degree:1.0000000\n",
      " invalid_euler_toofew:16.0000000\n",
      " invalid_euler_toomany:34.0000000\n",
      " avg_euler_error:3.9600000\n",
      " batch_node_degree:2.7060870\n",
      "Logits [32.59547805786133, 2.732363700866699, 60.49764633178711]\n",
      "Epoch duration: 1.9847517013549805\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_399\n",
      "Epoch: 400\n",
      "FGW torch.Size([29508, 5]) 6.753858178853989e-05\n",
      "Penalty params: tau=0.84139 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=400 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 400, train\n",
      " fgw:0.3432487\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3432487\n",
      "Measure Epoch 400, train\n",
      " similarity:0.0475398\n",
      " penlog:-2.4990077\n",
      "Metrics Epoch 400, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:96.0869565\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:16.4800000\n",
      " batch_invalid_valency_nodes:45.9130435\n",
      " batch_nodes_0degree:10.5200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-16.4800000\n",
      " batch_node_degree:0.9704348\n",
      "Logits [32.52355194091797, 2.8240129947662354, 57.62651443481445]\n",
      "Epoch duration: 1.971756935119629\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_400\n",
      "Epoch: 401\n",
      "FGW torch.Size([29508, 5]) 6.291150202741846e-05\n",
      "Penalty params: tau=0.84102 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=401 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 401, train\n",
      " fgw:0.3453450\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3453450\n",
      "Measure Epoch 401, train\n",
      " similarity:0.0211606\n",
      " penlog:0.9950550\n",
      "Metrics Epoch 401, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.6956522\n",
      " batch_molecular_disconnected_validity:100.0000000\n",
      " batch_connected_components:17.5800000\n",
      " batch_invalid_valency_nodes:53.5652174\n",
      " batch_nodes_0degree:12.3200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-17.5800000\n",
      " batch_node_degree:0.8834783\n",
      "Logits [31.80078125, 2.6492116451263428, 56.78384780883789]\n",
      "Epoch duration: 2.132797956466675\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_401\n",
      "Epoch: 402\n",
      "FGW torch.Size([29508, 5]) 6.975856376811862e-05\n",
      "Penalty params: tau=0.84066 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=402 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 402, train\n",
      " fgw:0.3052930\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3052930\n",
      "Measure Epoch 402, train\n",
      " similarity:0.0374173\n",
      " penlog:-15.7772735\n",
      "Metrics Epoch 402, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.2608696\n",
      " batch_molecular_disconnected_validity:84.0000000\n",
      " batch_connected_components:14.8400000\n",
      " batch_invalid_valency_nodes:36.0000000\n",
      " batch_nodes_0degree:8.0600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-14.8400000\n",
      " batch_node_degree:1.3217391\n",
      "Logits [30.372615814208984, 2.6126742362976074, 54.8556022644043]\n",
      "Epoch duration: 1.9782772064208984\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_402\n",
      "Epoch: 403\n",
      "FGW torch.Size([29508, 5]) 0.00010886005475185812\n",
      "Penalty params: tau=0.84030 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=403 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 403, train\n",
      " fgw:0.3258561\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.3258561\n",
      "Measure Epoch 403, train\n",
      " similarity:0.0248525\n",
      " penlog:-79.9413663\n",
      "Metrics Epoch 403, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:97.5652174\n",
      " batch_molecular_disconnected_validity:20.0000000\n",
      " batch_connected_components:7.6800000\n",
      " batch_invalid_valency_nodes:29.5652174\n",
      " batch_nodes_0degree:3.4000000\n",
      " batch_nodes_7plus_degree:1.5600000\n",
      " invalid_euler_toofew:70.0000000\n",
      " invalid_euler_toomany:12.0000000\n",
      " avg_euler_error:-2.5000000\n",
      " batch_node_degree:2.5791304\n",
      "Logits [29.18811798095703, 2.5783021450042725, 53.65536117553711]\n",
      "Epoch duration: 1.9826819896697998\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_403\n",
      "Epoch: 404\n",
      "FGW torch.Size([29508, 5]) 0.00011073650239268318\n",
      "Penalty params: tau=0.83993 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=404 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 404, train\n",
      " fgw:0.2981976\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2981976\n",
      "Measure Epoch 404, train\n",
      " similarity:0.0462953\n",
      " penlog:-41.9766434\n",
      "Metrics Epoch 404, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.0869565\n",
      " batch_molecular_disconnected_validity:58.0000000\n",
      " batch_connected_components:9.3400000\n",
      " batch_invalid_valency_nodes:24.3478261\n",
      " batch_nodes_0degree:3.8200000\n",
      " batch_nodes_7plus_degree:0.5400000\n",
      " invalid_euler_toofew:90.0000000\n",
      " invalid_euler_toomany:8.0000000\n",
      " avg_euler_error:-7.1000000\n",
      " batch_node_degree:1.8939130\n",
      "Logits [27.028621673583984, 2.28240704536438, 54.30055618286133]\n",
      "Epoch duration: 2.1068403720855713\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_404\n",
      "Epoch: 405\n",
      "FGW torch.Size([29508, 5]) 8.761670324020088e-05\n",
      "Penalty params: tau=0.83957 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=405 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 405, train\n",
      " fgw:0.2691456\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2691456\n",
      "Measure Epoch 405, train\n",
      " similarity:0.0360786\n",
      " penlog:-3.2526974\n",
      "Metrics Epoch 405, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.7826087\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:14.9200000\n",
      " batch_invalid_valency_nodes:35.4782609\n",
      " batch_nodes_0degree:8.1200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-14.8600000\n",
      " batch_node_degree:1.1200000\n",
      "Logits [26.156015396118164, 2.1699461936950684, 53.93686294555664]\n",
      "Epoch duration: 1.8647241592407227\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 406\n",
      "FGW torch.Size([29508, 5]) 8.281711780000478e-05\n",
      "Penalty params: tau=0.83921 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=406 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 406, train\n",
      " fgw:0.2901509\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2901509\n",
      "Measure Epoch 406, train\n",
      " similarity:0.0375275\n",
      " penlog:-17.5682441\n",
      "Metrics Epoch 406, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.0000000\n",
      " batch_molecular_disconnected_validity:82.0000000\n",
      " batch_connected_components:14.0600000\n",
      " batch_invalid_valency_nodes:32.6086957\n",
      " batch_nodes_0degree:7.2800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.9400000\n",
      " batch_node_degree:1.3078261\n",
      "Logits [26.391841888427734, 2.2332589626312256, 52.0063591003418]\n",
      "Epoch duration: 2.0081474781036377\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_406\n",
      "Epoch: 407\n",
      "FGW torch.Size([29508, 5]) 8.265623182523996e-05\n",
      "Penalty params: tau=0.83885 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=407 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 407, train\n",
      " fgw:0.2693237\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2693237\n",
      "Measure Epoch 407, train\n",
      " similarity:0.0461261\n",
      " penlog:-27.7121588\n",
      "Metrics Epoch 407, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.4347826\n",
      " batch_molecular_disconnected_validity:72.0000000\n",
      " batch_connected_components:13.4200000\n",
      " batch_invalid_valency_nodes:29.9130435\n",
      " batch_nodes_0degree:6.4200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.2200000\n",
      " batch_node_degree:1.4069565\n",
      "Logits [25.27406120300293, 2.12626051902771, 51.91694641113281]\n",
      "Epoch duration: 2.137810468673706\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_407\n",
      "Epoch: 408\n",
      "FGW torch.Size([29508, 5]) 8.657066791784018e-05\n",
      "Penalty params: tau=0.83848 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=408 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 408, train\n",
      " fgw:0.2768605\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2768605\n",
      "Measure Epoch 408, train\n",
      " similarity:0.0432953\n",
      " penlog:-25.7946222\n",
      "Metrics Epoch 408, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.3478261\n",
      " batch_molecular_disconnected_validity:74.0000000\n",
      " batch_connected_components:13.4600000\n",
      " batch_invalid_valency_nodes:30.0869565\n",
      " batch_nodes_0degree:6.4400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.3000000\n",
      " batch_node_degree:1.3860870\n",
      "Logits [24.52631187438965, 2.0108413696289062, 52.45519256591797]\n",
      "Epoch duration: 1.988800287246704\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_408\n",
      "Epoch: 409\n",
      "FGW torch.Size([29508, 5]) 9.386627789353952e-05\n",
      "Penalty params: tau=0.83812 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=409 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 409, train\n",
      " fgw:0.2642914\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2642914\n",
      "Measure Epoch 409, train\n",
      " similarity:0.0354539\n",
      " penlog:-33.5917167\n",
      "Metrics Epoch 409, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.7826087\n",
      " batch_molecular_disconnected_validity:66.0000000\n",
      " batch_connected_components:12.1800000\n",
      " batch_invalid_valency_nodes:24.0869565\n",
      " batch_nodes_0degree:4.9400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.0000000\n",
      " batch_node_degree:1.5600000\n",
      "Logits [23.878446578979492, 2.0142202377319336, 51.68673324584961]\n",
      "Epoch duration: 2.1011552810668945\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_409\n",
      "Epoch: 410\n",
      "FGW torch.Size([29508, 5]) 9.764472633833066e-05\n",
      "Penalty params: tau=0.83776 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=410 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 410, train\n",
      " fgw:0.2667335\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2667335\n",
      "Measure Epoch 410, train\n",
      " similarity:0.0330398\n",
      " penlog:-33.5222194\n",
      "Metrics Epoch 410, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.6086957\n",
      " batch_molecular_disconnected_validity:66.0000000\n",
      " batch_connected_components:12.0400000\n",
      " batch_invalid_valency_nodes:22.6956522\n",
      " batch_nodes_0degree:4.7400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.9800000\n",
      " batch_node_degree:1.5756522\n",
      "Logits [22.945547103881836, 2.0087757110595703, 50.96767044067383]\n",
      "Epoch duration: 2.0180366039276123\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_410\n",
      "Epoch: 411\n",
      "FGW torch.Size([29508, 5]) 9.378142567584291e-05\n",
      "Penalty params: tau=0.83740 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=411 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 411, train\n",
      " fgw:0.2537107\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2537107\n",
      "Measure Epoch 411, train\n",
      " similarity:0.0421478\n",
      " penlog:-13.6135755\n",
      "Metrics Epoch 411, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.3043478\n",
      " batch_molecular_disconnected_validity:86.0000000\n",
      " batch_connected_components:13.0400000\n",
      " batch_invalid_valency_nodes:25.3043478\n",
      " batch_nodes_0degree:5.6200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.0000000\n",
      " batch_node_degree:1.4539130\n",
      "Logits [22.01421546936035, 1.9425082206726074, 50.99311447143555]\n",
      "Epoch duration: 1.974459171295166\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_411\n",
      "Epoch: 412\n",
      "FGW torch.Size([29508, 5]) 8.981485734693706e-05\n",
      "Penalty params: tau=0.83704 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=412 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 412, train\n",
      " fgw:0.2600682\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2600682\n",
      "Measure Epoch 412, train\n",
      " similarity:0.0392079\n",
      " penlog:-3.4628908\n",
      "Metrics Epoch 412, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:98.6086957\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:13.9200000\n",
      " batch_invalid_valency_nodes:28.1739130\n",
      " batch_nodes_0degree:6.4200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.9000000\n",
      " batch_node_degree:1.3286957\n",
      "Logits [22.031137466430664, 1.9192782640457153, 51.070518493652344]\n",
      "Epoch duration: 2.0675363540649414\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_412\n",
      "Epoch: 413\n",
      "FGW torch.Size([29508, 5]) 8.775221795076504e-05\n",
      "Penalty params: tau=0.83668 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=413 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 413, train\n",
      " fgw:0.2459476\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2459476\n",
      "Measure Epoch 413, train\n",
      " similarity:0.0404795\n",
      " penlog:-1.2878577\n",
      "Metrics Epoch 413, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.3043478\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:13.9600000\n",
      " batch_invalid_valency_nodes:28.3478261\n",
      " batch_nodes_0degree:6.5000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.9400000\n",
      " batch_node_degree:1.3356522\n",
      "Logits [22.721229553222656, 1.990188479423523, 50.57861328125]\n",
      "Epoch duration: 1.9363572597503662\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_413\n",
      "Epoch: 414\n",
      "FGW torch.Size([29508, 5]) 8.748823893256485e-05\n",
      "Penalty params: tau=0.83632 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=414 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 414, train\n",
      " fgw:0.2495568\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2495568\n",
      "Measure Epoch 414, train\n",
      " similarity:0.0405102\n",
      " penlog:-5.3030263\n",
      "Metrics Epoch 414, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.3043478\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:13.7800000\n",
      " batch_invalid_valency_nodes:27.8260870\n",
      " batch_nodes_0degree:6.3400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.7600000\n",
      " batch_node_degree:1.3495652\n",
      "Logits [23.26645851135254, 2.019535541534424, 50.55629348754883]\n",
      "Epoch duration: 2.1816582679748535\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 415\n",
      "FGW torch.Size([29508, 5]) 8.728793181944638e-05\n",
      "Penalty params: tau=0.83595 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=415 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 415, train\n",
      " fgw:0.2458480\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2458480\n",
      "Measure Epoch 415, train\n",
      " similarity:0.0391664\n",
      " penlog:-7.3215418\n",
      "Metrics Epoch 415, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.3913043\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:13.5200000\n",
      " batch_invalid_valency_nodes:25.8260870\n",
      " batch_nodes_0degree:5.8600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.5000000\n",
      " batch_node_degree:1.3582609\n",
      "Logits [23.459569931030273, 1.972099781036377, 51.28913879394531]\n",
      "Epoch duration: 2.340360403060913\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_415\n",
      "Epoch: 416\n",
      "FGW torch.Size([29508, 5]) 8.813221211312339e-05\n",
      "Penalty params: tau=0.83559 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=416 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 416, train\n",
      " fgw:0.2451858\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2451858\n",
      "Measure Epoch 416, train\n",
      " similarity:0.0447842\n",
      " penlog:-7.3685847\n",
      "Metrics Epoch 416, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.3913043\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:13.0400000\n",
      " batch_invalid_valency_nodes:23.8260870\n",
      " batch_nodes_0degree:5.3800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-13.0200000\n",
      " batch_node_degree:1.3965217\n",
      "Logits [23.72231101989746, 1.9647414684295654, 51.8289794921875]\n",
      "Epoch duration: 2.0020248889923096\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_416\n",
      "Epoch: 417\n",
      "FGW torch.Size([29508, 5]) 8.891557081369683e-05\n",
      "Penalty params: tau=0.83523 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=417 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 417, train\n",
      " fgw:0.2399206\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2399206\n",
      "Measure Epoch 417, train\n",
      " similarity:0.0410099\n",
      " penlog:-9.4569170\n",
      "Metrics Epoch 417, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.4782609\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:12.4200000\n",
      " batch_invalid_valency_nodes:19.7391304\n",
      " batch_nodes_0degree:4.3800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.4000000\n",
      " batch_node_degree:1.5234783\n",
      "Logits [24.049434661865234, 2.0511114597320557, 51.41044235229492]\n",
      "Epoch duration: 2.0453073978424072\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_417\n",
      "Epoch: 418\n",
      "FGW torch.Size([29508, 5]) 8.688482193974778e-05\n",
      "Penalty params: tau=0.83487 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=418 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 418, train\n",
      " fgw:0.2416892\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2416892\n",
      "Measure Epoch 418, train\n",
      " similarity:0.0438206\n",
      " penlog:-7.3975850\n",
      "Metrics Epoch 418, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.5652174\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.2400000\n",
      " batch_invalid_valency_nodes:18.6086957\n",
      " batch_nodes_0degree:4.1200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.2200000\n",
      " batch_node_degree:1.5721739\n",
      "Logits [24.13495445251465, 2.0913712978363037, 51.343692779541016]\n",
      "Epoch duration: 2.023939371109009\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_418\n",
      "Epoch: 419\n",
      "FGW torch.Size([29508, 5]) 8.560438436688855e-05\n",
      "Penalty params: tau=0.83451 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=419 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 419, train\n",
      " fgw:0.2414246\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2414246\n",
      "Measure Epoch 419, train\n",
      " similarity:0.0426630\n",
      " penlog:-13.6194263\n",
      "Metrics Epoch 419, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.7391304\n",
      " batch_molecular_disconnected_validity:86.0000000\n",
      " batch_connected_components:12.2200000\n",
      " batch_invalid_valency_nodes:18.5217391\n",
      " batch_nodes_0degree:4.0400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.1800000\n",
      " batch_node_degree:1.5860870\n",
      "Logits [24.00168228149414, 2.066838026046753, 51.6913948059082]\n",
      "Epoch duration: 2.2631123065948486\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_419\n",
      "Epoch: 420\n",
      "FGW torch.Size([29508, 5]) 8.366798283532262e-05\n",
      "Penalty params: tau=0.83415 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=420 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 420, train\n",
      " fgw:0.2351582\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2351582\n",
      "Measure Epoch 420, train\n",
      " similarity:0.0422160\n",
      " penlog:-9.4243950\n",
      "Metrics Epoch 420, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.5652174\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:12.4600000\n",
      " batch_invalid_valency_nodes:19.0434783\n",
      " batch_nodes_0degree:4.1800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.4200000\n",
      " batch_node_degree:1.5617391\n",
      "Logits [23.951873779296875, 2.0463225841522217, 52.02790832519531]\n",
      "Epoch duration: 2.0613183975219727\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_420\n",
      "Epoch: 421\n",
      "FGW torch.Size([29508, 5]) 8.445594721706584e-05\n",
      "Penalty params: tau=0.83379 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=421 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 421, train\n",
      " fgw:0.2311741\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2311741\n",
      "Measure Epoch 421, train\n",
      " similarity:0.0387587\n",
      " penlog:-11.3752792\n",
      "Metrics Epoch 421, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:88.0000000\n",
      " batch_connected_components:12.2200000\n",
      " batch_invalid_valency_nodes:17.8260870\n",
      " batch_nodes_0degree:3.8800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.1800000\n",
      " batch_node_degree:1.5808696\n",
      "Logits [24.286579132080078, 2.080552577972412, 51.997230529785156]\n",
      "Epoch duration: 2.0677177906036377\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_421\n",
      "Epoch: 422\n",
      "FGW torch.Size([29508, 5]) 8.600323781138286e-05\n",
      "Penalty params: tau=0.83343 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=422 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 422, train\n",
      " fgw:0.2336110\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2336110\n",
      "Measure Epoch 422, train\n",
      " similarity:0.0360860\n",
      " penlog:-17.3619219\n",
      "Metrics Epoch 422, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.7391304\n",
      " batch_molecular_disconnected_validity:82.0000000\n",
      " batch_connected_components:12.1200000\n",
      " batch_invalid_valency_nodes:17.3043478\n",
      " batch_nodes_0degree:3.7400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.1000000\n",
      " batch_node_degree:1.5878261\n",
      "Logits [24.775161743164062, 2.125377655029297, 51.94602584838867]\n",
      "Epoch duration: 2.0516297817230225\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_422\n",
      "Epoch: 423\n",
      "FGW torch.Size([29508, 5]) 8.626362250652164e-05\n",
      "Penalty params: tau=0.83307 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=423 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 423, train\n",
      " fgw:0.2312866\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2312866\n",
      "Measure Epoch 423, train\n",
      " similarity:0.0407461\n",
      " penlog:-9.3419199\n",
      "Metrics Epoch 423, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:12.1400000\n",
      " batch_invalid_valency_nodes:17.0434783\n",
      " batch_nodes_0degree:3.7800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.1200000\n",
      " batch_node_degree:1.5721739\n",
      "Logits [25.158857345581055, 2.1375346183776855, 52.1450080871582]\n",
      "Epoch duration: 2.0948057174682617\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 424\n",
      "FGW torch.Size([29508, 5]) 8.521096606273204e-05\n",
      "Penalty params: tau=0.83271 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=424 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 424, train\n",
      " fgw:0.2304943\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2304943\n",
      "Measure Epoch 424, train\n",
      " similarity:0.0461473\n",
      " penlog:-7.4272200\n",
      "Metrics Epoch 424, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.8260870\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.2600000\n",
      " batch_invalid_valency_nodes:17.8260870\n",
      " batch_nodes_0degree:3.9800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.2600000\n",
      " batch_node_degree:1.5356522\n",
      "Logits [25.312856674194336, 2.139897584915161, 52.516021728515625]\n",
      "Epoch duration: 2.059217929840088\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_424\n",
      "Epoch: 425\n",
      "FGW torch.Size([29508, 5]) 8.420868834946305e-05\n",
      "Penalty params: tau=0.83235 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=425 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 425, train\n",
      " fgw:0.2293772\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2293772\n",
      "Measure Epoch 425, train\n",
      " similarity:0.0505572\n",
      " penlog:-5.3864594\n",
      "Metrics Epoch 425, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.8260870\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:12.3200000\n",
      " batch_invalid_valency_nodes:17.9130435\n",
      " batch_nodes_0degree:4.0600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.3000000\n",
      " batch_node_degree:1.5252174\n",
      "Logits [25.346464157104492, 2.166452646255493, 52.81101608276367]\n",
      "Epoch duration: 2.067150354385376\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_425\n",
      "Epoch: 426\n",
      "FGW torch.Size([29508, 5]) 8.274643187178299e-05\n",
      "Penalty params: tau=0.83199 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=426 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 426, train\n",
      " fgw:0.2323288\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2323288\n",
      "Measure Epoch 426, train\n",
      " similarity:0.0516523\n",
      " penlog:-5.3591616\n",
      "Metrics Epoch 426, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:12.2600000\n",
      " batch_invalid_valency_nodes:17.9130435\n",
      " batch_nodes_0degree:4.0400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.2400000\n",
      " batch_node_degree:1.5443478\n",
      "Logits [25.469715118408203, 2.2060694694519043, 52.84156036376953]\n",
      "Epoch duration: 2.2977194786071777\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_426\n",
      "Epoch: 427\n",
      "FGW torch.Size([29508, 5]) 8.22913134470582e-05\n",
      "Penalty params: tau=0.83163 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=427 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 427, train\n",
      " fgw:0.2292253\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2292253\n",
      "Measure Epoch 427, train\n",
      " similarity:0.0475455\n",
      " penlog:-7.3750485\n",
      "Metrics Epoch 427, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:12.1600000\n",
      " batch_invalid_valency_nodes:17.1304348\n",
      " batch_nodes_0degree:3.8200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-12.1600000\n",
      " batch_node_degree:1.5773913\n",
      "Logits [25.713369369506836, 2.2263264656066895, 52.789878845214844]\n",
      "Epoch duration: 2.082150459289551\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_427\n",
      "Epoch: 428\n",
      "FGW torch.Size([29508, 5]) 8.200749289244413e-05\n",
      "Penalty params: tau=0.83128 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=428 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 428, train\n",
      " fgw:0.2264309\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2264309\n",
      "Measure Epoch 428, train\n",
      " similarity:0.0448327\n",
      " penlog:-9.5238086\n",
      "Metrics Epoch 428, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:11.9000000\n",
      " batch_invalid_valency_nodes:16.0869565\n",
      " batch_nodes_0degree:3.5400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.8800000\n",
      " batch_node_degree:1.6208696\n",
      "Logits [26.01152229309082, 2.2307114601135254, 52.904998779296875]\n",
      "Epoch duration: 2.042090654373169\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_428\n",
      "Epoch: 429\n",
      "FGW torch.Size([29508, 5]) 8.402280218433589e-05\n",
      "Penalty params: tau=0.83092 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=429 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 429, train\n",
      " fgw:0.2251496\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2251496\n",
      "Measure Epoch 429, train\n",
      " similarity:0.0467447\n",
      " penlog:-9.5762137\n",
      "Metrics Epoch 429, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.8260870\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:11.6400000\n",
      " batch_invalid_valency_nodes:15.3043478\n",
      " batch_nodes_0degree:3.3800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.6200000\n",
      " batch_node_degree:1.6313043\n",
      "Logits [26.280494689941406, 2.229830265045166, 53.197933197021484]\n",
      "Epoch duration: 2.240938425064087\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_429\n",
      "Epoch: 430\n",
      "FGW torch.Size([29508, 5]) 8.432466711383313e-05\n",
      "Penalty params: tau=0.83056 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=430 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 430, train\n",
      " fgw:0.2255637\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2255637\n",
      "Measure Epoch 430, train\n",
      " similarity:0.0488488\n",
      " penlog:-9.6107596\n",
      "Metrics Epoch 430, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:11.4800000\n",
      " batch_invalid_valency_nodes:14.6956522\n",
      " batch_nodes_0degree:3.2400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.4600000\n",
      " batch_node_degree:1.6347826\n",
      "Logits [26.489418029785156, 2.243772029876709, 53.45951461791992]\n",
      "Epoch duration: 2.374105453491211\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_430\n",
      "Epoch: 431\n",
      "FGW torch.Size([29508, 5]) 8.36697654449381e-05\n",
      "Penalty params: tau=0.83020 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=431 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 431, train\n",
      " fgw:0.2248087\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2248087\n",
      "Measure Epoch 431, train\n",
      " similarity:0.0489551\n",
      " penlog:-9.5748682\n",
      "Metrics Epoch 431, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.8260870\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:11.4600000\n",
      " batch_invalid_valency_nodes:14.8695652\n",
      " batch_nodes_0degree:3.2800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.4400000\n",
      " batch_node_degree:1.6260870\n",
      "Logits [26.683019638061523, 2.2735159397125244, 53.55112838745117]\n",
      "Epoch duration: 2.056708335876465\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_431\n",
      "Epoch: 432\n",
      "FGW torch.Size([29508, 5]) 8.115251694107428e-05\n",
      "Penalty params: tau=0.82984 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=432 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 432, train\n",
      " fgw:0.2223589\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2223589\n",
      "Measure Epoch 432, train\n",
      " similarity:0.0501140\n",
      " penlog:-7.4733750\n",
      "Metrics Epoch 432, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:11.7800000\n",
      " batch_invalid_valency_nodes:15.7391304\n",
      " batch_nodes_0degree:3.5000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.7400000\n",
      " batch_node_degree:1.5913043\n",
      "Logits [26.867435455322266, 2.291119337081909, 53.74187088012695]\n",
      "Epoch duration: 2.165809154510498\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 433\n",
      "FGW torch.Size([29508, 5]) 8.029601303860545e-05\n",
      "Penalty params: tau=0.82948 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=433 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 433, train\n",
      " fgw:0.2238311\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2238311\n",
      "Measure Epoch 433, train\n",
      " similarity:0.0522332\n",
      " penlog:-5.4004376\n",
      "Metrics Epoch 433, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:11.9400000\n",
      " batch_invalid_valency_nodes:16.2608696\n",
      " batch_nodes_0degree:3.6400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.9000000\n",
      " batch_node_degree:1.5773913\n",
      "Logits [27.08637046813965, 2.2958879470825195, 54.06135177612305]\n",
      "Epoch duration: 2.2445456981658936\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_433\n",
      "Epoch: 434\n",
      "FGW torch.Size([29508, 5]) 8.009048178792e-05\n",
      "Penalty params: tau=0.82912 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=434 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 434, train\n",
      " fgw:0.2220262\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2220262\n",
      "Measure Epoch 434, train\n",
      " similarity:0.0529285\n",
      " penlog:-3.3452506\n",
      "Metrics Epoch 434, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:11.8400000\n",
      " batch_invalid_valency_nodes:15.3043478\n",
      " batch_nodes_0degree:3.4600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.8000000\n",
      " batch_node_degree:1.5947826\n",
      "Logits [27.358484268188477, 2.3073196411132812, 54.168190002441406]\n",
      "Epoch duration: 2.0319907665252686\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_434\n",
      "Epoch: 435\n",
      "FGW torch.Size([29508, 5]) 8.139472629409283e-05\n",
      "Penalty params: tau=0.82877 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=435 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 435, train\n",
      " fgw:0.2180936\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2180936\n",
      "Measure Epoch 435, train\n",
      " similarity:0.0533017\n",
      " penlog:-7.5457940\n",
      "Metrics Epoch 435, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:11.3800000\n",
      " batch_invalid_valency_nodes:13.5652174\n",
      " batch_nodes_0degree:3.0000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.3400000\n",
      " batch_node_degree:1.6660870\n",
      "Logits [27.625072479248047, 2.3444392681121826, 54.031124114990234]\n",
      "Epoch duration: 2.2344982624053955\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_435\n",
      "Epoch: 436\n",
      "FGW torch.Size([29508, 5]) 8.182612509699538e-05\n",
      "Penalty params: tau=0.82841 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=436 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 436, train\n",
      " fgw:0.2196053\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2196053\n",
      "Measure Epoch 436, train\n",
      " similarity:0.0414667\n",
      " penlog:-15.4973394\n",
      "Metrics Epoch 436, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:99.9130435\n",
      " batch_molecular_disconnected_validity:84.0000000\n",
      " batch_connected_components:11.0000000\n",
      " batch_invalid_valency_nodes:13.2173913\n",
      " batch_nodes_0degree:2.8400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.9400000\n",
      " batch_node_degree:1.7182609\n",
      "Logits [27.756282806396484, 2.370077610015869, 54.06922149658203]\n",
      "Epoch duration: 2.0419704914093018\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_436\n",
      "Epoch: 437\n",
      "FGW torch.Size([29508, 5]) 8.300846820930019e-05\n",
      "Penalty params: tau=0.82805 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=437 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 437, train\n",
      " fgw:0.2196035\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2196035\n",
      "Measure Epoch 437, train\n",
      " similarity:0.0480169\n",
      " penlog:-13.5648002\n",
      "Metrics Epoch 437, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:86.0000000\n",
      " batch_connected_components:11.0600000\n",
      " batch_invalid_valency_nodes:13.3043478\n",
      " batch_nodes_0degree:2.9000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.0200000\n",
      " batch_node_degree:1.6904348\n",
      "Logits [27.81641960144043, 2.37017822265625, 54.46283721923828]\n",
      "Epoch duration: 1.9427294731140137\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_437\n",
      "Epoch: 438\n",
      "FGW torch.Size([29508, 5]) 7.955811452120543e-05\n",
      "Penalty params: tau=0.82769 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=438 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 438, train\n",
      " fgw:0.2178985\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2178985\n",
      "Measure Epoch 438, train\n",
      " similarity:0.0568258\n",
      " penlog:-9.4041205\n",
      "Metrics Epoch 438, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:11.4200000\n",
      " batch_invalid_valency_nodes:14.3478261\n",
      " batch_nodes_0degree:3.1800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.4000000\n",
      " batch_node_degree:1.6417391\n",
      "Logits [28.03310775756836, 2.380459785461426, 54.68857192993164]\n",
      "Epoch duration: 1.9598820209503174\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_438\n",
      "Epoch: 439\n",
      "FGW torch.Size([29508, 5]) 7.729134813416749e-05\n",
      "Penalty params: tau=0.82734 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=439 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 439, train\n",
      " fgw:0.2104519\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2104519\n",
      "Measure Epoch 439, train\n",
      " similarity:0.0561502\n",
      " penlog:-3.4514278\n",
      "Metrics Epoch 439, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:11.6600000\n",
      " batch_invalid_valency_nodes:14.4347826\n",
      " batch_nodes_0degree:3.2600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.6400000\n",
      " batch_node_degree:1.6208696\n",
      "Logits [28.400060653686523, 2.4091830253601074, 54.594425201416016]\n",
      "Epoch duration: 1.9495842456817627\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_439\n",
      "Epoch: 440\n",
      "FGW torch.Size([29508, 5]) 7.844076753826812e-05\n",
      "Penalty params: tau=0.82698 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=440 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 440, train\n",
      " fgw:0.2154636\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2154636\n",
      "Measure Epoch 440, train\n",
      " similarity:0.0553232\n",
      " penlog:-5.3923822\n",
      "Metrics Epoch 440, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:11.5600000\n",
      " batch_invalid_valency_nodes:14.2608696\n",
      " batch_nodes_0degree:3.2000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.5400000\n",
      " batch_node_degree:1.6278261\n",
      "Logits [28.709531784057617, 2.4354982376098633, 54.635128021240234]\n",
      "Epoch duration: 1.8977150917053223\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_440\n",
      "Epoch: 441\n",
      "FGW torch.Size([29508, 5]) 8.055925718508661e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penalty params: tau=0.82662 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=441 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 441, train\n",
      " fgw:0.2185696\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2185696\n",
      "Measure Epoch 441, train\n",
      " similarity:0.0554238\n",
      " penlog:-5.4211402\n",
      "Metrics Epoch 441, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:11.4000000\n",
      " batch_invalid_valency_nodes:13.5652174\n",
      " batch_nodes_0degree:3.0400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.3800000\n",
      " batch_node_degree:1.6382609\n",
      "Logits [28.875333786010742, 2.4423539638519287, 54.84172439575195]\n",
      "Epoch duration: 2.139193058013916\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_441\n",
      "Epoch: 442\n",
      "FGW torch.Size([29508, 5]) 8.175751281669363e-05\n",
      "Penalty params: tau=0.82627 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=442 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 442, train\n",
      " fgw:0.2184264\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2184264\n",
      "Measure Epoch 442, train\n",
      " similarity:0.0531979\n",
      " penlog:-7.4382984\n",
      "Metrics Epoch 442, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:11.0600000\n",
      " batch_invalid_valency_nodes:12.6956522\n",
      " batch_nodes_0degree:2.8200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.0200000\n",
      " batch_node_degree:1.6678261\n",
      "Logits [28.960176467895508, 2.438488721847534, 55.106082916259766]\n",
      "Epoch duration: 1.851531744003296\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_442\n",
      "Epoch: 443\n",
      "FGW torch.Size([29508, 5]) 8.139148121699691e-05\n",
      "Penalty params: tau=0.82591 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=443 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 443, train\n",
      " fgw:0.2121276\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2121276\n",
      "Measure Epoch 443, train\n",
      " similarity:0.0524334\n",
      " penlog:-9.4886497\n",
      "Metrics Epoch 443, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:11.1400000\n",
      " batch_invalid_valency_nodes:12.8695652\n",
      " batch_nodes_0degree:2.8400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.1000000\n",
      " batch_node_degree:1.6660870\n",
      "Logits [29.021419525146484, 2.4444785118103027, 55.228206634521484]\n",
      "Epoch duration: 1.8896067142486572\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_443\n",
      "Epoch: 444\n",
      "FGW torch.Size([29508, 5]) 7.931204163469374e-05\n",
      "Penalty params: tau=0.82555 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=444 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 444, train\n",
      " fgw:0.2141066\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2141066\n",
      "Measure Epoch 444, train\n",
      " similarity:0.0599399\n",
      " penlog:-3.4593796\n",
      "Metrics Epoch 444, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:11.3200000\n",
      " batch_invalid_valency_nodes:13.2173913\n",
      " batch_nodes_0degree:2.9800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.3000000\n",
      " batch_node_degree:1.6504348\n",
      "Logits [29.10537338256836, 2.462493896484375, 55.30144119262695]\n",
      "Epoch duration: 2.03813099861145\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_444\n",
      "Epoch: 445\n",
      "FGW torch.Size([29508, 5]) 7.766042108414695e-05\n",
      "Penalty params: tau=0.82520 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=445 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 445, train\n",
      " fgw:0.2116166\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2116166\n",
      "Measure Epoch 445, train\n",
      " similarity:0.0560833\n",
      " penlog:-5.5128784\n",
      "Metrics Epoch 445, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:11.4400000\n",
      " batch_invalid_valency_nodes:13.8260870\n",
      " batch_nodes_0degree:3.0800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:100.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.4200000\n",
      " batch_node_degree:1.6521739\n",
      "Logits [29.364349365234375, 2.4907078742980957, 55.319236755371094]\n",
      "Epoch duration: 1.909522294998169\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_445\n",
      "Epoch: 446\n",
      "FGW torch.Size([29508, 5]) 7.715871470281854e-05\n",
      "Penalty params: tau=0.82484 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=446 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 446, train\n",
      " fgw:0.2088412\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2088412\n",
      "Measure Epoch 446, train\n",
      " similarity:0.0629267\n",
      " penlog:-5.6173527\n",
      "Metrics Epoch 446, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:11.2800000\n",
      " batch_invalid_valency_nodes:13.3043478\n",
      " batch_nodes_0degree:2.9400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.2600000\n",
      " batch_node_degree:1.6765217\n",
      "Logits [29.722585678100586, 2.5156285762786865, 55.423583984375]\n",
      "Epoch duration: 1.9766111373901367\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_446\n",
      "Epoch: 447\n",
      "FGW torch.Size([29508, 5]) 7.839666795916855e-05\n",
      "Penalty params: tau=0.82448 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=447 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 447, train\n",
      " fgw:0.2096883\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2096883\n",
      "Measure Epoch 447, train\n",
      " similarity:0.0702845\n",
      " penlog:-7.7039327\n",
      "Metrics Epoch 447, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:10.9400000\n",
      " batch_invalid_valency_nodes:12.6086957\n",
      " batch_nodes_0degree:2.7200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.9000000\n",
      " batch_node_degree:1.7113043\n",
      "Logits [30.066287994384766, 2.5327224731445312, 55.63127899169922]\n",
      "Epoch duration: 2.072222948074341\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_447\n",
      "Epoch: 448\n",
      "FGW torch.Size([29508, 5]) 7.929695857455954e-05\n",
      "Penalty params: tau=0.82413 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=448 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 448, train\n",
      " fgw:0.2086290\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2086290\n",
      "Measure Epoch 448, train\n",
      " similarity:0.0652703\n",
      " penlog:-9.6267599\n",
      "Metrics Epoch 448, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:10.8200000\n",
      " batch_invalid_valency_nodes:12.4347826\n",
      " batch_nodes_0degree:2.6600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.7800000\n",
      " batch_node_degree:1.7165217\n",
      "Logits [30.235061645507812, 2.5416412353515625, 55.855472564697266]\n",
      "Epoch duration: 1.9869911670684814\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_448\n",
      "Epoch: 449\n",
      "FGW torch.Size([29508, 5]) 7.800719322403893e-05\n",
      "Penalty params: tau=0.82377 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=449 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 449, train\n",
      " fgw:0.2067685\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2067685\n",
      "Measure Epoch 449, train\n",
      " similarity:0.0652143\n",
      " penlog:-7.5287297\n",
      "Metrics Epoch 449, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:11.0000000\n",
      " batch_invalid_valency_nodes:13.1304348\n",
      " batch_nodes_0degree:2.8400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.9600000\n",
      " batch_node_degree:1.6817391\n",
      "Logits [30.28721809387207, 2.5480387210845947, 55.962100982666016]\n",
      "Epoch duration: 2.1958582401275635\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 450\n",
      "FGW torch.Size([29508, 5]) 7.805187487974763e-05\n",
      "Penalty params: tau=0.82342 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=450 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 450, train\n",
      " fgw:0.2106955\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2106955\n",
      "Measure Epoch 450, train\n",
      " similarity:0.0596181\n",
      " penlog:-5.4574169\n",
      "Metrics Epoch 450, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:11.3600000\n",
      " batch_invalid_valency_nodes:13.5652174\n",
      " batch_nodes_0degree:3.0000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.3400000\n",
      " batch_node_degree:1.6417391\n",
      "Logits [30.3505916595459, 2.5557188987731934, 55.97969055175781]\n",
      "Epoch duration: 1.9482941627502441\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_450\n",
      "Epoch: 451\n",
      "FGW torch.Size([29508, 5]) 7.660573464818299e-05\n",
      "Penalty params: tau=0.82306 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=451 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 451, train\n",
      " fgw:0.2054385\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2054385\n",
      "Measure Epoch 451, train\n",
      " similarity:0.0612262\n",
      " penlog:-3.4421092\n",
      "Metrics Epoch 451, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:11.5600000\n",
      " batch_invalid_valency_nodes:14.3478261\n",
      " batch_nodes_0degree:3.2400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.5400000\n",
      " batch_node_degree:1.6156522\n",
      "Logits [30.46098518371582, 2.564711809158325, 56.111793518066406]\n",
      "Epoch duration: 2.132307767868042\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_451\n",
      "Epoch: 452\n",
      "FGW torch.Size([29508, 5]) 7.748076313873753e-05\n",
      "Penalty params: tau=0.82271 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=452 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 452, train\n",
      " fgw:0.2074227\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2074227\n",
      "Measure Epoch 452, train\n",
      " similarity:0.0594918\n",
      " penlog:-5.5384940\n",
      "Metrics Epoch 452, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:11.4000000\n",
      " batch_invalid_valency_nodes:13.6521739\n",
      " batch_nodes_0degree:3.0600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.3800000\n",
      " batch_node_degree:1.6365217\n",
      "Logits [30.642385482788086, 2.5768306255340576, 56.241851806640625]\n",
      "Epoch duration: 2.456083059310913\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_452\n",
      "Epoch: 453\n",
      "FGW torch.Size([29508, 5]) 7.841540355002508e-05\n",
      "Penalty params: tau=0.82235 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=453 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 453, train\n",
      " fgw:0.2102382\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2102382\n",
      "Measure Epoch 453, train\n",
      " similarity:0.0630495\n",
      " penlog:-9.6068281\n",
      "Metrics Epoch 453, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:11.2200000\n",
      " batch_invalid_valency_nodes:13.2173913\n",
      " batch_nodes_0degree:2.9200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.1800000\n",
      " batch_node_degree:1.6626087\n",
      "Logits [30.901737213134766, 2.5992178916931152, 56.35526657104492]\n",
      "Epoch duration: 2.0313854217529297\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_453\n",
      "Epoch: 454\n",
      "FGW torch.Size([29508, 5]) 7.818756421329454e-05\n",
      "Penalty params: tau=0.82200 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=454 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 454, train\n",
      " fgw:0.2141889\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2141889\n",
      "Measure Epoch 454, train\n",
      " similarity:0.0626072\n",
      " penlog:-5.5355494\n",
      "Metrics Epoch 454, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:11.0800000\n",
      " batch_invalid_valency_nodes:12.3478261\n",
      " batch_nodes_0degree:2.7600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.0400000\n",
      " batch_node_degree:1.6782609\n",
      "Logits [31.128156661987305, 2.617488145828247, 56.37178421020508]\n",
      "Epoch duration: 2.3613784313201904\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_454\n",
      "Epoch: 455\n",
      "FGW torch.Size([29508, 5]) 7.852792623452842e-05\n",
      "Penalty params: tau=0.82164 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=455 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 455, train\n",
      " fgw:0.2136317\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2136317\n",
      "Measure Epoch 455, train\n",
      " similarity:0.0659857\n",
      " penlog:-3.4906331\n",
      "Metrics Epoch 455, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:11.0800000\n",
      " batch_invalid_valency_nodes:12.4347826\n",
      " batch_nodes_0degree:2.8000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.0400000\n",
      " batch_node_degree:1.6713043\n",
      "Logits [31.26947784423828, 2.628667116165161, 56.509395599365234]\n",
      "Epoch duration: 2.2935688495635986\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_455\n",
      "Epoch: 456\n",
      "FGW torch.Size([29508, 5]) 7.824974454706535e-05\n",
      "Penalty params: tau=0.82129 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=456 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 456, train\n",
      " fgw:0.2090241\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2090241\n",
      "Measure Epoch 456, train\n",
      " similarity:0.0683906\n",
      " penlog:-1.5562892\n",
      "Metrics Epoch 456, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:11.2600000\n",
      " batch_invalid_valency_nodes:13.1304348\n",
      " batch_nodes_0degree:2.9800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.2200000\n",
      " batch_node_degree:1.6504348\n",
      "Logits [31.354307174682617, 2.642874002456665, 56.69413757324219]\n",
      "Epoch duration: 2.076390027999878\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_456\n",
      "Epoch: 457\n",
      "FGW torch.Size([29508, 5]) 7.711587386438623e-05\n",
      "Penalty params: tau=0.82093 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=457 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 457, train\n",
      " fgw:0.2098797\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2098797\n",
      "Measure Epoch 457, train\n",
      " similarity:0.0646299\n",
      " penlog:-5.6009174\n",
      "Metrics Epoch 457, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:11.2200000\n",
      " batch_invalid_valency_nodes:13.3043478\n",
      " batch_nodes_0degree:2.9800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:98.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.2000000\n",
      " batch_node_degree:1.6608696\n",
      "Logits [31.46455192565918, 2.6567115783691406, 56.72501754760742]\n",
      "Epoch duration: 2.046595573425293\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_457\n",
      "Epoch: 458\n",
      "FGW torch.Size([29508, 5]) 7.649351755389944e-05\n",
      "Penalty params: tau=0.82058 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=458 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 458, train\n",
      " fgw:0.2076764\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2076764\n",
      "Measure Epoch 458, train\n",
      " similarity:0.0650520\n",
      " penlog:-5.5365783\n",
      "Metrics Epoch 458, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:11.2200000\n",
      " batch_invalid_valency_nodes:12.6956522\n",
      " batch_nodes_0degree:2.7800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.1800000\n",
      " batch_node_degree:1.6765217\n",
      "Logits [31.624238967895508, 2.659166097640991, 56.73692321777344]\n",
      "Epoch duration: 2.1806578636169434\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 459\n",
      "FGW torch.Size([29508, 5]) 7.741754234302789e-05\n",
      "Penalty params: tau=0.82022 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=459 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 459, train\n",
      " fgw:0.2088293\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2088293\n",
      "Measure Epoch 459, train\n",
      " similarity:0.0698879\n",
      " penlog:-5.5729834\n",
      "Metrics Epoch 459, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:11.0400000\n",
      " batch_invalid_valency_nodes:12.3478261\n",
      " batch_nodes_0degree:2.6800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.9800000\n",
      " batch_node_degree:1.6956522\n",
      "Logits [31.801462173461914, 2.6641530990600586, 56.77625274658203]\n",
      "Epoch duration: 2.114142656326294\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_459\n",
      "Epoch: 460\n",
      "FGW torch.Size([29508, 5]) 7.820703467587009e-05\n",
      "Penalty params: tau=0.81987 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=460 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 460, train\n",
      " fgw:0.2055862\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2055862\n",
      "Measure Epoch 460, train\n",
      " similarity:0.0712651\n",
      " penlog:-5.5641282\n",
      "Metrics Epoch 460, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:10.9600000\n",
      " batch_invalid_valency_nodes:11.8260870\n",
      " batch_nodes_0degree:2.5600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.8800000\n",
      " batch_node_degree:1.7165217\n",
      "Logits [31.94106674194336, 2.6764254570007324, 56.943172454833984]\n",
      "Epoch duration: 2.15179181098938\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_460\n",
      "Epoch: 461\n",
      "FGW torch.Size([29508, 5]) 7.767663919366896e-05\n",
      "Penalty params: tau=0.81952 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=461 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 461, train\n",
      " fgw:0.2075065\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2075065\n",
      "Measure Epoch 461, train\n",
      " similarity:0.0691752\n",
      " penlog:-9.6393386\n",
      "Metrics Epoch 461, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:10.7600000\n",
      " batch_invalid_valency_nodes:11.3913043\n",
      " batch_nodes_0degree:2.4200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.6800000\n",
      " batch_node_degree:1.7356522\n",
      "Logits [32.09717559814453, 2.6940879821777344, 57.064090728759766]\n",
      "Epoch duration: 2.3207881450653076\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_461\n",
      "Epoch: 462\n",
      "FGW torch.Size([29508, 5]) 7.660360279260203e-05\n",
      "Penalty params: tau=0.81916 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=462 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 462, train\n",
      " fgw:0.2187437\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2187437\n",
      "Measure Epoch 462, train\n",
      " similarity:0.0723923\n",
      " penlog:-7.6327096\n",
      "Metrics Epoch 462, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:10.9400000\n",
      " batch_invalid_valency_nodes:11.7391304\n",
      " batch_nodes_0degree:2.5200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.8600000\n",
      " batch_node_degree:1.7060870\n",
      "Logits [32.21334457397461, 2.707197666168213, 57.13828659057617]\n",
      "Epoch duration: 2.094343662261963\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_462\n",
      "Epoch: 463\n",
      "FGW torch.Size([29508, 5]) 7.63734569773078e-05\n",
      "Penalty params: tau=0.81881 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=463 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 463, train\n",
      " fgw:0.2065226\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2065226\n",
      "Measure Epoch 463, train\n",
      " similarity:0.0712831\n",
      " penlog:-3.5513992\n",
      "Metrics Epoch 463, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:11.1800000\n",
      " batch_invalid_valency_nodes:12.2608696\n",
      " batch_nodes_0degree:2.7600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:94.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.0800000\n",
      " batch_node_degree:1.6713043\n",
      "Logits [32.315185546875, 2.7138688564300537, 57.146331787109375]\n",
      "Epoch duration: 2.188776731491089\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_463\n",
      "Epoch: 464\n",
      "FGW torch.Size([29508, 5]) 7.660427945666015e-05\n",
      "Penalty params: tau=0.81845 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=464 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 464, train\n",
      " fgw:0.2017085\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2017085\n",
      "Measure Epoch 464, train\n",
      " similarity:0.0716549\n",
      " penlog:-1.5483570\n",
      "Metrics Epoch 464, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:11.1600000\n",
      " batch_invalid_valency_nodes:12.4347826\n",
      " batch_nodes_0degree:2.8200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:94.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.0600000\n",
      " batch_node_degree:1.6695652\n",
      "Logits [32.364837646484375, 2.717473030090332, 57.255157470703125]\n",
      "Epoch duration: 2.0859458446502686\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_464\n",
      "Epoch: 465\n",
      "FGW torch.Size([29508, 5]) 7.74761283537373e-05\n",
      "Penalty params: tau=0.81810 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=465 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 465, train\n",
      " fgw:0.2054244\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2054244\n",
      "Measure Epoch 465, train\n",
      " similarity:0.0688823\n",
      " penlog:-3.5392178\n",
      "Metrics Epoch 465, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:11.1200000\n",
      " batch_invalid_valency_nodes:12.9565217\n",
      " batch_nodes_0degree:2.9200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:94.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.0400000\n",
      " batch_node_degree:1.6695652\n",
      "Logits [32.455142974853516, 2.732956886291504, 57.31996536254883]\n",
      "Epoch duration: 2.014223337173462\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_465\n",
      "Epoch: 466\n",
      "FGW torch.Size([29508, 5]) 7.65244330978021e-05\n",
      "Penalty params: tau=0.81775 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=466 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 466, train\n",
      " fgw:0.2104111\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2104111\n",
      "Measure Epoch 466, train\n",
      " similarity:0.0741753\n",
      " penlog:-1.5496795\n",
      "Metrics Epoch 466, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:11.1400000\n",
      " batch_invalid_valency_nodes:12.4347826\n",
      " batch_nodes_0degree:2.8200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:94.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.0200000\n",
      " batch_node_degree:1.6765217\n",
      "Logits [32.649986267089844, 2.7500343322753906, 57.3316764831543]\n",
      "Epoch duration: 2.170876979827881\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_466\n",
      "Epoch: 467\n",
      "FGW torch.Size([29508, 5]) 7.545789412688464e-05\n",
      "Penalty params: tau=0.81740 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=467 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 467, train\n",
      " fgw:0.2064193\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2064193\n",
      "Measure Epoch 467, train\n",
      " similarity:0.0708422\n",
      " penlog:-1.5061993\n",
      "Metrics Epoch 467, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:11.1000000\n",
      " batch_invalid_valency_nodes:12.0000000\n",
      " batch_nodes_0degree:2.7200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:94.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.9800000\n",
      " batch_node_degree:1.6904348\n",
      "Logits [32.826316833496094, 2.7556114196777344, 57.339691162109375]\n",
      "Epoch duration: 2.0036561489105225\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 468\n",
      "FGW torch.Size([29508, 5]) 7.57559246267192e-05\n",
      "Penalty params: tau=0.81704 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=468 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 468, train\n",
      " fgw:0.2079844\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2079844\n",
      "Measure Epoch 468, train\n",
      " similarity:0.0705543\n",
      " penlog:-1.5350692\n",
      "Metrics Epoch 468, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:11.0800000\n",
      " batch_invalid_valency_nodes:11.9130435\n",
      " batch_nodes_0degree:2.7000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:94.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.9400000\n",
      " batch_node_degree:1.6939130\n",
      "Logits [33.02677917480469, 2.756649971008301, 57.41078186035156]\n",
      "Epoch duration: 2.2436015605926514\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_468\n",
      "Epoch: 469\n",
      "FGW torch.Size([29508, 5]) 7.66161028877832e-05\n",
      "Penalty params: tau=0.81669 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=469 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 469, train\n",
      " fgw:0.2026695\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2026695\n",
      "Measure Epoch 469, train\n",
      " similarity:0.0712565\n",
      " penlog:-3.5422208\n",
      "Metrics Epoch 469, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:10.9600000\n",
      " batch_invalid_valency_nodes:11.9130435\n",
      " batch_nodes_0degree:2.6400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:94.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.8400000\n",
      " batch_node_degree:1.7078261\n",
      "Logits [33.13432312011719, 2.765961170196533, 57.43196487426758]\n",
      "Epoch duration: 2.0303635597229004\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_469\n",
      "Epoch: 470\n",
      "FGW torch.Size([29508, 5]) 7.800418097758666e-05\n",
      "Penalty params: tau=0.81634 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=470 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 470, train\n",
      " fgw:0.2154395\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2154395\n",
      "Measure Epoch 470, train\n",
      " similarity:0.0718719\n",
      " penlog:-5.5864273\n",
      "Metrics Epoch 470, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:10.7400000\n",
      " batch_invalid_valency_nodes:11.3043478\n",
      " batch_nodes_0degree:2.4800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:94.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.6400000\n",
      " batch_node_degree:1.7304348\n",
      "Logits [33.15864944458008, 2.786592483520508, 57.39460372924805]\n",
      "Epoch duration: 2.2625834941864014\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_470\n",
      "Epoch: 471\n",
      "FGW torch.Size([29508, 5]) 7.615992217324674e-05\n",
      "Penalty params: tau=0.81598 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=471 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 471, train\n",
      " fgw:0.2073549\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2073549\n",
      "Measure Epoch 471, train\n",
      " similarity:0.0751576\n",
      " penlog:-3.5695947\n",
      "Metrics Epoch 471, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:10.7800000\n",
      " batch_invalid_valency_nodes:11.3913043\n",
      " batch_nodes_0degree:2.5200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:94.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.6800000\n",
      " batch_node_degree:1.7200000\n",
      "Logits [33.12184143066406, 2.7886908054351807, 57.53300857543945]\n",
      "Epoch duration: 1.9007880687713623\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_471\n",
      "Epoch: 472\n",
      "FGW torch.Size([29508, 5]) 7.454499427694827e-05\n",
      "Penalty params: tau=0.81563 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=472 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 472, train\n",
      " fgw:0.2002814\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2002814\n",
      "Measure Epoch 472, train\n",
      " similarity:0.0735969\n",
      " penlog:-1.5582363\n",
      "Metrics Epoch 472, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:10.9600000\n",
      " batch_invalid_valency_nodes:11.3043478\n",
      " batch_nodes_0degree:2.5600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:94.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.8600000\n",
      " batch_node_degree:1.7026087\n",
      "Logits [33.158390045166016, 2.7854626178741455, 57.59551239013672]\n",
      "Epoch duration: 2.062288999557495\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_472\n",
      "Epoch: 473\n",
      "FGW torch.Size([29508, 5]) 7.484636444132775e-05\n",
      "Penalty params: tau=0.81528 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=473 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 473, train\n",
      " fgw:0.2028569\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2028569\n",
      "Measure Epoch 473, train\n",
      " similarity:0.0711162\n",
      " penlog:-1.5919332\n",
      "Metrics Epoch 473, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:11.0600000\n",
      " batch_invalid_valency_nodes:11.6521739\n",
      " batch_nodes_0degree:2.6400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:94.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.9600000\n",
      " batch_node_degree:1.6886957\n",
      "Logits [33.22417068481445, 2.7815091609954834, 57.63968276977539]\n",
      "Epoch duration: 2.092818260192871\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_473\n",
      "Epoch: 474\n",
      "FGW torch.Size([29508, 5]) 7.424976502079517e-05\n",
      "Penalty params: tau=0.81493 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=474 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 474, train\n",
      " fgw:0.1953840\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.1953840\n",
      "Measure Epoch 474, train\n",
      " similarity:0.0704887\n",
      " penlog:-1.5224335\n",
      "Metrics Epoch 474, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:11.1600000\n",
      " batch_invalid_valency_nodes:11.7391304\n",
      " batch_nodes_0degree:2.6600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:94.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-11.0600000\n",
      " batch_node_degree:1.6765217\n",
      "Logits [33.228919982910156, 2.785170316696167, 57.64762496948242]\n",
      "Epoch duration: 1.9742155075073242\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_474\n",
      "Epoch: 475\n",
      "FGW torch.Size([29508, 5]) 7.587658183183521e-05\n",
      "Penalty params: tau=0.81458 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=475 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 475, train\n",
      " fgw:0.1991601\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.1991601\n",
      "Measure Epoch 475, train\n",
      " similarity:0.0748997\n",
      " penlog:-1.5083591\n",
      "Metrics Epoch 475, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:10.9800000\n",
      " batch_invalid_valency_nodes:11.3913043\n",
      " batch_nodes_0degree:2.5800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:94.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.8600000\n",
      " batch_node_degree:1.6991304\n",
      "Logits [33.30598449707031, 2.7900145053863525, 57.672515869140625]\n",
      "Epoch duration: 1.8834631443023682\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_475\n",
      "Epoch: 476\n",
      "FGW torch.Size([29508, 5]) 7.59339309297502e-05\n",
      "Penalty params: tau=0.81422 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=476 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 476, train\n",
      " fgw:0.2028454\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2028454\n",
      "Measure Epoch 476, train\n",
      " similarity:0.0749019\n",
      " penlog:-1.5373103\n",
      "Metrics Epoch 476, train\n",
      " batch_molecular_validity:0.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:98.0000000\n",
      " batch_connected_components:10.9600000\n",
      " batch_invalid_valency_nodes:11.3043478\n",
      " batch_nodes_0degree:2.5600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:94.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.8400000\n",
      " batch_node_degree:1.6991304\n",
      "Logits [33.44852066040039, 2.799492359161377, 57.798805236816406]\n",
      "Epoch duration: 2.115830898284912\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 477\n",
      "FGW torch.Size([29508, 5]) 7.435221050400287e-05\n",
      "Penalty params: tau=0.81387 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=477 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 477, train\n",
      " fgw:0.1951527\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.1951527\n",
      "Measure Epoch 477, train\n",
      " similarity:0.0886130\n",
      " penlog:-5.6834139\n",
      "Metrics Epoch 477, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:10.9200000\n",
      " batch_invalid_valency_nodes:11.7391304\n",
      " batch_nodes_0degree:2.6200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:94.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.8400000\n",
      " batch_node_degree:1.6956522\n",
      "Logits [33.517555236816406, 2.81040096282959, 58.03749465942383]\n",
      "Epoch duration: 2.025338649749756\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_477\n",
      "Epoch: 478\n",
      "FGW torch.Size([29508, 5]) 7.366157660726458e-05\n",
      "Penalty params: tau=0.81352 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=478 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 478, train\n",
      " fgw:0.1964760\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.1964760\n",
      "Measure Epoch 478, train\n",
      " similarity:0.0895921\n",
      " penlog:-3.6098451\n",
      "Metrics Epoch 478, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:11.0200000\n",
      " batch_invalid_valency_nodes:11.7391304\n",
      " batch_nodes_0degree:2.6400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:94.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.9400000\n",
      " batch_node_degree:1.6904348\n",
      "Logits [33.63166427612305, 2.8236618041992188, 58.121578216552734]\n",
      "Epoch duration: 2.0615530014038086\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_478\n",
      "Epoch: 479\n",
      "FGW torch.Size([29508, 5]) 7.500170613639057e-05\n",
      "Penalty params: tau=0.81317 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=479 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 479, train\n",
      " fgw:0.2048429\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2048429\n",
      "Measure Epoch 479, train\n",
      " similarity:0.0893424\n",
      " penlog:-3.6442362\n",
      "Metrics Epoch 479, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:10.7800000\n",
      " batch_invalid_valency_nodes:11.0434783\n",
      " batch_nodes_0degree:2.4800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:94.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.7000000\n",
      " batch_node_degree:1.7182609\n",
      "Logits [33.83660888671875, 2.8434369564056396, 58.006126403808594]\n",
      "Epoch duration: 2.165679454803467\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_479\n",
      "Epoch: 480\n",
      "FGW torch.Size([29508, 5]) 7.578590884804726e-05\n",
      "Penalty params: tau=0.81282 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=480 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 480, train\n",
      " fgw:0.2040080\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2040080\n",
      "Measure Epoch 480, train\n",
      " similarity:0.0904412\n",
      " penlog:-7.6839826\n",
      "Metrics Epoch 480, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:10.5800000\n",
      " batch_invalid_valency_nodes:11.3913043\n",
      " batch_nodes_0degree:2.5200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:94.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.4800000\n",
      " batch_node_degree:1.7373913\n",
      "Logits [34.00350570678711, 2.855175256729126, 58.035377502441406]\n",
      "Epoch duration: 2.213308334350586\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_480\n",
      "Epoch: 481\n",
      "FGW torch.Size([29508, 5]) 7.698346598772332e-05\n",
      "Penalty params: tau=0.81247 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=481 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 481, train\n",
      " fgw:0.2011514\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2011514\n",
      "Measure Epoch 481, train\n",
      " similarity:0.0886901\n",
      " penlog:-5.6399177\n",
      "Metrics Epoch 481, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:10.5400000\n",
      " batch_invalid_valency_nodes:11.1304348\n",
      " batch_nodes_0degree:2.4800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.4800000\n",
      " batch_node_degree:1.7217391\n",
      "Logits [34.09817123413086, 2.8589465618133545, 58.264286041259766]\n",
      "Epoch duration: 1.9315669536590576\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_481\n",
      "Epoch: 482\n",
      "FGW torch.Size([29508, 5]) 7.599368836963549e-05\n",
      "Penalty params: tau=0.81212 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=482 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 482, train\n",
      " fgw:0.2049918\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2049918\n",
      "Measure Epoch 482, train\n",
      " similarity:0.0878054\n",
      " penlog:-5.6084159\n",
      "Metrics Epoch 482, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:10.7000000\n",
      " batch_invalid_valency_nodes:11.1304348\n",
      " batch_nodes_0degree:2.4800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.6400000\n",
      " batch_node_degree:1.7147826\n",
      "Logits [34.237369537353516, 2.8656210899353027, 58.46797180175781]\n",
      "Epoch duration: 2.019960880279541\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_482\n",
      "Epoch: 483\n",
      "FGW torch.Size([29508, 5]) 7.32936678105034e-05\n",
      "Penalty params: tau=0.81177 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=483 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 483, train\n",
      " fgw:0.2043952\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2043952\n",
      "Measure Epoch 483, train\n",
      " similarity:0.0873139\n",
      " penlog:-5.5965577\n",
      "Metrics Epoch 483, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:10.8000000\n",
      " batch_invalid_valency_nodes:11.2173913\n",
      " batch_nodes_0degree:2.5000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.7400000\n",
      " batch_node_degree:1.7095652\n",
      "Logits [34.40837860107422, 2.880302906036377, 58.3859748840332]\n",
      "Epoch duration: 1.9636824131011963\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_483\n",
      "Epoch: 484\n",
      "FGW torch.Size([29508, 5]) 7.374610140686855e-05\n",
      "Penalty params: tau=0.81142 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=484 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 484, train\n",
      " fgw:0.2081149\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2081149\n",
      "Measure Epoch 484, train\n",
      " similarity:0.0811573\n",
      " penlog:-9.6295624\n",
      "Metrics Epoch 484, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:10.6200000\n",
      " batch_invalid_valency_nodes:10.6956522\n",
      " batch_nodes_0degree:2.3000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.6000000\n",
      " batch_node_degree:1.7356522\n",
      "Logits [34.53087615966797, 2.8858284950256348, 58.37233352661133]\n",
      "Epoch duration: 2.0297553539276123\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_484\n",
      "Epoch: 485\n",
      "FGW torch.Size([29508, 5]) 7.496558828279376e-05\n",
      "Penalty params: tau=0.81107 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=485 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 485, train\n",
      " fgw:0.1972439\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.1972439\n",
      "Measure Epoch 485, train\n",
      " similarity:0.0855394\n",
      " penlog:-9.5886447\n",
      "Metrics Epoch 485, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:10.4800000\n",
      " batch_invalid_valency_nodes:10.6086957\n",
      " batch_nodes_0degree:2.2600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.4400000\n",
      " batch_node_degree:1.7495652\n",
      "Logits [34.53474426269531, 2.88618803024292, 58.481719970703125]\n",
      "Epoch duration: 2.1280007362365723\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 486\n",
      "FGW torch.Size([29508, 5]) 7.684392767259851e-05\n",
      "Penalty params: tau=0.81072 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=486 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 486, train\n",
      " fgw:0.2056659\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2056659\n",
      "Measure Epoch 486, train\n",
      " similarity:0.0948989\n",
      " penlog:-7.5551777\n",
      "Metrics Epoch 486, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:92.0000000\n",
      " batch_connected_components:10.4200000\n",
      " batch_invalid_valency_nodes:10.1739130\n",
      " batch_nodes_0degree:2.1800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:92.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.3200000\n",
      " batch_node_degree:1.7460870\n",
      "Logits [34.51192855834961, 2.881490468978882, 58.70412063598633]\n",
      "Epoch duration: 2.1274166107177734\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_486\n",
      "Epoch: 487\n",
      "FGW torch.Size([29508, 5]) 7.622270641149953e-05\n",
      "Penalty params: tau=0.81037 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=487 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 487, train\n",
      " fgw:0.2059727\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2059727\n",
      "Measure Epoch 487, train\n",
      " similarity:0.0907037\n",
      " penlog:-9.5618953\n",
      "Metrics Epoch 487, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:10.5000000\n",
      " batch_invalid_valency_nodes:10.5217391\n",
      " batch_nodes_0degree:2.2600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:92.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.4000000\n",
      " batch_node_degree:1.7356522\n",
      "Logits [34.57300567626953, 2.884915351867676, 58.823917388916016]\n",
      "Epoch duration: 1.9711923599243164\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_487\n",
      "Epoch: 488\n",
      "FGW torch.Size([29508, 5]) 7.40818286431022e-05\n",
      "Penalty params: tau=0.81002 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=488 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 488, train\n",
      " fgw:0.1993605\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.1993605\n",
      "Measure Epoch 488, train\n",
      " similarity:0.0817592\n",
      " penlog:-11.6359879\n",
      "Metrics Epoch 488, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:88.0000000\n",
      " batch_connected_components:10.7000000\n",
      " batch_invalid_valency_nodes:11.4782609\n",
      " batch_nodes_0degree:2.4400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.6400000\n",
      " batch_node_degree:1.7252174\n",
      "Logits [34.75129699707031, 2.8987953662872314, 58.78715515136719]\n",
      "Epoch duration: 1.937103509902954\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_488\n",
      "Epoch: 489\n",
      "FGW torch.Size([29508, 5]) 7.341633317992091e-05\n",
      "Penalty params: tau=0.80967 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=489 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 489, train\n",
      " fgw:0.1996587\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.1996587\n",
      "Measure Epoch 489, train\n",
      " similarity:0.0828224\n",
      " penlog:-9.6974996\n",
      "Metrics Epoch 489, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:10.7600000\n",
      " batch_invalid_valency_nodes:10.9565217\n",
      " batch_nodes_0degree:2.4000000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.7200000\n",
      " batch_node_degree:1.7234783\n",
      "Logits [34.8577995300293, 2.912670135498047, 58.70576477050781]\n",
      "Epoch duration: 2.1319010257720947\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_489\n",
      "Epoch: 490\n",
      "FGW torch.Size([29508, 5]) 7.480976637452841e-05\n",
      "Penalty params: tau=0.80932 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=490 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 490, train\n",
      " fgw:0.2041641\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2041641\n",
      "Measure Epoch 490, train\n",
      " similarity:0.0856272\n",
      " penlog:-5.6232456\n",
      "Metrics Epoch 490, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:10.7200000\n",
      " batch_invalid_valency_nodes:11.1304348\n",
      " batch_nodes_0degree:2.4600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:94.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.6600000\n",
      " batch_node_degree:1.7130435\n",
      "Logits [34.88827896118164, 2.9181859493255615, 58.847267150878906]\n",
      "Epoch duration: 2.2257919311523438\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_490\n",
      "Epoch: 491\n",
      "FGW torch.Size([29508, 5]) 7.54372522351332e-05\n",
      "Penalty params: tau=0.80897 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=491 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 491, train\n",
      " fgw:0.2003905\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2003905\n",
      "Measure Epoch 491, train\n",
      " similarity:0.0875340\n",
      " penlog:-3.6261812\n",
      "Metrics Epoch 491, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:96.0000000\n",
      " batch_connected_components:10.6800000\n",
      " batch_invalid_valency_nodes:11.2173913\n",
      " batch_nodes_0degree:2.5200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:94.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.6000000\n",
      " batch_node_degree:1.6991304\n",
      "Logits [34.903282165527344, 2.9184629917144775, 59.020751953125]\n",
      "Epoch duration: 1.969956398010254\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_491\n",
      "Epoch: 492\n",
      "FGW torch.Size([29508, 5]) 7.426088268402964e-05\n",
      "Penalty params: tau=0.80862 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=492 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 492, train\n",
      " fgw:0.2036216\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2036216\n",
      "Measure Epoch 492, train\n",
      " similarity:0.0854916\n",
      " penlog:-5.5989078\n",
      "Metrics Epoch 492, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:10.8200000\n",
      " batch_invalid_valency_nodes:12.0000000\n",
      " batch_nodes_0degree:2.6800000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:94.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.7400000\n",
      " batch_node_degree:1.7008696\n",
      "Logits [34.96405792236328, 2.9276812076568604, 58.989601135253906]\n",
      "Epoch duration: 2.2061851024627686\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_492\n",
      "Epoch: 493\n",
      "FGW torch.Size([29508, 5]) 7.3379444074817e-05\n",
      "Penalty params: tau=0.80827 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=493 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 493, train\n",
      " fgw:0.2033390\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2033390\n",
      "Measure Epoch 493, train\n",
      " similarity:0.0840174\n",
      " penlog:-5.6186704\n",
      "Metrics Epoch 493, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:10.9000000\n",
      " batch_invalid_valency_nodes:11.4782609\n",
      " batch_nodes_0degree:2.5600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:94.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.8400000\n",
      " batch_node_degree:1.7060870\n",
      "Logits [35.118106842041016, 2.9393060207366943, 58.87516403198242]\n",
      "Epoch duration: 2.0332789421081543\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_493\n",
      "Epoch: 494\n",
      "FGW torch.Size([29508, 5]) 7.406267832266167e-05\n",
      "Penalty params: tau=0.80792 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=494 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 494, train\n",
      " fgw:0.2023845\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2023845\n",
      "Measure Epoch 494, train\n",
      " similarity:0.0842603\n",
      " penlog:-5.6397296\n",
      "Metrics Epoch 494, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:10.6200000\n",
      " batch_invalid_valency_nodes:10.6086957\n",
      " batch_nodes_0degree:2.3200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:94.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.5800000\n",
      " batch_node_degree:1.7286957\n",
      "Logits [35.2806396484375, 2.946443557739258, 58.94902420043945]\n",
      "Epoch duration: 1.9153072834014893\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 495\n",
      "FGW torch.Size([29508, 5]) 7.652710337424651e-05\n",
      "Penalty params: tau=0.80757 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=495 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 495, train\n",
      " fgw:0.2047103\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2047103\n",
      "Measure Epoch 495, train\n",
      " similarity:0.0856275\n",
      " penlog:-5.6929483\n",
      "Metrics Epoch 495, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:10.3200000\n",
      " batch_invalid_valency_nodes:9.8260870\n",
      " batch_nodes_0degree:2.1400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.2600000\n",
      " batch_node_degree:1.7530435\n",
      "Logits [35.34276580810547, 2.94803786277771, 59.203556060791016]\n",
      "Epoch duration: 1.9927616119384766\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_495\n",
      "Epoch: 496\n",
      "FGW torch.Size([29508, 5]) 7.539873331552371e-05\n",
      "Penalty params: tau=0.80722 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=496 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 496, train\n",
      " fgw:0.2009570\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.2009570\n",
      "Measure Epoch 496, train\n",
      " similarity:0.0835984\n",
      " penlog:-5.6725569\n",
      "Metrics Epoch 496, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:10.4800000\n",
      " batch_invalid_valency_nodes:10.6956522\n",
      " batch_nodes_0degree:2.3400000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:94.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.4200000\n",
      " batch_node_degree:1.7304348\n",
      "Logits [35.36491394042969, 2.9564547538757324, 59.22423553466797]\n",
      "Epoch duration: 1.9727895259857178\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_496\n",
      "Epoch: 497\n",
      "FGW torch.Size([29508, 5]) 7.283452578121796e-05\n",
      "Penalty params: tau=0.80688 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=497 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 497, train\n",
      " fgw:0.1957946\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.1957946\n",
      "Measure Epoch 497, train\n",
      " similarity:0.0793939\n",
      " penlog:-5.6324917\n",
      "Metrics Epoch 497, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:10.7000000\n",
      " batch_invalid_valency_nodes:10.7826087\n",
      " batch_nodes_0degree:2.3600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.6400000\n",
      " batch_node_degree:1.7200000\n",
      "Logits [35.40095901489258, 2.9675586223602295, 59.10102081298828]\n",
      "Epoch duration: 1.9213380813598633\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_497\n",
      "Epoch: 498\n",
      "FGW torch.Size([29508, 5]) 7.301422010641545e-05\n",
      "Penalty params: tau=0.80653 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=498 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 498, train\n",
      " fgw:0.1981068\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.1981068\n",
      "Measure Epoch 498, train\n",
      " similarity:0.0811553\n",
      " penlog:-5.6060076\n",
      "Metrics Epoch 498, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:10.7600000\n",
      " batch_invalid_valency_nodes:10.9565217\n",
      " batch_nodes_0degree:2.4200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.7000000\n",
      " batch_node_degree:1.7095652\n",
      "Logits [35.42900085449219, 2.971827983856201, 59.060028076171875]\n",
      "Epoch duration: 1.9270801544189453\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_498\n",
      "Epoch: 499\n",
      "FGW torch.Size([29508, 5]) 7.464570080628619e-05\n",
      "Penalty params: tau=0.80618 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=499 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 499, train\n",
      " fgw:0.1999860\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.1999860\n",
      "Measure Epoch 499, train\n",
      " similarity:0.0823113\n",
      " penlog:-5.5949913\n",
      "Metrics Epoch 499, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:94.0000000\n",
      " batch_connected_components:10.7000000\n",
      " batch_invalid_valency_nodes:10.9565217\n",
      " batch_nodes_0degree:2.4200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:94.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.6200000\n",
      " batch_node_degree:1.7078261\n",
      "Logits [35.46162796020508, 2.965426445007324, 59.219364166259766]\n",
      "Epoch duration: 2.046081781387329\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_499\n",
      "Epoch: 500\n",
      "FGW torch.Size([29508, 5]) 7.540068327216431e-05\n",
      "Penalty params: tau=0.80583 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=500 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 500, train\n",
      " fgw:0.1992364\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.1992364\n",
      "Measure Epoch 500, train\n",
      " similarity:0.0723967\n",
      " penlog:-11.5627717\n",
      "Metrics Epoch 500, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:88.0000000\n",
      " batch_connected_components:10.6200000\n",
      " batch_invalid_valency_nodes:11.1304348\n",
      " batch_nodes_0degree:2.4200000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:94.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.4800000\n",
      " batch_node_degree:1.7269565\n",
      "Logits [35.524959564208984, 2.9671683311462402, 59.34623718261719]\n",
      "Epoch duration: 2.254714250564575\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_500\n",
      "Epoch: 501\n",
      "FGW torch.Size([29508, 5]) 7.333289249800146e-05\n",
      "Penalty params: tau=0.80548 conn_l=1.00000 val_l=10.00000 euler_l=1.00000 epoch=501 mode=[0 0 0] conn=False euler=False val=False\n",
      "Losses Epoch 501, train\n",
      " fgw:0.1955993\n",
      " conn_penalty:0.0000000\n",
      " val_penalty:0.0000000\n",
      " euler_penalty:0.0000000\n",
      " total:0.1955993\n",
      "Measure Epoch 501, train\n",
      " similarity:0.0807353\n",
      " penlog:-9.5094668\n",
      "Metrics Epoch 501, train\n",
      " batch_molecular_validity:2.0000000\n",
      " batch_correctness:0.0000000\n",
      " batch_symbol_accuracy:100.0000000\n",
      " batch_molecular_disconnected_validity:90.0000000\n",
      " batch_connected_components:10.8000000\n",
      " batch_invalid_valency_nodes:11.2173913\n",
      " batch_nodes_0degree:2.4600000\n",
      " batch_nodes_7plus_degree:0.0000000\n",
      " invalid_euler_toofew:96.0000000\n",
      " invalid_euler_toomany:0.0000000\n",
      " avg_euler_error:-10.7200000\n",
      " batch_node_degree:1.7130435\n",
      "Logits [35.631874084472656, 2.9795145988464355, 59.37336349487305]\n",
      "Epoch duration: 1.965343713760376\n",
      "mol_opt/output_dev2/molemb-base-longer-softmax5/model_molemb-base-longer-softmax5_501\n"
     ]
    }
   ],
   "source": [
    "molopt, molopt_decoder = main(args, train_data_loader = train_data_loader, val_data_loader = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "molopt, molopt_decoder = main(args, train_data_loader = train_data_loader, val_data_loader = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "Python argument types in\n    rdkit.Chem.rdmolops.Kekulize(NoneType)\ndid not match C++ signature:\n    Kekulize(RDKit::ROMol {lvalue} mol, bool clearAromaticFlags=False)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-96795611958e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mMolGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mMolGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gitrepos/tum-thesis/otgnn/graph/mol_graph.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, smiles_list, kekulize)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkekulize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkekulize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_molecules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiles_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_mols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gitrepos/tum-thesis/otgnn/graph/mol_graph.py\u001b[0m in \u001b[0;36m_parse_molecules\u001b[0;34m(self, smiles_list)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mrd_mol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMolFromSmiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkekulize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                 \u001b[0mChem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKekulize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrd_mol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclearAromaticFlags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrd_mols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrd_mol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mArgumentError\u001b[0m: Python argument types in\n    rdkit.Chem.rdmolops.Kekulize(NoneType)\ndid not match C++ signature:\n    Kekulize(RDKit::ROMol {lvalue} mol, bool clearAromaticFlags=False)"
     ]
    }
   ],
   "source": [
    "for i in train_data_loader:\n",
    "    X = (MolGraph(i[0]))\n",
    "    Y = (MolGraph(i[1]))\n",
    "    break\n",
    "    \n",
    "x_embedding = molopt.forward(X)\n",
    "yhat_logits = molopt_decoder.forward(x_embedding, X, Y)\n",
    "yhat_labels = molopt_decoder.discretize_argmax(*yhat_logits)\n",
    "# yhat_labels = molopt_decoder.discretize(*yhat_logits)\n",
    "pred_pack = (yhat_labels, yhat_logits, Y.scope), Y \n",
    "\n",
    "target = Y.get_graph_outputs()\n",
    "symbols_labels, charges_labels, bonds_labels = yhat_labels\n",
    "symbols_logits, charges_logits, bonds_logits = yhat_logits\n",
    "\n",
    "from mol_opt.ot_utils import FGW \n",
    "fgw_loss = FGW(alpha = 0.5)\n",
    "fgw_loss(*pred_pack, tau = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.optim.Adam([torch.autograd.Variable(torch.Tensor([0.]))]).param_groups[0]['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9791483623609768"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.9 ** (1/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cuda': True,\n",
       " 'output_dir': 'mol_opt/output_dev1/pointwise-test5',\n",
       " 'tb_logs_dir': 'mol_opt/logs_dev1/pointwise-test5',\n",
       " 'init_model': 'pointwise-test5',\n",
       " 'init_decoder_model': 'pointwise-test5_decode',\n",
       " 'task': 'qed',\n",
       " 'model_type': 'pointwise',\n",
       " 'one_batch_train': True,\n",
       " 'batch_size': 50,\n",
       " 'pred_hidden': 150,\n",
       " 'pc_hidden': 100,\n",
       " 'ffn_activation': 'LeakyReLU',\n",
       " 'n_epochs': 1000,\n",
       " 'dim_tangent_space': 40,\n",
       " 'n_layers': 5,\n",
       " 'n_hidden': 250,\n",
       " 'n_ffn_hidden': 100,\n",
       " 'linear_out': False,\n",
       " 'dropout_gcn': 0.0,\n",
       " 'dropout_ffn': 0.0,\n",
       " 'agg_func': 'sum',\n",
       " 'batch_norm': False,\n",
       " 'N_transformer': 6,\n",
       " 'n_ffn_transformer': 100,\n",
       " 'n_heads_transformer': 10,\n",
       " 'dropout_transformer': 0.1,\n",
       " 'ot_solver': 'emd',\n",
       " 'sinkhorn_entropy': 0.1,\n",
       " 'sinkhorn_max_it': 10000,\n",
       " 'connectivity': False,\n",
       " 'valency': True,\n",
       " 'euler_characteristic_penalty': True,\n",
       " 'annealing_rate': 0.002,\n",
       " 'connectivity_lambda': 0,\n",
       " 'valency_lambda': 0,\n",
       " 'euler_lambda': 1,\n",
       " 'connectivity_hard': False,\n",
       " 'valency_hard': False,\n",
       " 'scale_lambdas': False,\n",
       " 'conn_penalty_function': 'logdet',\n",
       " 'penalty_gumbel': False,\n",
       " 'device': 'cuda:0',\n",
       " 'n_labels': 1}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter(\"/home/octav/gitrepos/tum-thesis/mol_opt/dev7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [\n",
    "    \"pointwise10-dev8/train_avg_euler_error/mean\",\n",
    "    \"pointwise10-dev8/train_avg_euler_error/bot_band\",\n",
    "    \"pointwise10-dev8/train_avg_euler_error/top_band\"\n",
    "]\n",
    "# writer.add_custom_scalars_marginchart(tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer.add_scalar(\"plm2/plm2\", 0, 1)\n",
    "# writer.add_scalar(\"plm2/plm2\", 1, 2)\n",
    "# writer.add_scalar(\"plm2/plm2\", 0, 3)\n",
    "# writer.add_scalar(\"plm2/plm2\", 1, 4)\n",
    "tags = [\"plm\", \"plm\", \"plm\"]\n",
    "# layout = {\"plm\" : {\"plm\" : [\"Margin\", tags]}}\n",
    "layout = {\"plm1\" : {\"plm\" : [\"Margin\", tags], \"plm2\": [\"Multiline\", tags]},\n",
    "          \"plm2\": {\"plm\" : [\"Margin\", tags], \"plm2\": [\"Multiline\", tags]}}\n",
    "writer.add_custom_scalars(layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "add_summary() missing 1 required positional argument: 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-5a2081fe54a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_file_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: add_summary() missing 1 required positional argument: 'summary'"
     ]
    }
   ],
   "source": [
    "writer._get_file_writer().add_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/octav/gitrepos/tum-thesis/mol_opt/dev6'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writer.logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorboardX\n",
    "tensorboardX.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX.summary import custom_scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer._get_file_writer().add_summary(custom_scalars(layout), global_step = 3)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_scalars('plm', {'plm0' : 0.0, 'plmt' : 0.1, 'plmb' : -0.1}, 3)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 1\n",
    "symbols_nll, charges_nll, bonds_nll = F.gumbel_softmax(tau = tau, dim=1, logits = symbols_logits), F.gumbel_softmax(tau=tau,dim=1,logits=charges_logits), F.gumbel_softmax(tau=tau,dim=1, logits = bonds_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(symbols_nll.mean(axis = 0).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(molopt.opt0.parameters())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "molopt.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = torch.load('mol_opt/output_pointwise10-onebatch/model_pointwise10-onebatch_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict[\"model\"]['opt0.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "molopt2 = MolOpt(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(molopt2.opt0.parameters())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "molopt2.load_state_dict(model_dict[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(molopt2.opt0.parameters())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "molopt3,args3 = load_model('mol_opt/output_pointwise10-onebatch/model_pointwise10-onebatch_8', MolOpt, args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(molopt3.opt0.parameters())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgw_loss(*pred_pack, tau = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
