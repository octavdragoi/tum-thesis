{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/octav/gitrepos/tum-thesis\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if \"ntbk\" in os.getcwd():\n",
    "    os.chdir(\"..\")\n",
    "print(os.getcwd())\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), \"otgnn\"))\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from otgnn.models import GCN\n",
    "from otgnn.graph import MolGraph\n",
    "from otgnn.utils import save_model, load_model\n",
    "\n",
    "from mol_opt.data_mol_opt import MolOptDataset\n",
    "from mol_opt.data_mol_opt import get_loader\n",
    "from mol_opt.arguments import get_args\n",
    "from mol_opt.train_mol_opt import main\n",
    "\n",
    "from rdkit.Chem import MolFromSmiles\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/octav/anaconda3/envs/mol_ot/lib/python36.zip',\n",
       " '/home/octav/anaconda3/envs/mol_ot/lib/python3.6',\n",
       " '/home/octav/anaconda3/envs/mol_ot/lib/python3.6/lib-dynload',\n",
       " '',\n",
       " '/home/octav/anaconda3/envs/mol_ot/lib/python3.6/site-packages',\n",
       " '/home/octav/anaconda3/envs/mol_ot/lib/python3.6/site-packages/IPython/extensions',\n",
       " '/home/octav/.ipython',\n",
       " '/home/octav/gitrepos/tum-thesis/otgnn',\n",
       " '/home/octav/gitrepos/tum-thesis/otgnn',\n",
       " '..']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = get_loader(\"iclr19-graph2graph/data/qed\", 48, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data_loader:\n",
    "    X = (MolGraph(i[0]))\n",
    "    Y = (MolGraph(i[1]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = [\"\", \"-cuda\"]\n",
    "args = get_args()\n",
    "args.output_dir = \"mol_opt/output/\"\n",
    "# args.device = \"cpu\"\n",
    "gcn = GCN(args).to(args.device)\n",
    "opt = torch.nn.Linear(50, 50).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn = GCN(args).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (W_message_i): Linear(in_features=100, out_features=50, bias=False)\n",
       "  (W_message_h): Linear(in_features=50, out_features=50, bias=False)\n",
       "  (W_message_o): Linear(in_features=143, out_features=50, bias=True)\n",
       "  (W_mol_h): Linear(in_features=50, out_features=100, bias=True)\n",
       "  (W_mol_o): Linear(in_features=100, out_features=1, bias=True)\n",
       "  (dropout_gcn): Dropout(p=0.0, inplace=False)\n",
       "  (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (W_message_i): Linear(in_features=100, out_features=50, bias=False)\n",
       "  (W_message_h): Linear(in_features=50, out_features=50, bias=False)\n",
       "  (W_message_o): Linear(in_features=143, out_features=50, bias=True)\n",
       "  (W_mol_h): Linear(in_features=50, out_features=100, bias=True)\n",
       "  (W_mol_o): Linear(in_features=100, out_features=1, bias=True)\n",
       "  (dropout_gcn): Dropout(p=0.0, inplace=False)\n",
       "  (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1046, -0.0445,  0.0017,  ..., -0.1556, -0.0815,  0.0778],\n",
       "        [-0.1311,  0.0809,  0.0801,  ..., -0.1227, -0.0604,  0.1382],\n",
       "        [-0.0255, -0.0365, -0.0803,  ..., -0.0684, -0.0055,  0.0874],\n",
       "        ...,\n",
       "        [-0.1094,  0.1190,  0.0357,  ..., -0.1559, -0.0649,  0.0951],\n",
       "        [-0.0505, -0.0995, -0.1213,  ..., -0.1367, -0.0411,  0.0652],\n",
       "        [-0.1108,  0.1159,  0.0458,  ..., -0.0898, -0.0071, -0.0642]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt(gcn.forward(Y)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1041, 50])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcn.forward(X)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  22   40   61   79  100  120  144  164  186  205  226  251  276  298\n",
      "  321  340  360  386  411  433  455  476  500  523  542  566  586  612\n",
      "  635  659  679  695  719  738  762  781  799  822  844  866  890  914\n",
      "  938  958  978  998 1021 1041]\n"
     ]
    }
   ],
   "source": [
    "print (np.cumsum(([len(x.atoms) for x in X.mols])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X.mols[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MolOpt(\n",
       "  (GCN): GCN(\n",
       "    (W_message_i): Linear(in_features=100, out_features=50, bias=False)\n",
       "    (W_message_h): Linear(in_features=50, out_features=50, bias=False)\n",
       "    (W_message_o): Linear(in_features=143, out_features=50, bias=True)\n",
       "    (W_mol_h): Linear(in_features=50, out_features=100, bias=True)\n",
       "    (W_mol_o): Linear(in_features=100, out_features=1, bias=True)\n",
       "    (dropout_gcn): Dropout(p=0.0, inplace=False)\n",
       "    (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (opt0): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (opt1): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (delta_loss): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mol_opt import mol_opt\n",
    "molopt = mol_opt.MolOpt(args).to(device = args.device)\n",
    "molopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1987,  0.1258,  0.0032,  ..., -0.2445, -0.1795,  0.0221],\n",
       "        [-0.0593,  0.2086,  0.1284,  ..., -0.2243,  0.0441, -0.2639],\n",
       "        [-0.0183,  0.2303,  0.1695,  ..., -0.2584,  0.0225, -0.2690],\n",
       "        ...,\n",
       "        [-0.0792, -0.0209, -0.0698,  ..., -0.0574, -0.1829, -0.0780],\n",
       "        [-0.1281,  0.1095, -0.1443,  ..., -0.1179, -0.0484, -0.2110],\n",
       "        [ 0.0685,  0.2574,  0.2800,  ..., -0.0878, -0.1874, -0.1848]],\n",
       "       device='cuda:0', grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "molopt.align(molopt.encode(X), X, molopt.encode(Y), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1987,  0.1258,  0.0032,  ..., -0.2445, -0.1795,  0.0221],\n",
       "        [-0.0575,  0.1032,  0.1953,  ..., -0.0809, -0.1316, -0.2862],\n",
       "        [-0.1642,  0.1110, -0.1592,  ..., -0.0955, -0.0516, -0.2344],\n",
       "        ...,\n",
       "        [-0.1316,  0.0775,  0.1669,  ..., -0.1240, -0.0673, -0.1592],\n",
       "        [-0.0792, -0.0209, -0.0698,  ..., -0.0574, -0.1829, -0.0780],\n",
       "        [ 0.0685,  0.2574,  0.2800,  ..., -0.0878, -0.1874, -0.1848]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "molopt.encode(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0141, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "molopt.forward_train(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: mol_opt/output//model_test\n"
     ]
    }
   ],
   "source": [
    "save_model(molopt, args, args.output_dir, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, loss: 0.015534802339971066\n",
      "Iter: 1, loss: 0.015233045443892479\n",
      "Iter: 2, loss: 0.014541471377015114\n",
      "Iter: 3, loss: 0.014296744018793106\n",
      "Iter: 4, loss: 0.014862217009067535\n",
      "Iter: 5, loss: 0.014735095202922821\n",
      "Iter: 6, loss: 0.014787458814680576\n",
      "Iter: 7, loss: 0.0148368114605546\n",
      "Iter: 8, loss: 0.014837524853646755\n",
      "Iter: 9, loss: 0.014857822097837925\n",
      "Iter: 10, loss: 0.014195257797837257\n",
      "Iter: 11, loss: 0.01448412798345089\n",
      "Iter: 12, loss: 0.01481739804148674\n",
      "Iter: 13, loss: 0.01585579290986061\n",
      "Iter: 14, loss: 0.015088125132024288\n",
      "Iter: 15, loss: 0.014189892448484898\n",
      "Iter: 16, loss: 0.014590233564376831\n",
      "Iter: 17, loss: 0.014030560851097107\n",
      "Iter: 18, loss: 0.014501072466373444\n",
      "Iter: 19, loss: 0.013706115074455738\n",
      "Iter: 20, loss: 0.014175619930028915\n",
      "Iter: 21, loss: 0.014129242859780788\n",
      "Iter: 22, loss: 0.013829604722559452\n",
      "Iter: 23, loss: 0.013918042182922363\n",
      "Iter: 24, loss: 0.01380153838545084\n",
      "Iter: 25, loss: 0.014579208567738533\n",
      "Iter: 26, loss: 0.014408345334231853\n",
      "Iter: 27, loss: 0.013757500797510147\n",
      "Iter: 28, loss: 0.014586374163627625\n",
      "Iter: 29, loss: 0.014030367136001587\n",
      "Iter: 30, loss: 0.014972575008869171\n",
      "Iter: 31, loss: 0.013838625513017178\n",
      "Iter: 32, loss: 0.014057839289307594\n",
      "Iter: 33, loss: 0.014723531901836395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/octav/anaconda3/envs/mol_ot/lib/python3.6/site-packages/ot/lp/__init__.py:276: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 34, loss: 0.014029206708073616\n",
      "Iter: 35, loss: 0.013968406245112419\n",
      "Iter: 36, loss: 0.014084894210100174\n",
      "Iter: 37, loss: 0.014040748588740826\n",
      "Iter: 38, loss: 0.014291472733020782\n",
      "Iter: 39, loss: 0.014438503421843052\n",
      "Iter: 40, loss: 0.01469825953245163\n",
      "Iter: 41, loss: 0.013522089459002018\n",
      "Iter: 42, loss: 0.013808383606374264\n",
      "Iter: 43, loss: 0.014365884475409985\n",
      "Iter: 44, loss: 0.013690410181879997\n",
      "Iter: 45, loss: 0.01380215585231781\n",
      "Iter: 46, loss: 0.013775057159364223\n",
      "Iter: 47, loss: 0.013525346294045448\n",
      "Iter: 48, loss: 0.013605396263301373\n",
      "Iter: 49, loss: 0.013963649980723858\n",
      "Iter: 50, loss: 0.013937383890151978\n",
      "Iter: 51, loss: 0.013569599017500877\n",
      "Iter: 52, loss: 0.013187862001359463\n",
      "Iter: 53, loss: 0.013646628707647324\n",
      "Iter: 54, loss: 0.013621378690004349\n",
      "Iter: 55, loss: 0.013175789266824722\n",
      "Iter: 56, loss: 0.013652865774929523\n",
      "Iter: 57, loss: 0.01333098765462637\n",
      "Iter: 58, loss: 0.013325710780918598\n",
      "Iter: 59, loss: 0.013774197548627853\n",
      "Iter: 60, loss: 0.013378186151385307\n",
      "Iter: 61, loss: 0.013967080041766167\n",
      "Iter: 62, loss: 0.013819216750562191\n",
      "Iter: 63, loss: 0.013878257013857365\n",
      "Iter: 64, loss: 0.013549194671213627\n",
      "Iter: 65, loss: 0.013185909017920494\n",
      "Iter: 66, loss: 0.013645118102431297\n",
      "Iter: 67, loss: 0.013439894653856754\n",
      "Iter: 68, loss: 0.013092362321913242\n",
      "Iter: 69, loss: 0.013488792814314365\n",
      "Iter: 70, loss: 0.013175700791180134\n",
      "Iter: 71, loss: 0.013136427849531174\n",
      "Iter: 72, loss: 0.013679327443242073\n",
      "Iter: 73, loss: 0.013437462039291859\n",
      "Iter: 74, loss: 0.013416965492069721\n",
      "Iter: 75, loss: 0.013332183472812176\n",
      "Iter: 76, loss: 0.013342847116291523\n",
      "Iter: 77, loss: 0.012805301696062088\n",
      "Iter: 78, loss: 0.013922800309956074\n",
      "Iter: 79, loss: 0.013392427936196327\n",
      "Iter: 80, loss: 0.013061540201306343\n",
      "Iter: 81, loss: 0.013268196024000645\n",
      "Iter: 82, loss: 0.0130225894972682\n",
      "Iter: 83, loss: 0.013009865768253803\n",
      "Iter: 84, loss: 0.013326502405107021\n",
      "Iter: 85, loss: 0.013354449532926083\n",
      "Iter: 86, loss: 0.013951610773801804\n",
      "Iter: 87, loss: 0.012694334611296654\n",
      "Iter: 88, loss: 0.013131109066307545\n",
      "Iter: 89, loss: 0.013477041386067867\n",
      "Iter: 90, loss: 0.01290386263281107\n",
      "Iter: 91, loss: 0.01305649895220995\n",
      "Iter: 92, loss: 0.013091722503304482\n",
      "Iter: 93, loss: 0.012810495682060719\n",
      "Iter: 94, loss: 0.013209645636379719\n",
      "Iter: 95, loss: 0.012732145376503468\n",
      "Iter: 96, loss: 0.01311113964766264\n",
      "Iter: 97, loss: 0.012965523637831211\n",
      "Iter: 98, loss: 0.013695057481527328\n",
      "Iter: 99, loss: 0.013150534592568874\n",
      "Iter: 100, loss: 0.012929410673677921\n",
      "Iter: 101, loss: 0.013197200372815132\n",
      "Iter: 102, loss: 0.012924632988870144\n",
      "Iter: 103, loss: 0.012940888293087482\n",
      "Iter: 104, loss: 0.013044989667832851\n",
      "Iter: 105, loss: 0.012706185691058636\n",
      "Iter: 106, loss: 0.012751391157507896\n",
      "Iter: 107, loss: 0.012302065268158913\n",
      "Iter: 108, loss: 0.01226859726011753\n",
      "Iter: 109, loss: 0.01288294605910778\n",
      "Iter: 110, loss: 0.01291897613555193\n",
      "Iter: 111, loss: 0.013150287792086601\n",
      "Iter: 112, loss: 0.012742201797664165\n",
      "Iter: 113, loss: 0.012524046003818512\n",
      "Iter: 114, loss: 0.012817999348044395\n",
      "Iter: 115, loss: 0.01327043678611517\n",
      "Iter: 116, loss: 0.012598437257111073\n",
      "Iter: 117, loss: 0.013395052403211594\n",
      "Iter: 118, loss: 0.012979235500097275\n",
      "Iter: 119, loss: 0.012825231067836285\n",
      "Iter: 120, loss: 0.012201842851936817\n",
      "Iter: 121, loss: 0.012268640100955963\n",
      "Iter: 122, loss: 0.012919865548610687\n",
      "Iter: 123, loss: 0.012532426044344902\n",
      "Iter: 124, loss: 0.012841210700571537\n",
      "29.537330150604248\n",
      "Iter: 0, loss: 0.013314880430698395\n",
      "Iter: 1, loss: 0.013072304427623749\n",
      "Iter: 2, loss: 0.012425108812749386\n",
      "Iter: 3, loss: 0.012226542457938194\n",
      "Iter: 4, loss: 0.012742315419018269\n",
      "Iter: 5, loss: 0.012629089877009392\n",
      "Iter: 6, loss: 0.012709146365523338\n",
      "Iter: 7, loss: 0.012753823772072792\n",
      "Iter: 8, loss: 0.012764287181198597\n",
      "Iter: 9, loss: 0.012713998556137085\n",
      "Iter: 10, loss: 0.012158598750829697\n",
      "Iter: 11, loss: 0.012460031546652317\n",
      "Iter: 12, loss: 0.012747890315949917\n",
      "Iter: 13, loss: 0.013656259514391422\n",
      "Iter: 14, loss: 0.012969165109097958\n",
      "Iter: 15, loss: 0.012183734215795994\n",
      "Iter: 16, loss: 0.012538234703242779\n",
      "Iter: 17, loss: 0.012031381018459797\n",
      "Iter: 18, loss: 0.012443693354725838\n",
      "Iter: 19, loss: 0.011741459369659424\n",
      "Iter: 20, loss: 0.012172562070190907\n",
      "Iter: 21, loss: 0.012155761942267418\n",
      "Iter: 22, loss: 0.011884421110153198\n",
      "Iter: 23, loss: 0.011965585872530937\n",
      "Iter: 24, loss: 0.01185271330177784\n",
      "Iter: 25, loss: 0.01257330272346735\n",
      "Iter: 26, loss: 0.012399480678141117\n",
      "Iter: 27, loss: 0.01184318121522665\n",
      "Iter: 28, loss: 0.012596805579960346\n",
      "Iter: 29, loss: 0.012090994045138359\n",
      "Iter: 30, loss: 0.012908052653074265\n",
      "Iter: 31, loss: 0.011919578537344933\n",
      "Iter: 32, loss: 0.012113019824028015\n",
      "Iter: 33, loss: 0.012709660455584526\n",
      "Iter: 34, loss: 0.012093188241124153\n",
      "Iter: 35, loss: 0.012045026756823063\n",
      "Iter: 36, loss: 0.012131157331168652\n",
      "Iter: 37, loss: 0.012106908485293388\n",
      "Iter: 38, loss: 0.012340560555458069\n",
      "Iter: 39, loss: 0.01249023899435997\n",
      "Iter: 40, loss: 0.012681416235864162\n",
      "Iter: 41, loss: 0.011644751764833927\n",
      "Iter: 42, loss: 0.011913170106709003\n",
      "Iter: 43, loss: 0.012414944358170033\n",
      "Iter: 44, loss: 0.011810775846242905\n",
      "Iter: 45, loss: 0.011921463534235954\n",
      "Iter: 46, loss: 0.011929511092603207\n",
      "Iter: 47, loss: 0.011690777726471424\n",
      "Iter: 48, loss: 0.011735009029507637\n",
      "Iter: 49, loss: 0.01206178031861782\n",
      "Iter: 50, loss: 0.012021519243717194\n",
      "Iter: 51, loss: 0.011763849295675755\n",
      "Iter: 52, loss: 0.01140161044895649\n",
      "Iter: 53, loss: 0.011802662163972855\n",
      "Iter: 54, loss: 0.011788253672420979\n",
      "Iter: 55, loss: 0.011401958763599396\n",
      "Iter: 56, loss: 0.011835065670311451\n",
      "Iter: 57, loss: 0.011510658077895641\n",
      "Iter: 58, loss: 0.011539905332028866\n",
      "Iter: 59, loss: 0.011929831467568874\n",
      "Iter: 60, loss: 0.011599373072385788\n",
      "Iter: 61, loss: 0.012119518592953682\n",
      "Iter: 62, loss: 0.011971121653914452\n",
      "Iter: 63, loss: 0.012051412835717201\n",
      "Iter: 64, loss: 0.011755188927054405\n",
      "Iter: 65, loss: 0.011448003351688385\n",
      "Iter: 66, loss: 0.011816784739494324\n",
      "Iter: 67, loss: 0.011647269129753113\n",
      "Iter: 68, loss: 0.011348815634846687\n",
      "Iter: 69, loss: 0.011701228097081184\n",
      "Iter: 70, loss: 0.011422358453273773\n",
      "Iter: 71, loss: 0.011390525847673416\n",
      "Iter: 72, loss: 0.011878710240125656\n",
      "Iter: 73, loss: 0.011663753539323807\n",
      "Iter: 74, loss: 0.01164185255765915\n",
      "Iter: 75, loss: 0.011557336896657944\n",
      "Iter: 76, loss: 0.011599934659898281\n",
      "Iter: 77, loss: 0.011114384047687054\n",
      "Iter: 78, loss: 0.01211998425424099\n",
      "Iter: 79, loss: 0.011645146645605564\n",
      "Iter: 80, loss: 0.011326998472213745\n",
      "Iter: 81, loss: 0.011548752896487713\n",
      "Iter: 82, loss: 0.011325119994580746\n",
      "Iter: 83, loss: 0.01129138097167015\n",
      "Iter: 84, loss: 0.011611188761889935\n",
      "Iter: 85, loss: 0.011603065766394138\n",
      "Iter: 86, loss: 0.012159885838627815\n",
      "Iter: 87, loss: 0.011043562553822994\n",
      "Iter: 88, loss: 0.011423371732234955\n",
      "Iter: 89, loss: 0.011760825291275978\n",
      "Iter: 90, loss: 0.011246079578995705\n",
      "Iter: 91, loss: 0.011364420875906944\n",
      "Iter: 92, loss: 0.011403114534914494\n",
      "Iter: 93, loss: 0.01116891484707594\n",
      "Iter: 94, loss: 0.011508312076330185\n",
      "Iter: 95, loss: 0.011088106781244278\n",
      "Iter: 96, loss: 0.011393163353204727\n",
      "Iter: 97, loss: 0.01128936093300581\n",
      "Iter: 98, loss: 0.011958283372223377\n",
      "Iter: 99, loss: 0.01149334292858839\n",
      "Iter: 100, loss: 0.011270086281001568\n",
      "Iter: 101, loss: 0.011518120765686035\n",
      "Iter: 102, loss: 0.011262554675340652\n",
      "Iter: 103, loss: 0.01129167154431343\n",
      "Iter: 104, loss: 0.01140816044062376\n",
      "Iter: 105, loss: 0.01107370387762785\n",
      "Iter: 106, loss: 0.011119581758975983\n",
      "Iter: 107, loss: 0.01071028970181942\n",
      "Iter: 108, loss: 0.010692915879189968\n",
      "Iter: 109, loss: 0.01124150212854147\n",
      "Iter: 110, loss: 0.011276218108832836\n",
      "Iter: 111, loss: 0.011487564072012901\n",
      "Iter: 112, loss: 0.011135484091937542\n",
      "Iter: 113, loss: 0.010949484072625637\n",
      "Iter: 114, loss: 0.011207520961761475\n",
      "Iter: 115, loss: 0.011620252393186092\n",
      "Iter: 116, loss: 0.011009725742042065\n",
      "Iter: 117, loss: 0.01173393428325653\n",
      "Iter: 118, loss: 0.01136074960231781\n",
      "Iter: 119, loss: 0.011219275183975697\n",
      "Iter: 120, loss: 0.010645907372236252\n",
      "Iter: 121, loss: 0.010703658685088158\n",
      "Iter: 122, loss: 0.011349334381520748\n",
      "Iter: 123, loss: 0.010967321693897247\n",
      "Iter: 124, loss: 0.011241091415286064\n",
      "29.446598529815674\n",
      "Iter: 0, loss: 0.011667764745652676\n",
      "Iter: 1, loss: 0.011460945941507816\n",
      "Iter: 2, loss: 0.010861878283321857\n",
      "Iter: 3, loss: 0.01068966556340456\n",
      "Iter: 4, loss: 0.0111648915335536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 5, loss: 0.011062366887927055\n",
      "Iter: 6, loss: 0.011161036789417267\n",
      "Iter: 7, loss: 0.011199872009456158\n",
      "Iter: 8, loss: 0.01121568400412798\n",
      "Iter: 9, loss: 0.011119906790554523\n",
      "Iter: 10, loss: 0.010642554610967636\n",
      "Iter: 11, loss: 0.010950086638331413\n",
      "Iter: 12, loss: 0.011198685504496098\n",
      "Iter: 13, loss: 0.012002616189420223\n",
      "Iter: 14, loss: 0.011378305032849312\n",
      "Iter: 15, loss: 0.010684579610824585\n",
      "Iter: 16, loss: 0.010998154990375042\n",
      "Iter: 17, loss: 0.010535567998886108\n",
      "Iter: 18, loss: 0.010907185263931751\n",
      "Iter: 19, loss: 0.010277491994202137\n",
      "Iter: 20, loss: 0.010675954632461071\n",
      "Iter: 21, loss: 0.010673644952476025\n",
      "Iter: 22, loss: 0.010425758548080921\n",
      "Iter: 23, loss: 0.010501904413104057\n",
      "Iter: 24, loss: 0.010392596945166588\n",
      "Iter: 25, loss: 0.011063323356211185\n",
      "Iter: 26, loss: 0.010888702236115932\n",
      "Iter: 27, loss: 0.01040828786790371\n",
      "Iter: 28, loss: 0.011097054928541183\n",
      "Iter: 29, loss: 0.010628379881381989\n",
      "Iter: 30, loss: 0.011344282887876034\n",
      "Iter: 31, loss: 0.010476179420948029\n",
      "Iter: 32, loss: 0.010651170276105404\n",
      "Iter: 33, loss: 0.011188444681465626\n",
      "Iter: 34, loss: 0.010629061609506607\n",
      "Iter: 35, loss: 0.010597018525004387\n",
      "Iter: 36, loss: 0.01066096592694521\n",
      "Iter: 37, loss: 0.010648626834154129\n",
      "Iter: 38, loss: 0.010862833820283413\n",
      "Iter: 39, loss: 0.011015648953616619\n",
      "Iter: 40, loss: 0.011154345236718655\n",
      "Iter: 41, loss: 0.010229545645415783\n",
      "Iter: 42, loss: 0.010479368269443512\n",
      "Iter: 43, loss: 0.010930841788649559\n",
      "Iter: 44, loss: 0.010392403230071068\n",
      "Iter: 45, loss: 0.010490423999726772\n",
      "Iter: 46, loss: 0.010525756515562534\n",
      "Iter: 47, loss: 0.010301548056304455\n",
      "Iter: 48, loss: 0.010320812463760376\n",
      "Iter: 49, loss: 0.010620756074786186\n",
      "Iter: 50, loss: 0.010565743781626225\n",
      "Iter: 51, loss: 0.010383694432675838\n",
      "Iter: 52, loss: 0.010044017806649208\n",
      "Iter: 53, loss: 0.010402665473520756\n",
      "Iter: 54, loss: 0.010395064018666744\n",
      "Iter: 55, loss: 0.010052090510725975\n",
      "Iter: 56, loss: 0.010448195040225983\n",
      "Iter: 57, loss: 0.010127217508852482\n",
      "Iter: 58, loss: 0.010172169655561447\n",
      "Iter: 59, loss: 0.010522423312067986\n",
      "Iter: 60, loss: 0.010242179036140442\n",
      "Iter: 61, loss: 0.01070042047649622\n",
      "Iter: 62, loss: 0.010553056374192238\n",
      "Iter: 63, loss: 0.010649528354406357\n",
      "Iter: 64, loss: 0.010387469083070755\n",
      "Iter: 65, loss: 0.010120544582605362\n",
      "Iter: 66, loss: 0.010422086343169212\n",
      "Iter: 67, loss: 0.010274081490933895\n",
      "Iter: 68, loss: 0.010013371706008911\n",
      "Iter: 69, loss: 0.010328566655516624\n",
      "Iter: 70, loss: 0.010073760524392128\n",
      "Iter: 71, loss: 0.010047375224530697\n",
      "Iter: 72, loss: 0.010494791902601719\n",
      "Iter: 73, loss: 0.010300103574991226\n",
      "Iter: 74, loss: 0.010273915715515614\n",
      "Iter: 75, loss: 0.010197915136814117\n",
      "Iter: 76, loss: 0.010256855748593807\n",
      "Iter: 77, loss: 0.009818772785365582\n",
      "Iter: 78, loss: 0.010727137327194214\n",
      "Iter: 79, loss: 0.010296415537595749\n",
      "Iter: 80, loss: 0.009991670958697796\n",
      "Iter: 81, loss: 0.01022023893892765\n",
      "Iter: 82, loss: 0.010018112137913704\n",
      "Iter: 83, loss: 0.00996632780879736\n",
      "Iter: 84, loss: 0.010290625505149364\n",
      "Iter: 85, loss: 0.010249576531350613\n",
      "Iter: 86, loss: 0.010774509981274605\n",
      "Iter: 87, loss: 0.009768865071237087\n",
      "Iter: 88, loss: 0.010106134228408337\n",
      "Iter: 89, loss: 0.010428542271256447\n",
      "Iter: 90, loss: 0.009965385310351849\n",
      "Iter: 91, loss: 0.010054094716906548\n",
      "Iter: 92, loss: 0.01010038610547781\n",
      "Iter: 93, loss: 0.00989636778831482\n",
      "Iter: 94, loss: 0.010188418440520763\n",
      "Iter: 95, loss: 0.009816889651119709\n",
      "Iter: 96, loss: 0.010063408873975277\n",
      "Iter: 97, loss: 0.009988554753363132\n",
      "Iter: 98, loss: 0.010609881952404976\n",
      "Iter: 99, loss: 0.010209561325609684\n",
      "Iter: 100, loss: 0.009977388195693493\n",
      "Iter: 101, loss: 0.010211776942014694\n",
      "Iter: 102, loss: 0.009973909705877304\n",
      "Iter: 103, loss: 0.01001246739178896\n",
      "Iter: 104, loss: 0.010130491107702255\n",
      "Iter: 105, loss: 0.009803611785173416\n",
      "Iter: 106, loss: 0.009846883825957775\n",
      "Iter: 107, loss: 0.009474975988268852\n",
      "Iter: 108, loss: 0.009464679285883904\n",
      "Iter: 109, loss: 0.009964065626263618\n",
      "Iter: 110, loss: 0.009992717765271664\n",
      "Iter: 111, loss: 0.010191707871854305\n",
      "Iter: 112, loss: 0.009876231662929058\n",
      "Iter: 113, loss: 0.009720338508486748\n",
      "Iter: 114, loss: 0.009946377016603947\n",
      "Iter: 115, loss: 0.01033253688365221\n",
      "Iter: 116, loss: 0.009766650386154652\n",
      "Iter: 117, loss: 0.010431382805109024\n",
      "Iter: 118, loss: 0.010092190466821194\n",
      "Iter: 119, loss: 0.009966534562408924\n",
      "Iter: 120, loss: 0.009434216655790806\n",
      "Iter: 121, loss: 0.009483857080340385\n",
      "Iter: 122, loss: 0.010118857026100159\n",
      "Iter: 123, loss: 0.009743839502334595\n",
      "Iter: 124, loss: 0.009989193640649319\n",
      "29.29327630996704\n",
      "Iter: 0, loss: 0.010371473617851734\n",
      "Iter: 1, loss: 0.010188944637775421\n",
      "Iter: 2, loss: 0.009635228663682938\n",
      "Iter: 3, loss: 0.009484739042818546\n",
      "Iter: 4, loss: 0.009919742122292519\n",
      "Iter: 5, loss: 0.009832247160375118\n",
      "Iter: 6, loss: 0.00994191039353609\n",
      "Iter: 7, loss: 0.009978990070521832\n",
      "Iter: 8, loss: 0.0100009236484766\n",
      "Iter: 9, loss: 0.009866639040410519\n",
      "Iter: 10, loss: 0.009455544874072075\n",
      "Iter: 11, loss: 0.009758294560015202\n",
      "Iter: 12, loss: 0.009975739754736423\n",
      "Iter: 13, loss: 0.01069493405520916\n",
      "Iter: 14, loss: 0.010121027007699013\n",
      "Iter: 15, loss: 0.009500079788267612\n",
      "Iter: 16, loss: 0.009786417707800865\n",
      "Iter: 17, loss: 0.009354590438306332\n",
      "Iter: 18, loss: 0.009693102911114693\n",
      "Iter: 19, loss: 0.00912026222795248\n",
      "Iter: 20, loss: 0.009495128877460957\n",
      "Iter: 21, loss: 0.009505455382168293\n",
      "Iter: 22, loss: 0.009276050142943859\n",
      "Iter: 23, loss: 0.009345192462205887\n",
      "Iter: 24, loss: 0.00924364011734724\n",
      "Iter: 25, loss: 0.00986273493617773\n",
      "Iter: 26, loss: 0.009693327359855175\n",
      "Iter: 27, loss: 0.009277323260903358\n",
      "Iter: 28, loss: 0.009908698499202728\n",
      "Iter: 29, loss: 0.009469646029174328\n",
      "Iter: 30, loss: 0.010103556327521801\n",
      "Iter: 31, loss: 0.009336178191006184\n",
      "Iter: 32, loss: 0.009492945857346058\n",
      "Iter: 33, loss: 0.009978214278817177\n",
      "Iter: 34, loss: 0.009465750306844711\n",
      "Iter: 35, loss: 0.009444743394851685\n",
      "Iter: 36, loss: 0.009496851824223995\n",
      "Iter: 37, loss: 0.009492343291640282\n",
      "Iter: 38, loss: 0.009686349891126156\n",
      "Iter: 39, loss: 0.009842488914728165\n",
      "Iter: 40, loss: 0.009942649863660336\n",
      "Iter: 41, loss: 0.0091019868850708\n",
      "Iter: 42, loss: 0.009338763542473316\n",
      "Iter: 43, loss: 0.00974807795137167\n",
      "Iter: 44, loss: 0.009267347864806652\n",
      "Iter: 45, loss: 0.009351657703518867\n",
      "Iter: 46, loss: 0.009413403458893299\n",
      "Iter: 47, loss: 0.009195818565785885\n",
      "Iter: 48, loss: 0.009197277948260307\n",
      "Iter: 49, loss: 0.009471569210290909\n",
      "Iter: 50, loss: 0.009408380836248398\n",
      "Iter: 51, loss: 0.009286561980843544\n",
      "Iter: 52, loss: 0.008962288498878479\n",
      "Iter: 53, loss: 0.009291836060583591\n",
      "Iter: 54, loss: 0.009282526560127735\n",
      "Iter: 55, loss: 0.008977053686976433\n",
      "Iter: 56, loss: 0.009341159835457802\n",
      "Iter: 57, loss: 0.009023759514093399\n",
      "Iter: 58, loss: 0.009081550873816013\n",
      "Iter: 59, loss: 0.00939715001732111\n",
      "Iter: 60, loss: 0.009157619439065456\n",
      "Iter: 61, loss: 0.009567983448505402\n",
      "Iter: 62, loss: 0.00941977184265852\n",
      "Iter: 63, loss: 0.009524518623948097\n",
      "Iter: 64, loss: 0.009289608336985111\n",
      "Iter: 65, loss: 0.009058235213160515\n",
      "Iter: 66, loss: 0.009305517189204693\n",
      "Iter: 67, loss: 0.009176819585263729\n",
      "Iter: 68, loss: 0.008945056237280369\n",
      "Iter: 69, loss: 0.009228195995092392\n",
      "Iter: 70, loss: 0.008995162323117256\n",
      "Iter: 71, loss: 0.008968501351773739\n",
      "Iter: 72, loss: 0.009385326877236366\n",
      "Iter: 73, loss: 0.009202869608998299\n",
      "Iter: 74, loss: 0.009178762324154377\n",
      "Iter: 75, loss: 0.009112196043133736\n",
      "Iter: 76, loss: 0.009176941588521004\n",
      "Iter: 77, loss: 0.008779761381447315\n",
      "Iter: 78, loss: 0.009609107859432697\n",
      "Iter: 79, loss: 0.00921250693500042\n",
      "Iter: 80, loss: 0.008922142907977104\n",
      "Iter: 81, loss: 0.009153042919933796\n",
      "Iter: 82, loss: 0.00896687712520361\n",
      "Iter: 83, loss: 0.00889440905302763\n",
      "Iter: 84, loss: 0.009227767586708069\n",
      "Iter: 85, loss: 0.00916348397731781\n",
      "Iter: 86, loss: 0.009655955247581005\n",
      "Iter: 87, loss: 0.008742458187043667\n",
      "Iter: 88, loss: 0.009048267267644405\n",
      "Iter: 89, loss: 0.009349824860692024\n",
      "Iter: 90, loss: 0.00893372017890215\n",
      "Iter: 91, loss: 0.008997487835586071\n",
      "Iter: 92, loss: 0.009052049368619919\n",
      "Iter: 93, loss: 0.008869816549122334\n",
      "Iter: 94, loss: 0.009126110002398491\n",
      "Iter: 95, loss: 0.008793085813522339\n",
      "Iter: 96, loss: 0.008992972783744335\n",
      "Iter: 97, loss: 0.008936102502048016\n",
      "Iter: 98, loss: 0.00951679889112711\n",
      "Iter: 99, loss: 0.00917506031692028\n",
      "Iter: 100, loss: 0.00893212016671896\n",
      "Iter: 101, loss: 0.009156395681202412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 102, loss: 0.0089333551004529\n",
      "Iter: 103, loss: 0.008979018777608871\n",
      "Iter: 104, loss: 0.009097672067582607\n",
      "Iter: 105, loss: 0.008776803500950336\n",
      "Iter: 106, loss: 0.00881857518106699\n",
      "Iter: 107, loss: 0.008477690629661083\n",
      "Iter: 108, loss: 0.008470491506159306\n",
      "Iter: 109, loss: 0.008933167904615402\n",
      "Iter: 110, loss: 0.008951665833592415\n",
      "Iter: 111, loss: 0.00914007518440485\n",
      "Iter: 112, loss: 0.008857402950525284\n",
      "Iter: 113, loss: 0.008729237131774426\n",
      "Iter: 114, loss: 0.008920636959373951\n",
      "Iter: 115, loss: 0.009286978282034397\n",
      "Iter: 116, loss: 0.008761647157371044\n",
      "Iter: 117, loss: 0.00937346089631319\n",
      "Iter: 118, loss: 0.009057339280843735\n",
      "Iter: 119, loss: 0.008951058611273766\n",
      "Iter: 120, loss: 0.008448668755590916\n",
      "Iter: 121, loss: 0.008493450470268726\n",
      "Iter: 122, loss: 0.009120439179241657\n",
      "Iter: 123, loss: 0.008751928806304932\n",
      "Iter: 124, loss: 0.00897366926074028\n",
      "29.838321924209595\n",
      "Iter: 0, loss: 0.009316830895841122\n",
      "Iter: 1, loss: 0.009152702987194061\n",
      "Iter: 2, loss: 0.008635593578219414\n",
      "Iter: 3, loss: 0.008508560247719288\n",
      "Iter: 4, loss: 0.00891126599162817\n",
      "Iter: 5, loss: 0.008829684928059578\n",
      "Iter: 6, loss: 0.008945103734731674\n",
      "Iter: 7, loss: 0.008985408581793308\n",
      "Iter: 8, loss: 0.009009350091218948\n",
      "Iter: 9, loss: 0.008846933953464031\n",
      "Iter: 10, loss: 0.008492414839565754\n",
      "Iter: 11, loss: 0.008785629644989967\n",
      "Iter: 12, loss: 0.008977428078651428\n",
      "Iter: 13, loss: 0.009628365747630596\n",
      "Iter: 14, loss: 0.009097907692193985\n",
      "Iter: 15, loss: 0.008532878011465073\n",
      "Iter: 16, loss: 0.00879878830164671\n",
      "Iter: 17, loss: 0.008397297002375126\n",
      "Iter: 18, loss: 0.008702792227268219\n",
      "Iter: 19, loss: 0.008174949325621128\n",
      "Iter: 20, loss: 0.008527033030986786\n",
      "Iter: 21, loss: 0.00855162087827921\n",
      "Iter: 22, loss: 0.008337544277310371\n",
      "Iter: 23, loss: 0.00839995127171278\n",
      "Iter: 24, loss: 0.008304189890623093\n",
      "Iter: 25, loss: 0.008880122564733028\n",
      "Iter: 26, loss: 0.008713947609066963\n",
      "Iter: 27, loss: 0.008352432399988174\n",
      "Iter: 28, loss: 0.00893036276102066\n",
      "Iter: 29, loss: 0.008525214157998562\n",
      "Iter: 30, loss: 0.00908681657165289\n",
      "Iter: 31, loss: 0.008403888903558254\n",
      "Iter: 32, loss: 0.008543004281818867\n",
      "Iter: 33, loss: 0.008988572284579277\n",
      "Iter: 34, loss: 0.008513711392879486\n",
      "Iter: 35, loss: 0.008503813296556473\n",
      "Iter: 36, loss: 0.008544988930225372\n",
      "Iter: 37, loss: 0.008544067852199078\n",
      "Iter: 38, loss: 0.008723342791199684\n",
      "Iter: 39, loss: 0.008882912807166576\n",
      "Iter: 40, loss: 0.008948694914579391\n",
      "Iter: 41, loss: 0.00817838218063116\n",
      "Iter: 42, loss: 0.008404916152358055\n",
      "Iter: 43, loss: 0.008777182549238205\n",
      "Iter: 44, loss: 0.008347677066922188\n",
      "Iter: 45, loss: 0.008413812145590782\n",
      "Iter: 46, loss: 0.00849919207394123\n",
      "Iter: 47, loss: 0.00829186849296093\n",
      "Iter: 48, loss: 0.008274434134364128\n",
      "Iter: 49, loss: 0.008530855178833008\n",
      "Iter: 50, loss: 0.008454767055809498\n",
      "Iter: 51, loss: 0.00838245265185833\n",
      "Iter: 52, loss: 0.008069214411079884\n",
      "Iter: 53, loss: 0.008379213511943817\n",
      "Iter: 54, loss: 0.008368582464754581\n",
      "Iter: 55, loss: 0.008091633208096027\n",
      "Iter: 56, loss: 0.008430084213614464\n",
      "Iter: 57, loss: 0.00811934657394886\n",
      "Iter: 58, loss: 0.00818270817399025\n",
      "Iter: 59, loss: 0.008472094312310219\n",
      "Iter: 60, loss: 0.008262766525149345\n",
      "Iter: 61, loss: 0.008636623620986938\n",
      "Iter: 62, loss: 0.008489010855555534\n",
      "Iter: 63, loss: 0.008599553257226944\n",
      "Iter: 64, loss: 0.008389833383262157\n",
      "Iter: 65, loss: 0.008181569166481495\n",
      "Iter: 66, loss: 0.00838919822126627\n",
      "Iter: 67, loss: 0.008275119587779045\n",
      "Iter: 68, loss: 0.008064991794526577\n",
      "Iter: 69, loss: 0.008322891779243946\n",
      "Iter: 70, loss: 0.00810831505805254\n",
      "Iter: 71, loss: 0.00807802565395832\n",
      "Iter: 72, loss: 0.008468758314847946\n",
      "Iter: 73, loss: 0.00829724594950676\n",
      "Iter: 74, loss: 0.008276456966996193\n",
      "Iter: 75, loss: 0.008218718692660332\n",
      "Iter: 76, loss: 0.008284180425107479\n",
      "Iter: 77, loss: 0.007921918295323849\n",
      "Iter: 78, loss: 0.00868632085621357\n",
      "Iter: 79, loss: 0.008314577862620354\n",
      "Iter: 80, loss: 0.008041257038712502\n",
      "Iter: 81, loss: 0.008272749371826649\n",
      "Iter: 82, loss: 0.008096463046967983\n",
      "Iter: 83, loss: 0.008014188148081303\n",
      "Iter: 84, loss: 0.008349176496267319\n",
      "Iter: 85, loss: 0.008265646174550056\n",
      "Iter: 86, loss: 0.008732395246624947\n",
      "Iter: 87, loss: 0.007894445210695267\n",
      "Iter: 88, loss: 0.008173095062375069\n",
      "Iter: 89, loss: 0.008457157760858536\n",
      "Iter: 90, loss: 0.008081306703388691\n",
      "Iter: 91, loss: 0.008121746592223644\n",
      "Iter: 92, loss: 0.008185038343071938\n",
      "Iter: 93, loss: 0.008018202148377895\n",
      "Iter: 94, loss: 0.008243510499596596\n",
      "Iter: 95, loss: 0.007945507764816284\n",
      "Iter: 96, loss: 0.008109032176434994\n",
      "Iter: 97, loss: 0.008063276298344135\n",
      "Iter: 98, loss: 0.00861044880002737\n",
      "Iter: 99, loss: 0.008314959704875946\n",
      "Iter: 100, loss: 0.008067814633250237\n",
      "Iter: 101, loss: 0.008280683308839798\n",
      "Iter: 102, loss: 0.00807051733136177\n",
      "Iter: 103, loss: 0.008124709129333496\n",
      "Iter: 104, loss: 0.008242852985858917\n",
      "Iter: 105, loss: 0.007924414239823818\n",
      "Iter: 106, loss: 0.007966598495841026\n",
      "Iter: 107, loss: 0.0076532126404345036\n",
      "Iter: 108, loss: 0.0076480102725327015\n",
      "Iter: 109, loss: 0.00807920005172491\n",
      "Iter: 110, loss: 0.008088226430118084\n",
      "Iter: 111, loss: 0.008266828022897243\n",
      "Iter: 112, loss: 0.008010362274944782\n",
      "Iter: 113, loss: 0.007905607111752033\n",
      "Iter: 114, loss: 0.008069537580013275\n",
      "Iter: 115, loss: 0.008417828939855099\n",
      "Iter: 116, loss: 0.007924949750304222\n",
      "Iter: 117, loss: 0.008494368754327297\n",
      "Iter: 118, loss: 0.008198495022952557\n",
      "Iter: 119, loss: 0.00810861587524414\n",
      "Iter: 120, loss: 0.0076327575370669365\n",
      "Iter: 121, loss: 0.007668778300285339\n",
      "Iter: 122, loss: 0.008286044001579285\n",
      "Iter: 123, loss: 0.00792170874774456\n",
      "Iter: 124, loss: 0.008130456320941448\n",
      "28.85675859451294\n",
      "Iter: 0, loss: 0.008437815122306347\n",
      "Iter: 1, loss: 0.008288823999464512\n",
      "Iter: 2, loss: 0.007805409841239452\n",
      "Iter: 3, loss: 0.007696966640651226\n",
      "Iter: 4, loss: 0.008070419542491436\n",
      "Iter: 5, loss: 0.007996401749551296\n",
      "Iter: 6, loss: 0.00811783131211996\n",
      "Iter: 7, loss: 0.008158355951309204\n",
      "Iter: 8, loss: 0.008183851838111877\n",
      "Iter: 9, loss: 0.008000238798558712\n",
      "Iter: 10, loss: 0.007689017336815596\n",
      "Iter: 11, loss: 0.007975516840815544\n",
      "Iter: 12, loss: 0.00814067292958498\n",
      "Iter: 13, loss: 0.008736792951822281\n",
      "Iter: 14, loss: 0.00824093259871006\n",
      "Iter: 15, loss: 0.007724041119217873\n",
      "Iter: 16, loss: 0.007972732186317444\n",
      "Iter: 17, loss: 0.007596449926495552\n",
      "Iter: 18, loss: 0.007876357063651085\n",
      "Iter: 19, loss: 0.007386018522083759\n",
      "Iter: 20, loss: 0.007721141446381807\n",
      "Iter: 21, loss: 0.007757623679935932\n",
      "Iter: 22, loss: 0.0075547052547335625\n",
      "Iter: 23, loss: 0.007611276116222143\n",
      "Iter: 24, loss: 0.007519904524087906\n",
      "Iter: 25, loss: 0.008059588260948658\n",
      "Iter: 26, loss: 0.007894929498434067\n",
      "Iter: 27, loss: 0.007581198588013649\n",
      "Iter: 28, loss: 0.008110779337584972\n",
      "Iter: 29, loss: 0.00773449894040823\n",
      "Iter: 30, loss: 0.008235331624746323\n",
      "Iter: 31, loss: 0.007623498793691397\n",
      "Iter: 32, loss: 0.007750425487756729\n",
      "Iter: 33, loss: 0.00816422887146473\n",
      "Iter: 34, loss: 0.0077127693220973015\n",
      "Iter: 35, loss: 0.0077162450179457664\n",
      "Iter: 36, loss: 0.007748651783913374\n",
      "Iter: 37, loss: 0.007750900462269783\n",
      "Iter: 38, loss: 0.007917221635580063\n",
      "Iter: 39, loss: 0.00807971227914095\n",
      "Iter: 40, loss: 0.00811953004449606\n",
      "Iter: 41, loss: 0.007405691780149937\n",
      "Iter: 42, loss: 0.00762251066043973\n",
      "Iter: 43, loss: 0.00796250719577074\n",
      "Iter: 44, loss: 0.0075799524784088135\n",
      "Iter: 45, loss: 0.007628589868545532\n",
      "Iter: 46, loss: 0.007732892408967018\n",
      "Iter: 47, loss: 0.0075278934091329575\n",
      "Iter: 48, loss: 0.0075021847151219845\n",
      "Iter: 49, loss: 0.007741663604974747\n",
      "Iter: 50, loss: 0.00765622965991497\n",
      "Iter: 51, loss: 0.007623642683029175\n",
      "Iter: 52, loss: 0.0073213279247283936\n",
      "Iter: 53, loss: 0.00761177483946085\n",
      "Iter: 54, loss: 0.007602898869663477\n",
      "Iter: 55, loss: 0.007351026404649019\n",
      "Iter: 56, loss: 0.007666017394512892\n",
      "Iter: 57, loss: 0.007360025309026241\n",
      "Iter: 58, loss: 0.007430371828377247\n",
      "Iter: 59, loss: 0.007695453241467476\n",
      "Iter: 60, loss: 0.007513268385082483\n",
      "Iter: 61, loss: 0.007853292860090733\n",
      "Iter: 62, loss: 0.007706144824624062\n",
      "Iter: 63, loss: 0.007824688218533993\n",
      "Iter: 64, loss: 0.0076288217678666115\n",
      "Iter: 65, loss: 0.007445179857313633\n",
      "Iter: 66, loss: 0.007621122989803553\n",
      "Iter: 67, loss: 0.007512621581554413\n",
      "Iter: 68, loss: 0.007324607111513615\n",
      "Iter: 69, loss: 0.0075626675970852375\n",
      "Iter: 70, loss: 0.007361899595707655\n",
      "Iter: 71, loss: 0.007330442778766155\n",
      "Iter: 72, loss: 0.007697668392211199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 73, loss: 0.007536970544606447\n",
      "Iter: 74, loss: 0.007517963647842407\n",
      "Iter: 75, loss: 0.007469389121979475\n",
      "Iter: 76, loss: 0.0075345225632190704\n",
      "Iter: 77, loss: 0.007199797313660383\n",
      "Iter: 78, loss: 0.007907253690063953\n",
      "Iter: 79, loss: 0.007559444755315781\n",
      "Iter: 80, loss: 0.007300992496311665\n",
      "Iter: 81, loss: 0.007529605645686388\n",
      "Iter: 82, loss: 0.00736561045050621\n",
      "Iter: 83, loss: 0.007269708905369043\n",
      "Iter: 84, loss: 0.0076103536412119865\n",
      "Iter: 85, loss: 0.007507013622671366\n",
      "Iter: 86, loss: 0.007951815612614155\n",
      "Iter: 87, loss: 0.007180726155638695\n",
      "Iter: 88, loss: 0.007435615174472332\n",
      "Iter: 89, loss: 0.007705459836870432\n",
      "Iter: 90, loss: 0.0073601980693638325\n",
      "Iter: 91, loss: 0.0073839761316776276\n",
      "Iter: 92, loss: 0.0074530672281980515\n",
      "Iter: 93, loss: 0.007299473974853754\n",
      "Iter: 94, loss: 0.007499695755541325\n",
      "Iter: 95, loss: 0.007230495568364859\n",
      "Iter: 96, loss: 0.007363138720393181\n",
      "Iter: 97, loss: 0.007327426690608263\n",
      "Iter: 98, loss: 0.00784387905150652\n",
      "Iter: 99, loss: 0.007590943947434425\n",
      "Iter: 100, loss: 0.007337769027799368\n",
      "Iter: 101, loss: 0.007541488390415907\n",
      "Iter: 102, loss: 0.007343053352087736\n",
      "Iter: 103, loss: 0.007403324358165264\n",
      "Iter: 104, loss: 0.007520151790231466\n",
      "Iter: 105, loss: 0.007206809241324663\n",
      "Iter: 106, loss: 0.007246650289744139\n",
      "Iter: 107, loss: 0.00695708068087697\n",
      "Iter: 108, loss: 0.006954506505280733\n",
      "Iter: 109, loss: 0.007357629016041756\n",
      "Iter: 110, loss: 0.007359635084867477\n",
      "Iter: 111, loss: 0.007528137881308794\n",
      "Iter: 112, loss: 0.007294313050806522\n",
      "Iter: 113, loss: 0.007211612071841955\n",
      "Iter: 114, loss: 0.007349670398980379\n",
      "Iter: 115, loss: 0.007682415656745434\n",
      "Iter: 116, loss: 0.007219148334115744\n",
      "Iter: 117, loss: 0.007750747725367546\n",
      "Iter: 118, loss: 0.007472805678844452\n",
      "Iter: 119, loss: 0.007395237684249878\n",
      "Iter: 120, loss: 0.006942263338714838\n",
      "Iter: 121, loss: 0.006973007693886757\n",
      "Iter: 122, loss: 0.007579068187624216\n",
      "Iter: 123, loss: 0.0072206659242510796\n",
      "Iter: 124, loss: 0.007416360080242157\n",
      "29.160040855407715\n",
      "Iter: 0, loss: 0.0076905181631445885\n",
      "Iter: 1, loss: 0.007560637779533863\n",
      "Iter: 2, loss: 0.0071031744591891766\n",
      "Iter: 3, loss: 0.007010100409388542\n",
      "Iter: 4, loss: 0.007358725648373365\n",
      "Iter: 5, loss: 0.007290873676538467\n",
      "Iter: 6, loss: 0.007414945401251316\n",
      "Iter: 7, loss: 0.007455278653651476\n",
      "Iter: 8, loss: 0.007484257221221924\n",
      "Iter: 9, loss: 0.007284163497388363\n",
      "Iter: 10, loss: 0.007003454957157373\n",
      "Iter: 11, loss: 0.0072883134707808495\n",
      "Iter: 12, loss: 0.007431205827742815\n",
      "Iter: 13, loss: 0.007983972318470478\n",
      "Iter: 14, loss: 0.007514472585171461\n",
      "Iter: 15, loss: 0.007036710623651743\n",
      "Iter: 16, loss: 0.007273678667843342\n",
      "Iter: 17, loss: 0.006920421961694956\n",
      "Iter: 18, loss: 0.0071741328574717045\n",
      "Iter: 19, loss: 0.00671799574047327\n",
      "Iter: 20, loss: 0.00703766942024231\n",
      "Iter: 21, loss: 0.007083529140800238\n",
      "Iter: 22, loss: 0.006889968179166317\n",
      "Iter: 23, loss: 0.0069421869702637196\n",
      "Iter: 24, loss: 0.006858048960566521\n",
      "Iter: 25, loss: 0.007362771779298782\n",
      "Iter: 26, loss: 0.00720049487426877\n",
      "Iter: 27, loss: 0.00692515866830945\n",
      "Iter: 28, loss: 0.007413456682115793\n",
      "Iter: 29, loss: 0.007063603959977627\n",
      "Iter: 30, loss: 0.007512650452554226\n",
      "Iter: 31, loss: 0.006959171034395695\n",
      "Iter: 32, loss: 0.007074200082570314\n",
      "Iter: 33, loss: 0.0074626081623137\n",
      "Iter: 34, loss: 0.007030067034065723\n",
      "Iter: 35, loss: 0.007048070430755615\n",
      "Iter: 36, loss: 0.0070705721154809\n",
      "Iter: 37, loss: 0.007080083712935448\n",
      "Iter: 38, loss: 0.007231018040329218\n",
      "Iter: 39, loss: 0.007395229768007994\n",
      "Iter: 40, loss: 0.007412537932395935\n",
      "Iter: 41, loss: 0.006745623890310526\n",
      "Iter: 42, loss: 0.006956878118216991\n",
      "Iter: 43, loss: 0.007272546179592609\n",
      "Iter: 44, loss: 0.006925971247255802\n",
      "Iter: 45, loss: 0.006960156373679638\n",
      "Iter: 46, loss: 0.007079537492245436\n",
      "Iter: 47, loss: 0.006873434875160456\n",
      "Iter: 48, loss: 0.006846209987998009\n",
      "Iter: 49, loss: 0.007071749307215214\n",
      "Iter: 50, loss: 0.006977998651564121\n",
      "Iter: 51, loss: 0.006976080592721701\n",
      "Iter: 52, loss: 0.006684828549623489\n",
      "Iter: 53, loss: 0.006960072088986635\n",
      "Iter: 54, loss: 0.006951329298317432\n",
      "Iter: 55, loss: 0.006721362471580505\n",
      "Iter: 56, loss: 0.007013471331447363\n",
      "Iter: 57, loss: 0.006712885107845068\n",
      "Iter: 58, loss: 0.0067873927764594555\n",
      "Iter: 59, loss: 0.007033827248960733\n",
      "Iter: 60, loss: 0.006874999962747097\n",
      "Iter: 61, loss: 0.007181695196777582\n",
      "Iter: 62, loss: 0.007039396092295647\n",
      "Iter: 63, loss: 0.007164578419178724\n",
      "Iter: 64, loss: 0.006979808676987886\n",
      "Iter: 65, loss: 0.006815957836806774\n",
      "Iter: 66, loss: 0.0069662462919950485\n",
      "Iter: 67, loss: 0.006865784991532564\n",
      "Iter: 68, loss: 0.006693494971841574\n",
      "Iter: 69, loss: 0.0069131809286773205\n",
      "Iter: 70, loss: 0.0067269220016896725\n",
      "Iter: 71, loss: 0.0066919405944645405\n",
      "Iter: 72, loss: 0.007041616830974817\n",
      "Iter: 73, loss: 0.006887971423566341\n",
      "Iter: 74, loss: 0.0068695335648953915\n",
      "Iter: 75, loss: 0.006830703932791948\n",
      "Iter: 76, loss: 0.006894935853779316\n",
      "Iter: 77, loss: 0.0065861111506819725\n",
      "Iter: 78, loss: 0.007245111744850874\n",
      "Iter: 79, loss: 0.006915149744600058\n",
      "Iter: 80, loss: 0.006669932976365089\n",
      "Iter: 81, loss: 0.006899621803313494\n",
      "Iter: 82, loss: 0.006740964017808437\n",
      "Iter: 83, loss: 0.0066369338892400265\n",
      "Iter: 84, loss: 0.006978064309805632\n",
      "Iter: 85, loss: 0.006863586138933897\n",
      "Iter: 86, loss: 0.007283201906830072\n",
      "Iter: 87, loss: 0.006569864694029093\n",
      "Iter: 88, loss: 0.0068029784597456455\n",
      "Iter: 89, loss: 0.007060057483613491\n",
      "Iter: 90, loss: 0.006744980346411467\n",
      "Iter: 91, loss: 0.006754082627594471\n",
      "Iter: 92, loss: 0.006827081087976694\n",
      "Iter: 93, loss: 0.006684495136141777\n",
      "Iter: 94, loss: 0.0068621509708464146\n",
      "Iter: 95, loss: 0.006619374267756939\n",
      "Iter: 96, loss: 0.006728768348693848\n",
      "Iter: 97, loss: 0.006696347147226334\n",
      "Iter: 98, loss: 0.0071915495209395885\n",
      "Iter: 99, loss: 0.006969521287828684\n",
      "Iter: 100, loss: 0.006715096533298492\n",
      "Iter: 101, loss: 0.006908046547323465\n",
      "Iter: 102, loss: 0.006723381578922272\n",
      "Iter: 103, loss: 0.006787415593862534\n",
      "Iter: 104, loss: 0.006901140324771404\n",
      "Iter: 105, loss: 0.006594090722501278\n",
      "Iter: 106, loss: 0.006632352713495493\n",
      "Iter: 107, loss: 0.006363198161125183\n",
      "Iter: 108, loss: 0.006361565552651882\n",
      "Iter: 109, loss: 0.006739174015820026\n",
      "Iter: 110, loss: 0.006739113479852676\n",
      "Iter: 111, loss: 0.0068975770846009254\n",
      "Iter: 112, loss: 0.006681131664663553\n",
      "Iter: 113, loss: 0.006615942809730768\n",
      "Iter: 114, loss: 0.006732241716235876\n",
      "Iter: 115, loss: 0.007052959408611059\n",
      "Iter: 116, loss: 0.0066098119132220745\n",
      "Iter: 117, loss: 0.007112004328519106\n",
      "Iter: 118, loss: 0.006850554142147303\n",
      "Iter: 119, loss: 0.0067845797166228294\n",
      "Iter: 120, loss: 0.006354121491312981\n",
      "Iter: 121, loss: 0.0063782655633986\n",
      "Iter: 122, loss: 0.006973568815737963\n",
      "Iter: 123, loss: 0.006620162166655064\n",
      "Iter: 124, loss: 0.00680500315502286\n",
      "29.40696930885315\n",
      "Iter: 0, loss: 0.007051272317767143\n",
      "Iter: 1, loss: 0.006934717763215303\n",
      "Iter: 2, loss: 0.006500390823930502\n",
      "Iter: 3, loss: 0.006422548554837704\n",
      "Iter: 4, loss: 0.0067466129548847675\n",
      "Iter: 5, loss: 0.006685012485831976\n",
      "Iter: 6, loss: 0.006813008803874254\n",
      "Iter: 7, loss: 0.0068541779182851315\n",
      "Iter: 8, loss: 0.0068831355310976505\n",
      "Iter: 9, loss: 0.006670526694506407\n",
      "Iter: 10, loss: 0.00641764048486948\n",
      "Iter: 11, loss: 0.006697849836200476\n",
      "Iter: 12, loss: 0.006823830772191286\n",
      "Iter: 13, loss: 0.007336599752306938\n",
      "Iter: 14, loss: 0.00689372094348073\n",
      "Iter: 15, loss: 0.0064459326677024364\n",
      "Iter: 16, loss: 0.006671973969787359\n",
      "Iter: 17, loss: 0.006341203581541777\n",
      "Iter: 18, loss: 0.006572366692125797\n",
      "Iter: 19, loss: 0.006144368089735508\n",
      "Iter: 20, loss: 0.006451982073485851\n",
      "Iter: 21, loss: 0.006502846721559763\n",
      "Iter: 22, loss: 0.006317504215985537\n",
      "Iter: 23, loss: 0.006369202397763729\n",
      "Iter: 24, loss: 0.006290020886808634\n",
      "Iter: 25, loss: 0.006764495279639959\n",
      "Iter: 26, loss: 0.00660310871899128\n",
      "Iter: 27, loss: 0.006361955311149359\n",
      "Iter: 28, loss: 0.00681457482278347\n",
      "Iter: 29, loss: 0.006489080376923084\n",
      "Iter: 30, loss: 0.006889862474054098\n",
      "Iter: 31, loss: 0.006387343630194664\n",
      "Iter: 32, loss: 0.0064939772710204124\n",
      "Iter: 33, loss: 0.00686216913163662\n",
      "Iter: 34, loss: 0.006444510538130999\n",
      "Iter: 35, loss: 0.006474385038018227\n",
      "Iter: 36, loss: 0.006488482002168894\n",
      "Iter: 37, loss: 0.006501645781099796\n",
      "Iter: 38, loss: 0.006642138585448265\n",
      "Iter: 39, loss: 0.00680554797872901\n",
      "Iter: 40, loss: 0.0068051451817154884\n",
      "Iter: 41, loss: 0.006180365104228258\n",
      "Iter: 42, loss: 0.006386001594364643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 43, loss: 0.006678000092506409\n",
      "Iter: 44, loss: 0.006363663822412491\n",
      "Iter: 45, loss: 0.006386504042893648\n",
      "Iter: 46, loss: 0.006521744187921286\n",
      "Iter: 47, loss: 0.006312240846455097\n",
      "Iter: 48, loss: 0.006283583119511604\n",
      "Iter: 49, loss: 0.006495081819593906\n",
      "Iter: 50, loss: 0.00639758724719286\n",
      "Iter: 51, loss: 0.0064171007834374905\n",
      "Iter: 52, loss: 0.006138720083981752\n",
      "Iter: 53, loss: 0.006400314625352621\n",
      "Iter: 54, loss: 0.006390412803739309\n",
      "Iter: 55, loss: 0.006179599557071924\n",
      "Iter: 56, loss: 0.006450003478676081\n",
      "Iter: 57, loss: 0.006157366558909416\n",
      "Iter: 58, loss: 0.006233284715563059\n",
      "Iter: 59, loss: 0.0064674667082726955\n",
      "Iter: 60, loss: 0.00632502930238843\n",
      "Iter: 61, loss: 0.00660732202231884\n",
      "Iter: 62, loss: 0.006467925850301981\n",
      "Iter: 63, loss: 0.006596527993679047\n",
      "Iter: 64, loss: 0.0064226905815303326\n",
      "Iter: 65, loss: 0.0062757087871432304\n",
      "Iter: 66, loss: 0.006404914893209934\n",
      "Iter: 67, loss: 0.006311205215752125\n",
      "Iter: 68, loss: 0.006149180233478546\n",
      "Iter: 69, loss: 0.006356267724186182\n",
      "Iter: 70, loss: 0.006181239616125822\n",
      "Iter: 71, loss: 0.006143074948340654\n",
      "Iter: 72, loss: 0.006475519388914108\n",
      "Iter: 73, loss: 0.00632780697196722\n",
      "Iter: 74, loss: 0.0063138664700090885\n",
      "Iter: 75, loss: 0.00627986341714859\n",
      "Iter: 76, loss: 0.006342405453324318\n",
      "Iter: 77, loss: 0.006057613994926214\n",
      "Iter: 78, loss: 0.006673470605164766\n",
      "Iter: 79, loss: 0.006359160412102938\n",
      "Iter: 80, loss: 0.006128700450062752\n",
      "Iter: 81, loss: 0.006354519631713629\n",
      "Iter: 82, loss: 0.0062054977752268314\n",
      "Iter: 83, loss: 0.0060914792120456696\n",
      "Iter: 84, loss: 0.006433733273297548\n",
      "Iter: 85, loss: 0.006309061776846647\n",
      "Iter: 86, loss: 0.006705349311232567\n",
      "Iter: 87, loss: 0.006043260917067528\n",
      "Iter: 88, loss: 0.0062597570940852165\n",
      "Iter: 89, loss: 0.006503962446004152\n",
      "Iter: 90, loss: 0.006215665489435196\n",
      "Iter: 91, loss: 0.006212877109646797\n",
      "Iter: 92, loss: 0.006289838347584009\n",
      "Iter: 93, loss: 0.006154686212539673\n",
      "Iter: 94, loss: 0.006312736310064793\n",
      "Iter: 95, loss: 0.006092939991503954\n",
      "Iter: 96, loss: 0.006181316915899515\n",
      "Iter: 97, loss: 0.00615350529551506\n",
      "Iter: 98, loss: 0.006627783179283142\n",
      "Iter: 99, loss: 0.00643443688750267\n",
      "Iter: 100, loss: 0.006178425159305334\n",
      "Iter: 101, loss: 0.00636454951018095\n",
      "Iter: 102, loss: 0.006187858060002327\n",
      "Iter: 103, loss: 0.00625674519687891\n",
      "Iter: 104, loss: 0.00636714743450284\n",
      "Iter: 105, loss: 0.0060635823756456375\n",
      "Iter: 106, loss: 0.006103195250034332\n",
      "Iter: 107, loss: 0.0058531248942017555\n",
      "Iter: 108, loss: 0.005852121394127607\n",
      "Iter: 109, loss: 0.00620795413851738\n",
      "Iter: 110, loss: 0.006199023686349392\n",
      "Iter: 111, loss: 0.0063548157922923565\n",
      "Iter: 112, loss: 0.006152115296572447\n",
      "Iter: 113, loss: 0.00610195379704237\n",
      "Iter: 114, loss: 0.0061990381218492985\n",
      "Iter: 115, loss: 0.00650860695168376\n",
      "Iter: 116, loss: 0.0060863192193210125\n",
      "Iter: 117, loss: 0.006561729125678539\n",
      "Iter: 118, loss: 0.006314686965197325\n",
      "Iter: 119, loss: 0.0062567731365561485\n",
      "Iter: 120, loss: 0.0058461083099246025\n",
      "Iter: 121, loss: 0.005867845378816128\n",
      "Iter: 122, loss: 0.006449825596064329\n",
      "Iter: 123, loss: 0.006101988255977631\n",
      "Iter: 124, loss: 0.006277063861489296\n",
      "29.009713649749756\n",
      "Iter: 0, loss: 0.006499267183244228\n",
      "Iter: 1, loss: 0.0063937073573470116\n",
      "Iter: 2, loss: 0.005980042740702629\n",
      "Iter: 3, loss: 0.005915358662605286\n",
      "Iter: 4, loss: 0.006219843402504921\n",
      "Iter: 5, loss: 0.006161412689834833\n",
      "Iter: 6, loss: 0.006291058845818043\n",
      "Iter: 7, loss: 0.006334052886813879\n",
      "Iter: 8, loss: 0.00636331457644701\n",
      "Iter: 9, loss: 0.006141887046396732\n",
      "Iter: 10, loss: 0.005912667140364647\n",
      "Iter: 11, loss: 0.006186882499605417\n",
      "Iter: 12, loss: 0.0062977890484035015\n",
      "Iter: 13, loss: 0.006776700261980295\n",
      "Iter: 14, loss: 0.00635484280064702\n",
      "Iter: 15, loss: 0.005937161855399609\n",
      "Iter: 16, loss: 0.006151089910417795\n",
      "Iter: 17, loss: 0.005840770900249481\n",
      "Iter: 18, loss: 0.006049380172044039\n",
      "Iter: 19, loss: 0.005648026242852211\n",
      "Iter: 20, loss: 0.0059438711032271385\n",
      "Iter: 21, loss: 0.006002781447023153\n",
      "Iter: 22, loss: 0.005823799408972263\n",
      "Iter: 23, loss: 0.0058729867450892925\n",
      "Iter: 24, loss: 0.00579956267029047\n",
      "Iter: 25, loss: 0.006245660595595837\n",
      "Iter: 26, loss: 0.006086348090320826\n",
      "Iter: 27, loss: 0.005874252412468195\n",
      "Iter: 28, loss: 0.006296343635767698\n",
      "Iter: 29, loss: 0.005991634912788868\n",
      "Iter: 30, loss: 0.0063533056527376175\n",
      "Iter: 31, loss: 0.005894188303500414\n",
      "Iter: 32, loss: 0.00599200464785099\n",
      "Iter: 33, loss: 0.006342490203678608\n",
      "Iter: 34, loss: 0.005938517861068249\n",
      "Iter: 35, loss: 0.005975349806249142\n",
      "Iter: 36, loss: 0.005986754782497883\n",
      "Iter: 37, loss: 0.006001496687531471\n",
      "Iter: 38, loss: 0.006132508162409067\n",
      "Iter: 39, loss: 0.00629465002566576\n",
      "Iter: 40, loss: 0.006280031055212021\n",
      "Iter: 41, loss: 0.0056919073686003685\n",
      "Iter: 42, loss: 0.005892303306609392\n",
      "Iter: 43, loss: 0.006162640638649464\n",
      "Iter: 44, loss: 0.005876464769244194\n",
      "Iter: 45, loss: 0.005890449974685907\n",
      "Iter: 46, loss: 0.006035393103957176\n",
      "Iter: 47, loss: 0.005826212465763092\n",
      "Iter: 48, loss: 0.00579698896035552\n",
      "Iter: 49, loss: 0.005997378379106522\n",
      "Iter: 50, loss: 0.00589203555136919\n",
      "Iter: 51, loss: 0.00593359163030982\n",
      "Iter: 52, loss: 0.005667148623615503\n",
      "Iter: 53, loss: 0.005913907662034035\n",
      "Iter: 54, loss: 0.005904868710786104\n",
      "Iter: 55, loss: 0.005710079800337553\n",
      "Iter: 56, loss: 0.005961187183856964\n",
      "Iter: 57, loss: 0.005677307024598122\n",
      "Iter: 58, loss: 0.005753585137426853\n",
      "Iter: 59, loss: 0.005974579136818647\n",
      "Iter: 60, loss: 0.005847262218594551\n",
      "Iter: 61, loss: 0.006107916124165058\n",
      "Iter: 62, loss: 0.005971620324999094\n",
      "Iter: 63, loss: 0.006102332845330238\n",
      "Iter: 64, loss: 0.005939398426562548\n",
      "Iter: 65, loss: 0.005806729197502136\n",
      "Iter: 66, loss: 0.005917751230299473\n",
      "Iter: 67, loss: 0.005831719376146793\n",
      "Iter: 68, loss: 0.005676671396940947\n",
      "Iter: 69, loss: 0.0058719078078866005\n",
      "Iter: 70, loss: 0.005708134733140469\n",
      "Iter: 71, loss: 0.0056682247668504715\n",
      "Iter: 72, loss: 0.005982298869639635\n",
      "Iter: 73, loss: 0.005841868929564953\n",
      "Iter: 74, loss: 0.005831912159919739\n",
      "Iter: 75, loss: 0.005804962012916803\n",
      "Iter: 76, loss: 0.005864774342626333\n",
      "Iter: 77, loss: 0.00559783773496747\n",
      "Iter: 78, loss: 0.006176736205816269\n",
      "Iter: 79, loss: 0.005877943709492683\n",
      "Iter: 80, loss: 0.005660699680447578\n",
      "Iter: 81, loss: 0.005881903227418661\n",
      "Iter: 82, loss: 0.005738984793424606\n",
      "Iter: 83, loss: 0.00561881810426712\n",
      "Iter: 84, loss: 0.005961017217487097\n",
      "Iter: 85, loss: 0.0058267866261303425\n",
      "Iter: 86, loss: 0.0062051317654550076\n",
      "Iter: 87, loss: 0.005586104933172464\n",
      "Iter: 88, loss: 0.005788056645542383\n",
      "Iter: 89, loss: 0.006019212771207094\n",
      "Iter: 90, loss: 0.0057548838667571545\n",
      "Iter: 91, loss: 0.005739323329180479\n",
      "Iter: 92, loss: 0.005823980085551739\n",
      "Iter: 93, loss: 0.005695715546607971\n",
      "Iter: 94, loss: 0.005835589487105608\n",
      "Iter: 95, loss: 0.005636370740830898\n",
      "Iter: 96, loss: 0.005707304924726486\n",
      "Iter: 97, loss: 0.00568404421210289\n",
      "Iter: 98, loss: 0.006139267235994339\n",
      "Iter: 99, loss: 0.005970019847154617\n",
      "Iter: 100, loss: 0.005711353849619627\n",
      "Iter: 101, loss: 0.005889452528208494\n",
      "Iter: 102, loss: 0.005722957197576761\n",
      "Iter: 103, loss: 0.005794895347207785\n",
      "Iter: 104, loss: 0.005903509445488453\n",
      "Iter: 105, loss: 0.005603006575256586\n",
      "Iter: 106, loss: 0.005642992909997702\n",
      "Iter: 107, loss: 0.0054105473682284355\n",
      "Iter: 108, loss: 0.005408015567809343\n",
      "Iter: 109, loss: 0.005746112205088139\n",
      "Iter: 110, loss: 0.005731952376663685\n",
      "Iter: 111, loss: 0.005884623154997826\n",
      "Iter: 112, loss: 0.005692607723176479\n",
      "Iter: 113, loss: 0.005655630491673946\n",
      "Iter: 114, loss: 0.005736439023166895\n",
      "Iter: 115, loss: 0.006035706494003534\n",
      "Iter: 116, loss: 0.005630049854516983\n",
      "Iter: 117, loss: 0.006082815583795309\n",
      "Iter: 118, loss: 0.005849451757967472\n",
      "Iter: 119, loss: 0.005798587575554848\n",
      "Iter: 120, loss: 0.005406423471868038\n",
      "Iter: 121, loss: 0.005423628259450197\n",
      "Iter: 122, loss: 0.005993493366986513\n",
      "Iter: 123, loss: 0.005652297288179398\n",
      "Iter: 124, loss: 0.005817847792059183\n",
      "29.460012197494507\n",
      "Iter: 0, loss: 0.006018606945872307\n",
      "Iter: 1, loss: 0.005924977827817202\n",
      "Iter: 2, loss: 0.005527430679649115\n",
      "Iter: 3, loss: 0.005475078709423542\n",
      "Iter: 4, loss: 0.005760825704783201\n",
      "Iter: 5, loss: 0.005707684438675642\n",
      "Iter: 6, loss: 0.005836028140038252\n",
      "Iter: 7, loss: 0.005881187971681356\n",
      "Iter: 8, loss: 0.005909009836614132\n",
      "Iter: 9, loss: 0.005681444890797138\n",
      "Iter: 10, loss: 0.005475127138197422\n",
      "Iter: 11, loss: 0.005742795765399933\n",
      "Iter: 12, loss: 0.0058385953307151794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 13, loss: 0.006290254648774862\n",
      "Iter: 14, loss: 0.005886174738407135\n",
      "Iter: 15, loss: 0.005493386182934046\n",
      "Iter: 16, loss: 0.0056996108032763\n",
      "Iter: 17, loss: 0.005404071882367134\n",
      "Iter: 18, loss: 0.005594437941908836\n",
      "Iter: 19, loss: 0.005218156613409519\n",
      "Iter: 20, loss: 0.005501819308847189\n",
      "Iter: 21, loss: 0.005564944818615913\n",
      "Iter: 22, loss: 0.005394055973738432\n",
      "Iter: 23, loss: 0.005439731292426586\n",
      "Iter: 24, loss: 0.00537204509600997\n",
      "Iter: 25, loss: 0.005793604534119368\n",
      "Iter: 26, loss: 0.00563765550032258\n",
      "Iter: 27, loss: 0.0054502300918102264\n",
      "Iter: 28, loss: 0.005842233542352915\n",
      "Iter: 29, loss: 0.005558673292398453\n",
      "Iter: 30, loss: 0.005884532816708088\n",
      "Iter: 31, loss: 0.005464375484734774\n",
      "Iter: 32, loss: 0.0055547053925693035\n",
      "Iter: 33, loss: 0.00588854169473052\n",
      "Iter: 34, loss: 0.0054968721233308315\n",
      "Iter: 35, loss: 0.005542881321161985\n",
      "Iter: 36, loss: 0.0055511388927698135\n",
      "Iter: 37, loss: 0.005565284751355648\n",
      "Iter: 38, loss: 0.005689182318747044\n",
      "Iter: 39, loss: 0.0058484990149736404\n",
      "Iter: 40, loss: 0.005823461804538965\n",
      "Iter: 41, loss: 0.005268503446131945\n",
      "Iter: 42, loss: 0.00546289561316371\n",
      "Iter: 43, loss: 0.005713258404284716\n",
      "Iter: 44, loss: 0.0054527632892131805\n",
      "Iter: 45, loss: 0.00545890536159277\n",
      "Iter: 46, loss: 0.005611885339021683\n",
      "Iter: 47, loss: 0.005403625313192606\n",
      "Iter: 48, loss: 0.005373061168938875\n",
      "Iter: 49, loss: 0.0055619534105062485\n",
      "Iter: 50, loss: 0.005452718585729599\n",
      "Iter: 51, loss: 0.0055129267275333405\n",
      "Iter: 52, loss: 0.005255925469100475\n",
      "Iter: 53, loss: 0.005490623414516449\n",
      "Iter: 54, loss: 0.005481385160237551\n",
      "Iter: 55, loss: 0.005299525335431099\n",
      "Iter: 56, loss: 0.005533766467124224\n",
      "Iter: 57, loss: 0.005257788579910994\n",
      "Iter: 58, loss: 0.005336526781320572\n",
      "Iter: 59, loss: 0.005545525345951319\n",
      "Iter: 60, loss: 0.005431264638900757\n",
      "Iter: 61, loss: 0.005674462299793959\n",
      "Iter: 62, loss: 0.0055381618440151215\n",
      "Iter: 63, loss: 0.005671593360602856\n",
      "Iter: 64, loss: 0.005517779849469662\n",
      "Iter: 65, loss: 0.0053963493555784225\n",
      "Iter: 66, loss: 0.005493691191077232\n",
      "Iter: 67, loss: 0.005412270314991474\n",
      "Iter: 68, loss: 0.005265443120151758\n",
      "Iter: 69, loss: 0.005449475254863501\n",
      "Iter: 70, loss: 0.005297175608575344\n",
      "Iter: 71, loss: 0.005253824405372143\n",
      "Iter: 72, loss: 0.005553855560719967\n",
      "Iter: 73, loss: 0.005417913664132357\n",
      "Iter: 74, loss: 0.005410084035247564\n",
      "Iter: 75, loss: 0.0053912014700472355\n",
      "Iter: 76, loss: 0.005446462891995907\n",
      "Iter: 77, loss: 0.005199009086936712\n",
      "Iter: 78, loss: 0.005744675174355507\n",
      "Iter: 79, loss: 0.005456574261188507\n",
      "Iter: 80, loss: 0.005251377355307341\n",
      "Iter: 81, loss: 0.005468784831464291\n",
      "Iter: 82, loss: 0.0053339870646595955\n",
      "Iter: 83, loss: 0.005206992384046316\n",
      "Iter: 84, loss: 0.005548766348510981\n",
      "Iter: 85, loss: 0.005407286342233419\n",
      "Iter: 86, loss: 0.005768615752458572\n",
      "Iter: 87, loss: 0.005189691204577684\n",
      "Iter: 88, loss: 0.005375328473746777\n",
      "Iter: 89, loss: 0.005597698036581278\n",
      "Iter: 90, loss: 0.005353420041501522\n",
      "Iter: 91, loss: 0.005327398888766766\n",
      "Iter: 92, loss: 0.005416322033852339\n",
      "Iter: 93, loss: 0.005295770708471537\n",
      "Iter: 94, loss: 0.005418093875050545\n",
      "Iter: 95, loss: 0.0052387909963727\n",
      "Iter: 96, loss: 0.0052935476414859295\n",
      "Iter: 97, loss: 0.005273627582937479\n",
      "Iter: 98, loss: 0.005710655357688665\n",
      "Iter: 99, loss: 0.0055634560994803905\n",
      "Iter: 100, loss: 0.00530390627682209\n",
      "Iter: 101, loss: 0.005475562531501055\n",
      "Iter: 102, loss: 0.005315687507390976\n",
      "Iter: 103, loss: 0.005393311381340027\n",
      "Iter: 104, loss: 0.005498659797012806\n",
      "Iter: 105, loss: 0.005202246829867363\n",
      "Iter: 106, loss: 0.005240566097199917\n",
      "Iter: 107, loss: 0.005024014040827751\n",
      "Iter: 108, loss: 0.0050206296145915985\n",
      "Iter: 109, loss: 0.005343073047697544\n",
      "Iter: 110, loss: 0.005323814693838358\n",
      "Iter: 111, loss: 0.005471399985253811\n",
      "Iter: 112, loss: 0.0052910055965185165\n",
      "Iter: 113, loss: 0.005267793778330088\n",
      "Iter: 114, loss: 0.005329673178493977\n",
      "Iter: 115, loss: 0.005619577132165432\n",
      "Iter: 116, loss: 0.005230452865362167\n",
      "Iter: 117, loss: 0.005662268493324518\n",
      "Iter: 118, loss: 0.00544060580432415\n",
      "Iter: 119, loss: 0.005396762862801552\n",
      "Iter: 120, loss: 0.005022610537707806\n",
      "Iter: 121, loss: 0.005035632289946079\n",
      "Iter: 122, loss: 0.005592850968241692\n",
      "Iter: 123, loss: 0.005257697775959969\n",
      "Iter: 124, loss: 0.005415511783212423\n",
      "29.4804265499115\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.n_epochs):\n",
    "    start = time.time()\n",
    "    for idx, i in enumerate(data_loader):\n",
    "        X = (MolGraph(i[0]))\n",
    "        Y = (MolGraph(i[1]))\n",
    "\n",
    "        # create your optimizer\n",
    "        optimizer = torch.optim.SGD(molopt.parameters(), lr=0.01)\n",
    "\n",
    "        # in your training loop:\n",
    "        optimizer.zero_grad()   # zero the gradient buffers\n",
    "        loss = molopt.forward_train(X, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()    # Does the update\n",
    "\n",
    "        print (\"Iter: {}, loss: {}\".format(idx, loss.item()))\n",
    "    end = time.time()\n",
    "    print(\"Time for epoch {}: {}\", epoch, end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 22),\n",
       " (22, 18),\n",
       " (40, 21),\n",
       " (61, 18),\n",
       " (79, 21),\n",
       " (100, 20),\n",
       " (120, 24),\n",
       " (144, 20),\n",
       " (164, 22),\n",
       " (186, 19),\n",
       " (205, 21),\n",
       " (226, 25),\n",
       " (251, 25),\n",
       " (276, 22),\n",
       " (298, 23),\n",
       " (321, 19),\n",
       " (340, 20),\n",
       " (360, 26),\n",
       " (386, 25),\n",
       " (411, 22),\n",
       " (433, 22),\n",
       " (455, 21),\n",
       " (476, 24),\n",
       " (500, 23),\n",
       " (523, 19),\n",
       " (542, 24),\n",
       " (566, 20),\n",
       " (586, 26),\n",
       " (612, 23),\n",
       " (635, 24),\n",
       " (659, 20),\n",
       " (679, 16),\n",
       " (695, 24),\n",
       " (719, 19),\n",
       " (738, 24),\n",
       " (762, 19),\n",
       " (781, 18),\n",
       " (799, 23),\n",
       " (822, 22),\n",
       " (844, 22),\n",
       " (866, 24),\n",
       " (890, 24),\n",
       " (914, 24),\n",
       " (938, 20),\n",
       " (958, 20),\n",
       " (978, 20),\n",
       " (998, 23),\n",
       " (1021, 20)]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0455,  0.0447,  0.0261,  ..., -0.0160,  0.0526,  0.0204],\n",
       "        [-0.0465,  0.0597,  0.0177,  ..., -0.1785, -0.0618,  0.0922],\n",
       "        [ 0.0161,  0.0673, -0.1091,  ...,  0.0081, -0.0677,  0.1738],\n",
       "        ...,\n",
       "        [ 0.0850, -0.0669, -0.0290,  ..., -0.0392, -0.0300,  0.0311],\n",
       "        [ 0.1073, -0.0826, -0.0462,  ..., -0.0227, -0.0654,  0.0300],\n",
       "        [ 0.0796,  0.0683,  0.0362,  ..., -0.0448, -0.0863,  0.0600]],\n",
       "       device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_embedding.narrow(0, 0, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 50])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_embedding[0:22,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-9.2475e-02,  4.4592e-02,  4.1157e-02, -6.2958e-02, -2.6449e-02,\n",
       "          5.1672e-03, -4.6384e-02, -5.3797e-02, -7.3464e-02, -7.3005e-02,\n",
       "         -1.7704e-02,  8.3490e-02, -1.3573e-01,  8.2092e-03, -5.4852e-02,\n",
       "         -1.0005e-01,  1.0265e-01,  8.8467e-02,  1.8191e-02,  8.6351e-02,\n",
       "          1.7423e-01,  2.4751e-02,  6.3913e-03,  3.5624e-03,  8.4073e-03,\n",
       "         -2.3538e-02,  1.3579e-01,  4.8769e-02,  1.5200e-01, -2.5218e-02,\n",
       "          2.4999e-01, -7.4404e-03,  6.2257e-02, -1.2847e-01, -1.4130e-01,\n",
       "          1.4734e-01, -2.1842e-03,  8.3792e-02, -8.7801e-02,  4.2352e-02,\n",
       "          2.5093e-01, -3.2312e-02,  7.7363e-02,  6.2747e-02,  1.1126e-01,\n",
       "          6.4715e-02, -3.6868e-02,  1.1756e-03,  7.7242e-03,  7.6595e-03],\n",
       "        [ 2.0119e-03, -1.1420e-01, -6.6094e-03, -6.3875e-02,  8.7873e-02,\n",
       "          1.7288e-02,  9.8871e-02, -1.4052e-01, -5.1341e-02, -1.6861e-01,\n",
       "         -7.1023e-02,  3.3994e-02,  2.1864e-02, -9.5770e-03,  4.5321e-02,\n",
       "          1.1468e-01,  1.7144e-01,  1.0275e-01,  6.4650e-02,  6.7548e-02,\n",
       "          1.1096e-01, -1.7862e-01, -1.1572e-01,  3.9683e-03,  2.1316e-02,\n",
       "          5.1599e-02,  1.9089e-02, -9.7457e-02, -5.7705e-02, -3.2013e-02,\n",
       "          1.1510e-01, -1.3428e-01,  7.6023e-02,  1.1371e-01, -1.2995e-01,\n",
       "          1.5539e-01,  6.3551e-02,  6.3039e-02, -3.3870e-02,  2.1248e-01,\n",
       "         -5.7164e-03,  5.5832e-02, -1.1985e-01,  3.5071e-02, -5.7318e-02,\n",
       "         -6.9915e-02, -1.2773e-02,  2.2984e-02, -1.9392e-01,  5.6128e-02],\n",
       "        [-7.4831e-02,  1.9202e-02,  2.6232e-02, -4.9086e-02,  1.6536e-01,\n",
       "          1.1132e-02,  2.0409e-02, -1.3349e-01,  1.0190e-02, -1.5394e-01,\n",
       "         -6.3496e-02, -6.5835e-02,  1.8784e-02,  1.0282e-02,  1.2250e-01,\n",
       "          3.5327e-02,  9.5941e-02,  9.9225e-02,  1.0245e-01,  1.0312e-01,\n",
       "          1.3071e-01, -3.9658e-02, -1.2481e-01,  2.3864e-02,  4.8434e-02,\n",
       "          2.9407e-02, -7.0972e-02, -9.2338e-02, -2.1639e-02,  1.9514e-02,\n",
       "          9.3547e-02, -9.2258e-02,  1.4259e-01,  1.0263e-01, -1.0799e-01,\n",
       "          3.9730e-02, -4.9214e-03,  8.1520e-03,  5.2861e-02,  1.0459e-01,\n",
       "          3.7120e-02,  5.9990e-02, -7.3661e-02,  4.6982e-02,  1.8996e-02,\n",
       "         -1.5189e-02,  5.7292e-02, -1.1482e-01, -1.5137e-01,  4.1276e-02],\n",
       "        [ 1.1107e-01, -1.1086e-01, -6.8608e-02, -7.9348e-02, -9.8589e-03,\n",
       "         -3.3475e-02,  2.5361e-02, -1.0676e-01, -3.0541e-02, -1.3399e-01,\n",
       "          7.1472e-02,  1.2067e-02, -1.1203e-01, -6.5732e-02,  3.8560e-02,\n",
       "         -3.7062e-02,  5.2307e-03, -7.6133e-04,  1.2335e-01,  1.3654e-01,\n",
       "          5.2324e-02,  1.1503e-02, -3.3225e-02,  4.9500e-02, -3.9982e-03,\n",
       "         -1.1418e-01,  9.5772e-02, -2.7959e-02,  5.9731e-02, -7.4483e-02,\n",
       "          1.1130e-01, -5.3144e-02,  4.5784e-02,  7.8103e-02, -9.3262e-02,\n",
       "          1.4370e-01,  3.0391e-02,  1.0941e-02, -6.8041e-02,  1.2573e-01,\n",
       "          8.3403e-02, -1.9750e-02,  4.3076e-02,  5.5770e-02, -1.0587e-04,\n",
       "          1.9467e-02, -4.1530e-02, -5.3311e-02, -6.1343e-02,  5.2183e-02],\n",
       "        [ 1.4965e-01, -4.3767e-02, -1.6036e-02, -4.3057e-02, -1.3387e-02,\n",
       "         -4.2743e-02,  3.6037e-02, -1.3668e-01,  7.7198e-03, -1.7244e-01,\n",
       "          4.1195e-02,  7.0311e-02, -1.4857e-01, -1.4191e-01,  5.1213e-02,\n",
       "         -7.0471e-02,  7.4260e-02,  2.3955e-02,  4.7956e-02,  1.9671e-01,\n",
       "         -2.9506e-02, -7.5589e-02, -4.6484e-02, -9.1569e-02,  1.0470e-01,\n",
       "          6.0248e-03,  4.1826e-02,  5.8838e-02,  6.5742e-03,  8.4379e-02,\n",
       "         -4.3867e-02, -1.1369e-02,  1.2448e-01, -2.8557e-02,  5.5058e-02,\n",
       "          1.8164e-01,  6.6553e-03,  9.0238e-02, -2.0040e-01,  1.6113e-01,\n",
       "          1.0416e-01,  9.9796e-02, -4.6932e-02, -2.1982e-02, -1.3103e-01,\n",
       "          5.8650e-02,  1.5278e-02,  8.2953e-02, -1.0486e-01,  1.0686e-01],\n",
       "        [-1.3811e-01, -1.0962e-02,  1.5299e-01, -1.5090e-01,  4.2050e-02,\n",
       "         -7.9550e-02,  1.4646e-01, -3.8062e-02,  1.0760e-02, -9.9155e-02,\n",
       "         -1.3323e-01,  1.3856e-01, -6.6188e-02, -3.5772e-02,  1.2616e-03,\n",
       "         -5.8006e-02,  5.4338e-02,  3.4116e-02,  3.9887e-02, -2.6394e-02,\n",
       "         -3.1722e-02, -7.0152e-02, -1.0378e-01, -6.5871e-02, -2.9889e-02,\n",
       "         -4.4786e-02, -3.9086e-02,  1.8439e-02,  1.9467e-01,  1.3721e-01,\n",
       "          8.1558e-04, -2.5913e-02,  3.7295e-03, -1.0363e-01, -1.8557e-01,\n",
       "          9.6840e-02, -7.1866e-02,  1.1633e-01,  3.3533e-02, -8.6680e-02,\n",
       "          9.4380e-02,  3.4461e-02, -5.2381e-02,  7.9040e-02,  3.4064e-02,\n",
       "          1.6273e-01,  1.3577e-02, -1.4633e-01, -8.3325e-02,  1.2317e-01],\n",
       "        [ 1.3637e-01, -4.4353e-02, -4.4834e-02, -3.8192e-02, -1.4762e-02,\n",
       "         -6.8501e-02,  5.0284e-02, -1.4969e-01, -3.9461e-03, -1.5554e-01,\n",
       "          2.2196e-02,  9.1206e-02, -2.2257e-01, -9.7455e-02,  3.4719e-02,\n",
       "          5.4771e-03,  6.0597e-02,  4.3464e-02,  8.3396e-02,  1.6212e-01,\n",
       "          5.4218e-02, -9.5843e-02, -8.6093e-02, -6.3744e-02,  9.9415e-02,\n",
       "          2.2308e-02,  1.7616e-02,  2.0128e-02,  3.0082e-02,  8.9772e-02,\n",
       "         -1.0687e-02, -8.5193e-02,  1.4852e-01, -1.3911e-02,  7.9518e-02,\n",
       "          2.1588e-01,  2.2159e-02,  5.5309e-02, -2.2926e-01,  1.1593e-01,\n",
       "          1.5672e-01,  7.2799e-02, -5.9888e-04, -4.4272e-02, -7.3670e-02,\n",
       "          1.2101e-01,  6.7118e-03,  4.4005e-02, -7.9488e-02,  1.4179e-01],\n",
       "        [ 5.0304e-04, -4.7265e-02, -6.8680e-02, -1.8989e-02, -4.3069e-02,\n",
       "          1.2028e-02,  3.0058e-02, -1.1381e-01, -4.9087e-03, -9.1327e-02,\n",
       "          4.2059e-02,  3.9389e-02, -1.3250e-01, -4.4926e-02,  2.9262e-02,\n",
       "          9.3829e-03,  6.7096e-02, -1.2767e-02,  1.2730e-01,  1.1479e-01,\n",
       "         -1.2171e-03, -7.9559e-02,  1.3363e-02, -3.9249e-02,  1.0448e-01,\n",
       "         -4.9952e-02,  5.7367e-02,  7.9590e-03,  7.2742e-02,  1.9224e-02,\n",
       "         -5.6658e-02, -7.9287e-02,  3.9887e-02,  6.7840e-02,  1.2043e-01,\n",
       "          2.2017e-01, -2.5548e-02,  6.9413e-02, -1.2256e-01,  1.0479e-01,\n",
       "          8.6722e-02,  1.2362e-01, -3.2506e-02,  1.9682e-02, -4.7247e-02,\n",
       "          1.0296e-01, -2.9497e-02, -6.7464e-02, -5.0787e-02,  2.1128e-01],\n",
       "        [-8.4101e-02, -1.2952e-01,  4.6561e-03,  2.5336e-02,  8.9385e-02,\n",
       "         -3.3152e-02, -6.2429e-02, -1.1558e-01,  3.9478e-02,  7.0717e-04,\n",
       "          1.9344e-03, -1.9591e-03, -4.0175e-02, -1.8285e-01, -2.4802e-02,\n",
       "         -1.2054e-01,  4.9930e-02,  2.6806e-02,  1.5114e-02,  3.2357e-02,\n",
       "          3.1959e-02,  3.0837e-02, -1.8034e-01, -4.4719e-02, -1.4276e-02,\n",
       "         -4.3431e-03, -1.8570e-02,  1.0516e-01,  1.9579e-02,  8.9768e-02,\n",
       "          2.8135e-02, -3.8529e-02,  3.8870e-02, -6.8335e-02, -1.2181e-01,\n",
       "          2.4701e-01, -5.1530e-02,  1.2441e-01, -5.6590e-02,  1.2005e-01,\n",
       "          1.3201e-01, -2.4858e-02,  8.6666e-02, -4.5921e-02,  9.3157e-02,\n",
       "          6.3326e-02,  3.9900e-02, -1.4168e-02, -2.9439e-02,  8.9053e-02],\n",
       "        [-1.1275e-01,  1.2943e-03, -5.4671e-02, -6.5457e-02, -7.1936e-02,\n",
       "         -1.3738e-02, -3.0450e-02,  1.2147e-01,  9.9804e-02, -1.1622e-01,\n",
       "          1.0904e-01,  9.0601e-02, -8.1999e-02, -3.5589e-02,  2.1940e-01,\n",
       "          1.2496e-02, -5.0089e-02, -8.2836e-02,  3.4453e-02, -6.3917e-03,\n",
       "          9.5113e-02,  4.1443e-02,  8.1753e-02,  3.9420e-02, -5.5499e-02,\n",
       "         -8.2504e-02,  7.4723e-02, -1.4665e-01,  7.0595e-02, -3.6533e-02,\n",
       "          8.3312e-02, -8.9807e-02, -3.5077e-02,  1.9412e-01,  1.8954e-02,\n",
       "          2.2515e-01,  3.8089e-02, -5.1842e-02,  8.1517e-02, -4.1861e-03,\n",
       "          2.3012e-02,  1.4055e-02,  3.7473e-02, -3.2812e-02, -1.2586e-02,\n",
       "         -4.8062e-02, -4.3280e-02, -2.8999e-02, -2.0492e-01,  2.0290e-01],\n",
       "        [-3.6591e-02,  4.6016e-03, -7.6796e-02, -1.8898e-02, -4.2509e-02,\n",
       "          5.2543e-02, -9.9672e-03, -1.1597e-01,  1.5406e-02, -3.6737e-02,\n",
       "          6.9244e-02,  1.5420e-02, -1.1354e-01, -5.1621e-02,  5.0807e-02,\n",
       "          2.1520e-02,  5.6531e-02,  2.4925e-02,  1.6732e-01,  9.6959e-02,\n",
       "          7.0073e-03, -7.5256e-02, -4.8453e-02, -2.2980e-02,  9.7129e-02,\n",
       "          6.9751e-02,  6.9503e-02,  1.0228e-02,  1.1353e-01,  3.1293e-02,\n",
       "         -2.1551e-03, -9.0327e-02,  8.6456e-02,  7.5242e-02,  6.5324e-02,\n",
       "          2.2482e-01, -4.8340e-02,  7.8806e-02, -8.1247e-02,  1.1624e-01,\n",
       "          1.5895e-01,  9.4216e-02, -4.1416e-02,  4.3831e-02, -7.5481e-02,\n",
       "          1.1918e-01, -6.2028e-02, -4.3639e-02, -4.4378e-02,  1.7389e-01],\n",
       "        [-5.7643e-02,  9.0985e-03, -2.9833e-02,  4.5143e-03, -9.8575e-03,\n",
       "         -5.3548e-02,  1.5851e-02, -1.0652e-01,  6.3350e-03, -2.0842e-03,\n",
       "          5.1616e-02,  4.2332e-02,  2.1081e-03, -1.9878e-01,  8.5597e-02,\n",
       "         -1.6425e-01, -5.8760e-03,  3.3283e-02, -1.0720e-01,  1.4510e-01,\n",
       "          2.2784e-02,  4.8351e-02, -1.7794e-01,  1.0362e-02, -6.6807e-02,\n",
       "          2.6328e-02,  4.2177e-02,  4.3030e-02,  4.8680e-02,  1.0780e-01,\n",
       "          1.0842e-01, -6.3398e-02,  1.0684e-02,  3.4621e-02, -3.8505e-02,\n",
       "          9.0589e-02,  3.8153e-02,  1.3906e-01, -1.6519e-02,  4.2114e-02,\n",
       "          1.2325e-01, -7.1997e-02, -3.0775e-02,  6.2293e-02, -3.6598e-02,\n",
       "         -6.2821e-02,  8.2474e-03, -8.8951e-02,  5.3630e-02, -1.4264e-02],\n",
       "        [-7.8254e-02, -8.2792e-02,  1.8675e-02, -3.5932e-02,  5.6793e-02,\n",
       "          5.2942e-02,  7.4303e-02, -3.8158e-02, -8.5450e-02, -1.1032e-01,\n",
       "         -4.6928e-02, -3.1212e-03,  8.4089e-02,  4.6139e-02,  5.1325e-02,\n",
       "          1.7325e-01,  1.2102e-01,  7.5074e-02,  1.6370e-01, -4.7505e-02,\n",
       "          1.2629e-01, -2.2183e-01, -8.4482e-02,  6.4749e-02, -9.5201e-03,\n",
       "         -1.6193e-02,  1.7835e-02, -1.0828e-01,  5.5775e-02, -1.0897e-01,\n",
       "          5.6066e-03, -1.4808e-01, -3.7711e-02,  2.1916e-01, -1.1704e-01,\n",
       "          1.4177e-01,  3.2967e-02,  9.5712e-02, -3.4131e-03,  2.3827e-01,\n",
       "         -1.3770e-02,  3.3080e-02, -1.6552e-01,  9.0251e-02,  1.5492e-02,\n",
       "         -2.8265e-02, -4.8025e-02, -1.1888e-01, -1.7046e-01,  1.3865e-01],\n",
       "        [ 3.4114e-02,  1.0204e-01, -9.0270e-02,  2.8761e-02, -3.7776e-02,\n",
       "         -1.2258e-02, -1.4382e-01,  4.7977e-02,  4.9830e-02,  1.5700e-02,\n",
       "          1.3497e-01,  6.7086e-02, -1.2683e-01, -4.5996e-02,  1.0811e-01,\n",
       "          1.4485e-02,  1.6109e-02,  4.6190e-02,  5.7579e-02,  7.8621e-02,\n",
       "          1.5290e-01, -7.5824e-03,  3.9437e-03, -1.0607e-02,  2.2816e-02,\n",
       "         -8.0109e-02,  7.4569e-02, -3.1358e-02,  9.5366e-02,  3.2356e-03,\n",
       "         -4.7866e-02,  8.0441e-02,  1.0015e-01,  2.1060e-02,  5.2461e-02,\n",
       "          6.3485e-02,  1.0180e-03, -5.4314e-02, -3.3123e-03,  3.4116e-02,\n",
       "          7.1588e-02,  1.3165e-01,  1.0242e-02,  1.3733e-02, -2.7791e-02,\n",
       "         -9.8141e-03, -5.0253e-02,  1.8194e-02, -9.1925e-02,  1.7552e-01],\n",
       "        [ 3.7890e-02,  7.0168e-02, -9.0855e-02,  6.6424e-02, -4.5429e-02,\n",
       "         -5.5615e-02, -1.0286e-01,  9.2074e-03, -1.1636e-02, -2.2935e-02,\n",
       "          8.6468e-02,  2.9843e-02, -1.0275e-01, -3.1200e-02,  5.9485e-02,\n",
       "          8.5958e-03, -1.5521e-02,  1.9868e-02,  8.7264e-02,  8.1197e-02,\n",
       "          1.2720e-01, -2.3281e-02, -1.1109e-02, -2.8612e-05,  8.5473e-03,\n",
       "         -9.9978e-02,  8.3859e-02, -4.0947e-03,  8.3056e-02, -1.4108e-02,\n",
       "         -9.8817e-02,  1.2138e-01,  7.1917e-02, -3.6474e-03,  3.2560e-02,\n",
       "          3.9896e-02,  5.0349e-04, -3.1494e-02, -4.4667e-02,  5.3413e-02,\n",
       "          3.8768e-02,  1.0952e-01,  2.6022e-02, -1.3302e-02, -3.4250e-02,\n",
       "         -1.2320e-02, -2.2463e-02, -1.0446e-03, -5.0203e-02,  2.0068e-01],\n",
       "        [ 3.0044e-03,  9.2889e-02,  4.0899e-02,  6.2183e-02,  8.6703e-02,\n",
       "         -1.2006e-02,  5.7864e-03,  1.6809e-02,  1.0045e-01, -8.5322e-03,\n",
       "          9.9359e-02, -3.2882e-02, -1.6531e-02, -2.0355e-01,  1.4233e-01,\n",
       "         -4.0833e-02, -6.8080e-02,  2.4784e-02,  5.1234e-02,  6.8301e-02,\n",
       "         -6.0353e-02, -5.3563e-02, -7.3836e-02,  4.4007e-02,  6.5564e-02,\n",
       "         -6.5315e-02,  2.8460e-02, -2.5883e-02,  1.4184e-02,  8.0203e-02,\n",
       "         -7.9824e-02, -9.4156e-02,  5.4985e-02,  7.5851e-02, -3.3991e-02,\n",
       "          2.1319e-02, -3.2957e-02,  1.0702e-01, -2.6242e-03,  1.0723e-01,\n",
       "          3.0318e-02,  4.1110e-03, -9.3678e-02, -2.2851e-02, -2.4640e-02,\n",
       "         -7.2871e-02,  7.6433e-02, -1.9022e-01, -3.6434e-02,  1.3717e-01],\n",
       "        [ 3.7890e-02,  7.0168e-02, -9.0855e-02,  6.6424e-02, -4.5429e-02,\n",
       "         -5.5615e-02, -1.0286e-01,  9.2074e-03, -1.1636e-02, -2.2935e-02,\n",
       "          8.6468e-02,  2.9843e-02, -1.0275e-01, -3.1200e-02,  5.9485e-02,\n",
       "          8.5958e-03, -1.5521e-02,  1.9868e-02,  8.7264e-02,  8.1197e-02,\n",
       "          1.2720e-01, -2.3281e-02, -1.1109e-02, -2.8612e-05,  8.5473e-03,\n",
       "         -9.9978e-02,  8.3859e-02, -4.0947e-03,  8.3056e-02, -1.4108e-02,\n",
       "         -9.8817e-02,  1.2138e-01,  7.1917e-02, -3.6474e-03,  3.2560e-02,\n",
       "          3.9896e-02,  5.0349e-04, -3.1494e-02, -4.4667e-02,  5.3413e-02,\n",
       "          3.8768e-02,  1.0952e-01,  2.6022e-02, -1.3302e-02, -3.4250e-02,\n",
       "         -1.2320e-02, -2.2463e-02, -1.0446e-03, -5.0203e-02,  2.0068e-01],\n",
       "        [ 3.4114e-02,  1.0204e-01, -9.0270e-02,  2.8761e-02, -3.7776e-02,\n",
       "         -1.2258e-02, -1.4382e-01,  4.7977e-02,  4.9830e-02,  1.5700e-02,\n",
       "          1.3497e-01,  6.7086e-02, -1.2683e-01, -4.5996e-02,  1.0811e-01,\n",
       "          1.4485e-02,  1.6109e-02,  4.6190e-02,  5.7579e-02,  7.8621e-02,\n",
       "          1.5290e-01, -7.5824e-03,  3.9437e-03, -1.0607e-02,  2.2816e-02,\n",
       "         -8.0109e-02,  7.4569e-02, -3.1358e-02,  9.5366e-02,  3.2356e-03,\n",
       "         -4.7866e-02,  8.0441e-02,  1.0015e-01,  2.1060e-02,  5.2461e-02,\n",
       "          6.3485e-02,  1.0180e-03, -5.4314e-02, -3.3123e-03,  3.4116e-02,\n",
       "          7.1588e-02,  1.3165e-01,  1.0242e-02,  1.3733e-02, -2.7791e-02,\n",
       "         -9.8141e-03, -5.0253e-02,  1.8194e-02, -9.1925e-02,  1.7552e-01]],\n",
       "       device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_embedding.narrow(0, 22, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.0229, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "1 tensor(0.0221, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "2 tensor(0.0220, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "3 tensor(0.0217, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "4 tensor(0.0227, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "5 tensor(0.0220, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "6 tensor(0.0214, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "7 tensor(0.0216, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "8 tensor(0.0213, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "9 tensor(0.0206, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "10 tensor(0.0207, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "11 tensor(0.0208, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "12 tensor(0.0221, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "13 tensor(0.0214, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "14 tensor(0.0210, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "15 tensor(0.0206, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "16 tensor(0.0205, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "17 tensor(0.0206, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "18 tensor(0.0203, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "19 tensor(0.0203, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "20 tensor(0.0204, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "21 tensor(0.0202, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "22 tensor(0.0207, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "23 tensor(0.0198, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "24 tensor(0.0198, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "25 tensor(0.0207, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "26 tensor(0.0202, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "27 tensor(0.0202, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "28 tensor(0.0196, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "29 tensor(0.0196, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "30 tensor(0.0197, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "31 tensor(0.0189, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "32 tensor(0.0191, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "33 tensor(0.0195, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "34 tensor(0.0194, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "35 tensor(0.0191, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "36 tensor(0.0191, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "37 tensor(0.0187, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "38 tensor(0.0192, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "39 tensor(0.0184, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "40 tensor(0.0188, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "41 tensor(0.0193, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "42 tensor(0.0184, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "43 tensor(0.0189, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "44 tensor(0.0183, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "45 tensor(0.0190, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "46 tensor(0.0187, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "47 tensor(0.0187, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "48 tensor(0.0178, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "49 tensor(0.0184, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "50 tensor(0.0188, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "51 tensor(0.0178, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "52 tensor(0.0177, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "53 tensor(0.0184, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "54 tensor(0.0185, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "55 tensor(0.0176, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "56 tensor(0.0178, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "57 tensor(0.0175, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "58 tensor(0.0180, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "59 tensor(0.0176, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "60 tensor(0.0176, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "61 tensor(0.0174, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "62 tensor(0.0174, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "63 tensor(0.0172, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "64 tensor(0.0173, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "65 tensor(0.0171, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "66 tensor(0.0169, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "67 tensor(0.0170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "68 tensor(0.0175, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "69 tensor(0.0170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "70 tensor(0.0171, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "71 tensor(0.0167, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "72 tensor(0.0172, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "73 tensor(0.0172, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "74 tensor(0.0163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "75 tensor(0.0163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "76 tensor(0.0171, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "77 tensor(0.0163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "78 tensor(0.0171, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "79 tensor(0.0161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "80 tensor(0.0157, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "81 tensor(0.0167, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "82 tensor(0.0164, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "83 tensor(0.0168, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "84 tensor(0.0163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "85 tensor(0.0165, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "86 tensor(0.0163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "87 tensor(0.0163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "88 tensor(0.0159, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "89 tensor(0.0163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "90 tensor(0.0161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "91 tensor(0.0159, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "92 tensor(0.0158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "93 tensor(0.0159, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "94 tensor(0.0163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "95 tensor(0.0155, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "96 tensor(0.0153, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "97 tensor(0.0152, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "98 tensor(0.0160, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "99 tensor(0.0150, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "100 tensor(0.0152, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "101 tensor(0.0159, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "102 tensor(0.0154, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "103 tensor(0.0156, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "104 tensor(0.0158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "105 tensor(0.0152, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "106 tensor(0.0163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "107 tensor(0.0152, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "108 tensor(0.0153, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "109 tensor(0.0152, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "110 tensor(0.0152, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "111 tensor(0.0150, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "112 tensor(0.0151, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "113 tensor(0.0150, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "114 tensor(0.0148, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "115 tensor(0.0149, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "116 tensor(0.0148, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "117 tensor(0.0149, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "118 tensor(0.0145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "119 tensor(0.0149, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "120 tensor(0.0148, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "121 tensor(0.0151, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "122 tensor(0.0144, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "123 tensor(0.0141, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "124 tensor(0.0133, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "31.26579213142395\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for idx, i in enumerate(data_loader):\n",
    "    X = (MolGraph(i[0]))\n",
    "    Y = (MolGraph(i[1]))\n",
    "    \n",
    "    # create your optimizer\n",
    "    optimizer = torch.optim.SGD(molopt.parameters(), lr=0.01)\n",
    "\n",
    "    # in your training loop:\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    loss = molopt.forward_train(X, Y)\n",
    "    loss.backward()\n",
    "    optimizer.step()    # Does the update\n",
    "\n",
    "    print (idx, loss)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run the implemented function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(agg_func='sum', batch_norm=False, cuda=True, device='cuda:0', dropout_ffn=0.0, dropout_gcn=0.0, ffn_activation='LeakyReLU', init_model='gcn', linear_out=False, n_epochs=50, n_ffn_hidden=100, n_hidden=50, n_labels=1, n_layers=5, output_dir='mol_opt/output', pc_hidden=50)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = get_args()\n",
    "args.n_epochs = 50 \n",
    "args.output_dir = \"mol_opt/output\"\n",
    "args.init_model = \"gcn\"\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52\n",
      "Iter: 0, loss: 0.0015154393622651696\n",
      "Iter: 1, loss: 0.0015764724230393767\n",
      "Iter: 2, loss: 0.00131795194465667\n",
      "Iter: 3, loss: 0.0013846399961039424\n",
      "Iter: 4, loss: 0.0014670012751594186\n",
      "Iter: 5, loss: 0.001498000929132104\n",
      "Iter: 6, loss: 0.0015312510076910257\n",
      "Iter: 7, loss: 0.0016158594517037272\n",
      "Iter: 8, loss: 0.0015854890225455165\n",
      "Iter: 9, loss: 0.001475658849813044\n",
      "Iter: 10, loss: 0.0014154849341139197\n",
      "Iter: 11, loss: 0.0015659993514418602\n",
      "Iter: 12, loss: 0.0015320033999159932\n",
      "Iter: 13, loss: 0.0016959970816969872\n",
      "Iter: 14, loss: 0.0014738845638930798\n",
      "Iter: 15, loss: 0.0014262058539316058\n",
      "Iter: 16, loss: 0.0015164392534643412\n",
      "Iter: 17, loss: 0.0014213065151125193\n",
      "Iter: 18, loss: 0.0013883609790354967\n",
      "Iter: 19, loss: 0.0013576673809438944\n",
      "Iter: 20, loss: 0.0013929243432357907\n",
      "Iter: 21, loss: 0.0014252114342525601\n",
      "Iter: 22, loss: 0.0013886707602068782\n",
      "Iter: 23, loss: 0.001398151507601142\n",
      "Iter: 24, loss: 0.0014596356777474284\n",
      "Iter: 25, loss: 0.0015204607043415308\n",
      "Iter: 26, loss: 0.0014528791652992368\n",
      "Iter: 27, loss: 0.0014598304405808449\n",
      "Iter: 28, loss: 0.0015439719427376986\n",
      "Iter: 29, loss: 0.0015085932100191712\n",
      "Iter: 30, loss: 0.001505170832388103\n",
      "Iter: 31, loss: 0.0013874098658561707\n",
      "Iter: 32, loss: 0.0014268368249759078\n",
      "Iter: 33, loss: 0.0015921022277325392\n",
      "Iter: 34, loss: 0.0014210797380656004\n",
      "Iter: 35, loss: 0.001503275940194726\n",
      "Iter: 36, loss: 0.0014477033400908113\n",
      "Iter: 37, loss: 0.0015087818028405309\n",
      "Iter: 38, loss: 0.0015496413689106703\n",
      "Iter: 39, loss: 0.0015321595128625631\n",
      "Iter: 40, loss: 0.0015391475753858685\n",
      "Iter: 41, loss: 0.0013670100597664714\n",
      "Iter: 42, loss: 0.0014450467424467206\n",
      "Iter: 43, loss: 0.001475660945288837\n",
      "Iter: 44, loss: 0.0014112271601334214\n",
      "Iter: 45, loss: 0.0014536703238263726\n",
      "Iter: 46, loss: 0.0015670537250116467\n",
      "Iter: 47, loss: 0.0013881826307624578\n",
      "Iter: 48, loss: 0.0014257257571443915\n",
      "Iter: 49, loss: 0.001489490270614624\n",
      "Iter: 50, loss: 0.0013691949425265193\n",
      "Iter: 51, loss: 0.0014787983382120728\n",
      "Iter: 52, loss: 0.0014448176370933652\n",
      "Iter: 53, loss: 0.0014258390292525291\n",
      "Iter: 54, loss: 0.0014521724078804255\n",
      "Iter: 55, loss: 0.0014372101286426187\n",
      "Iter: 56, loss: 0.0014346839161589742\n",
      "Iter: 57, loss: 0.0013610554160550237\n",
      "Iter: 58, loss: 0.0014135059900581837\n",
      "Iter: 59, loss: 0.001564716687425971\n",
      "Iter: 60, loss: 0.0014708596281707287\n",
      "Iter: 61, loss: 0.0015593556454405189\n",
      "Iter: 62, loss: 0.0014443766558542848\n",
      "Iter: 63, loss: 0.001567274215631187\n",
      "Iter: 64, loss: 0.0015173651045188308\n",
      "Iter: 65, loss: 0.001433555968105793\n",
      "Iter: 66, loss: 0.0014690839452669024\n",
      "Iter: 67, loss: 0.0014597037807106972\n",
      "Iter: 68, loss: 0.0013787662610411644\n",
      "Iter: 69, loss: 0.001491562114097178\n",
      "Iter: 70, loss: 0.001449905103072524\n",
      "Iter: 71, loss: 0.0013745587784796953\n",
      "Iter: 72, loss: 0.0014587466139346361\n",
      "Iter: 73, loss: 0.001405219780281186\n",
      "Iter: 74, loss: 0.0014343224465847015\n",
      "Iter: 75, loss: 0.0014414220349863172\n",
      "Iter: 76, loss: 0.0014570299535989761\n",
      "Iter: 77, loss: 0.0014095479855313897\n",
      "Iter: 78, loss: 0.0015802144771441817\n",
      "Iter: 79, loss: 0.001475340104661882\n",
      "Iter: 80, loss: 0.0014262418262660503\n",
      "Iter: 81, loss: 0.0014580945717170835\n",
      "Iter: 82, loss: 0.00147646211553365\n",
      "Iter: 83, loss: 0.0013221922563388944\n",
      "Iter: 84, loss: 0.0015220376662909985\n",
      "Iter: 85, loss: 0.0014233568217605352\n",
      "Iter: 86, loss: 0.0015236985636875033\n",
      "Iter: 87, loss: 0.001476228586398065\n",
      "Iter: 88, loss: 0.0013936092145740986\n",
      "Iter: 89, loss: 0.0014831919688731432\n",
      "Iter: 90, loss: 0.001552449306473136\n",
      "Iter: 91, loss: 0.0014297709567472339\n",
      "Iter: 92, loss: 0.0014810251304879785\n",
      "Iter: 93, loss: 0.0014858401846140623\n",
      "Iter: 94, loss: 0.0014495215145871043\n",
      "Iter: 95, loss: 0.0014908680459484458\n",
      "Iter: 96, loss: 0.001419331762008369\n",
      "Iter: 97, loss: 0.0014137264806777239\n",
      "Iter: 98, loss: 0.0015979660674929619\n",
      "Iter: 99, loss: 0.0015907019842416048\n",
      "Iter: 100, loss: 0.0014059733366593719\n",
      "Iter: 101, loss: 0.001487306202761829\n",
      "Iter: 102, loss: 0.001396886189468205\n",
      "Iter: 103, loss: 0.001561500714160502\n",
      "Iter: 104, loss: 0.0015174122527241707\n",
      "Iter: 105, loss: 0.0013802440371364355\n",
      "Iter: 106, loss: 0.0014143545413389802\n",
      "Iter: 107, loss: 0.0014222533209249377\n",
      "Iter: 108, loss: 0.0013834808487445116\n",
      "Iter: 109, loss: 0.001461351988837123\n",
      "Iter: 110, loss: 0.0014550855848938227\n",
      "Iter: 111, loss: 0.0014405346009880304\n",
      "Iter: 112, loss: 0.001475561293773353\n",
      "Iter: 113, loss: 0.0014891480095684528\n",
      "Iter: 114, loss: 0.0014734701253473759\n",
      "Iter: 115, loss: 0.0015191351994872093\n",
      "Iter: 116, loss: 0.0014228459913283587\n",
      "Iter: 117, loss: 0.0015851266216486692\n",
      "Iter: 118, loss: 0.0015029916539788246\n",
      "Iter: 119, loss: 0.0014392853481695056\n",
      "Iter: 120, loss: 0.0013705561868846416\n",
      "Iter: 121, loss: 0.0013462111819535494\n",
      "Iter: 122, loss: 0.001619964255951345\n",
      "Iter: 123, loss: 0.0014481587568297982\n",
      "Iter: 124, loss: 0.0015358058735728264\n",
      "Epoch duration: 28.56106424331665\n",
      "Model saved to: mol_opt/output/model_gcn_52\n",
      "Epoch: 53\n",
      "Iter: 0, loss: 0.001495362026616931\n",
      "Iter: 1, loss: 0.0015572267584502697\n",
      "Iter: 2, loss: 0.0013004939537495375\n",
      "Iter: 3, loss: 0.0013671723427250981\n",
      "Iter: 4, loss: 0.0014476494397968054\n",
      "Iter: 5, loss: 0.0014790156856179237\n",
      "Iter: 6, loss: 0.0015122467884793878\n",
      "Iter: 7, loss: 0.0015962865436449647\n",
      "Iter: 8, loss: 0.001566563150845468\n",
      "Iter: 9, loss: 0.001457589096389711\n",
      "Iter: 10, loss: 0.0013977163471281528\n",
      "Iter: 11, loss: 0.0015468256315216422\n",
      "Iter: 12, loss: 0.0015130892861634493\n",
      "Iter: 13, loss: 0.001675046281889081\n",
      "Iter: 14, loss: 0.0014548476319760084\n",
      "Iter: 15, loss: 0.0014085283037275076\n",
      "Iter: 16, loss: 0.0014975183876231313\n",
      "Iter: 17, loss: 0.00140447320882231\n",
      "Iter: 18, loss: 0.0013701231218874454\n",
      "Iter: 19, loss: 0.0013414521235972643\n",
      "Iter: 20, loss: 0.0013751523802056909\n",
      "Iter: 21, loss: 0.0014075213111937046\n",
      "Iter: 22, loss: 0.0013713670196011662\n",
      "Iter: 23, loss: 0.0013814346166327596\n",
      "Iter: 24, loss: 0.001441526459529996\n",
      "Iter: 25, loss: 0.00150164938531816\n",
      "Iter: 26, loss: 0.0014344347873702645\n",
      "Iter: 27, loss: 0.0014420122606679797\n",
      "Iter: 28, loss: 0.001523943617939949\n",
      "Iter: 29, loss: 0.0014900941168889403\n",
      "Iter: 30, loss: 0.0014857397181913257\n",
      "Iter: 31, loss: 0.0013696715468540788\n",
      "Iter: 32, loss: 0.0014092506607994437\n",
      "Iter: 33, loss: 0.0015722791431471705\n",
      "Iter: 34, loss: 0.0014031169703230262\n",
      "Iter: 35, loss: 0.0014848030405119061\n",
      "Iter: 36, loss: 0.0014298729365691543\n",
      "Iter: 37, loss: 0.0014900056412443519\n",
      "Iter: 38, loss: 0.0015311826718971133\n",
      "Iter: 39, loss: 0.0015126895159482956\n",
      "Iter: 40, loss: 0.0015188471879810095\n",
      "Iter: 41, loss: 0.0013502106303349137\n",
      "Iter: 42, loss: 0.0014275307767093182\n",
      "Iter: 43, loss: 0.0014561560237780213\n",
      "Iter: 44, loss: 0.0013939531054347754\n",
      "Iter: 45, loss: 0.0014362750807777047\n",
      "Iter: 46, loss: 0.0015482879243791103\n",
      "Iter: 47, loss: 0.001369960024021566\n",
      "Iter: 48, loss: 0.001407969743013382\n",
      "Iter: 49, loss: 0.0014705837238579988\n",
      "Iter: 50, loss: 0.0013515229802578688\n",
      "Iter: 51, loss: 0.001460328116081655\n",
      "Iter: 52, loss: 0.0014275875873863697\n",
      "Iter: 53, loss: 0.0014075633371248841\n",
      "Iter: 54, loss: 0.0014334946172311902\n",
      "Iter: 55, loss: 0.0014201531885191798\n",
      "Iter: 56, loss: 0.0014166238252073526\n",
      "Iter: 57, loss: 0.001344159129075706\n",
      "Iter: 58, loss: 0.001395374070852995\n",
      "Iter: 59, loss: 0.0015460819704458117\n",
      "Iter: 60, loss: 0.001452700118534267\n",
      "Iter: 61, loss: 0.0015406295424327254\n",
      "Iter: 62, loss: 0.0014269681414589286\n",
      "Iter: 63, loss: 0.0015482886228710413\n",
      "Iter: 64, loss: 0.0014987444737926126\n",
      "Iter: 65, loss: 0.001416647108271718\n",
      "Iter: 66, loss: 0.0014510570326820016\n",
      "Iter: 67, loss: 0.0014425847912207246\n",
      "Iter: 68, loss: 0.0013618319062516093\n",
      "Iter: 69, loss: 0.001473627402447164\n",
      "Iter: 70, loss: 0.0014319373294711113\n",
      "Iter: 71, loss: 0.0013573620235547423\n",
      "Iter: 72, loss: 0.001440522144548595\n",
      "Iter: 73, loss: 0.001387102180160582\n",
      "Iter: 74, loss: 0.0014160261489450932\n",
      "Iter: 75, loss: 0.0014237852301448584\n",
      "Iter: 76, loss: 0.0014384196838364005\n",
      "Iter: 77, loss: 0.0013925308594480157\n",
      "Iter: 78, loss: 0.0015618912875652313\n",
      "Iter: 79, loss: 0.0014566379832103848\n",
      "Iter: 80, loss: 0.00140997557900846\n",
      "Iter: 81, loss: 0.001440353342331946\n",
      "Iter: 82, loss: 0.0014589279890060425\n",
      "Iter: 83, loss: 0.001305430312640965\n",
      "Iter: 84, loss: 0.0015037611592561007\n",
      "Iter: 85, loss: 0.0014049520250409842\n",
      "Iter: 86, loss: 0.0015042531304061413\n",
      "Iter: 87, loss: 0.0014585173921659589\n",
      "Iter: 88, loss: 0.0013757364358752966\n",
      "Iter: 89, loss: 0.0014642135938629508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 90, loss: 0.0015334351919591427\n",
      "Iter: 91, loss: 0.0014113035285845399\n",
      "Iter: 92, loss: 0.0014628315111622214\n",
      "Iter: 93, loss: 0.0014688977971673012\n",
      "Iter: 94, loss: 0.0014318919274955988\n",
      "Iter: 95, loss: 0.001473608543165028\n",
      "Iter: 96, loss: 0.0014016531640663743\n",
      "Iter: 97, loss: 0.0013961315853521228\n",
      "Iter: 98, loss: 0.0015785063151270151\n",
      "Iter: 99, loss: 0.0015707776183262467\n",
      "Iter: 100, loss: 0.0013885425869375467\n",
      "Iter: 101, loss: 0.001468927483074367\n",
      "Iter: 102, loss: 0.0013795050326734781\n",
      "Iter: 103, loss: 0.0015419666888192296\n",
      "Iter: 104, loss: 0.0014987076865509152\n",
      "Iter: 105, loss: 0.0013632476329803467\n",
      "Iter: 106, loss: 0.0013969521969556808\n",
      "Iter: 107, loss: 0.0014050210593268275\n",
      "Iter: 108, loss: 0.0013667945750057697\n",
      "Iter: 109, loss: 0.0014434163458645344\n",
      "Iter: 110, loss: 0.001437335624359548\n",
      "Iter: 111, loss: 0.0014224598417058587\n",
      "Iter: 112, loss: 0.0014573378721252084\n",
      "Iter: 113, loss: 0.0014713676646351814\n",
      "Iter: 114, loss: 0.001455248799175024\n",
      "Iter: 115, loss: 0.0015000849962234497\n",
      "Iter: 116, loss: 0.0014048328157514334\n",
      "Iter: 117, loss: 0.0015654473099857569\n",
      "Iter: 118, loss: 0.001484659849666059\n",
      "Iter: 119, loss: 0.0014212814858183265\n",
      "Iter: 120, loss: 0.0013544460525736213\n",
      "Iter: 121, loss: 0.0013301200233399868\n",
      "Iter: 122, loss: 0.0016000989126041532\n",
      "Iter: 123, loss: 0.0014300575712695718\n",
      "Iter: 124, loss: 0.001516423188149929\n",
      "Epoch duration: 28.603351354599\n",
      "Model saved to: mol_opt/output/model_gcn_53\n",
      "Epoch: 54\n",
      "Iter: 0, loss: 0.0014766367385163903\n",
      "Iter: 1, loss: 0.001538741635158658\n",
      "Iter: 2, loss: 0.001283495221287012\n",
      "Iter: 3, loss: 0.001350137870758772\n",
      "Iter: 4, loss: 0.0014288175152614713\n",
      "Iter: 5, loss: 0.0014602438313886523\n",
      "Iter: 6, loss: 0.0014934567734599113\n",
      "Iter: 7, loss: 0.0015772603219375014\n",
      "Iter: 8, loss: 0.0015483268070966005\n",
      "Iter: 9, loss: 0.001439897809177637\n",
      "Iter: 10, loss: 0.001380446250550449\n",
      "Iter: 11, loss: 0.0015283358516171575\n",
      "Iter: 12, loss: 0.001494720228947699\n",
      "Iter: 13, loss: 0.001654636231251061\n",
      "Iter: 14, loss: 0.0014366642571985722\n",
      "Iter: 15, loss: 0.0013913708971813321\n",
      "Iter: 16, loss: 0.0014791353605687618\n",
      "Iter: 17, loss: 0.0013881627237424254\n",
      "Iter: 18, loss: 0.0013526349794119596\n",
      "Iter: 19, loss: 0.001325604971498251\n",
      "Iter: 20, loss: 0.0013577735517174006\n",
      "Iter: 21, loss: 0.001390553661622107\n",
      "Iter: 22, loss: 0.0013549801660701632\n",
      "Iter: 23, loss: 0.0013651930494233966\n",
      "Iter: 24, loss: 0.0014241538010537624\n",
      "Iter: 25, loss: 0.0014832186279818416\n",
      "Iter: 26, loss: 0.0014165343018248677\n",
      "Iter: 27, loss: 0.0014246333157643676\n",
      "Iter: 28, loss: 0.001504559419117868\n",
      "Iter: 29, loss: 0.0014721360057592392\n",
      "Iter: 30, loss: 0.0014668828807771206\n",
      "Iter: 31, loss: 0.0013524498790502548\n",
      "Iter: 32, loss: 0.0013923582155257463\n",
      "Iter: 33, loss: 0.0015527618816122413\n",
      "Iter: 34, loss: 0.0013856615405529737\n",
      "Iter: 35, loss: 0.0014664643676951528\n",
      "Iter: 36, loss: 0.0014123666333034635\n",
      "Iter: 37, loss: 0.0014718340244144201\n",
      "Iter: 38, loss: 0.0015134033747017384\n",
      "Iter: 39, loss: 0.0014937776140868664\n",
      "Iter: 40, loss: 0.0014996733516454697\n",
      "Iter: 41, loss: 0.0013325903564691544\n",
      "Iter: 42, loss: 0.0014104372821748257\n",
      "Iter: 43, loss: 0.0014375237515196204\n",
      "Iter: 44, loss: 0.0013770235236734152\n",
      "Iter: 45, loss: 0.0014193260576575994\n",
      "Iter: 46, loss: 0.0015300503000617027\n",
      "Iter: 47, loss: 0.0013531094882637262\n",
      "Iter: 48, loss: 0.0013904519146308303\n",
      "Iter: 49, loss: 0.0014523770660161972\n",
      "Iter: 50, loss: 0.0013343584723770618\n",
      "Iter: 51, loss: 0.0014424980618059635\n",
      "Iter: 52, loss: 0.0014108717441558838\n",
      "Iter: 53, loss: 0.001390078803524375\n",
      "Iter: 54, loss: 0.0014156519901007414\n",
      "Iter: 55, loss: 0.0014032382750883698\n",
      "Iter: 56, loss: 0.0013994758483022451\n",
      "Iter: 57, loss: 0.0013277591206133366\n",
      "Iter: 58, loss: 0.0013778567081317306\n",
      "Iter: 59, loss: 0.0015275346813723445\n",
      "Iter: 60, loss: 0.0014351099962368608\n",
      "Iter: 61, loss: 0.001522549195215106\n",
      "Iter: 62, loss: 0.0014097257517278194\n",
      "Iter: 63, loss: 0.0015298627549782395\n",
      "Iter: 64, loss: 0.0014806968392804265\n",
      "Iter: 65, loss: 0.001400495762936771\n",
      "Iter: 66, loss: 0.0014333920553326607\n",
      "Iter: 67, loss: 0.0014254419365897775\n",
      "Iter: 68, loss: 0.0013453519204631448\n",
      "Iter: 69, loss: 0.001455902005545795\n",
      "Iter: 70, loss: 0.001414692960679531\n",
      "Iter: 71, loss: 0.0013407415244728327\n",
      "Iter: 72, loss: 0.0014227349311113358\n",
      "Iter: 73, loss: 0.0013695608358830214\n",
      "Iter: 74, loss: 0.001398203894495964\n",
      "Iter: 75, loss: 0.0014066932490095496\n",
      "Iter: 76, loss: 0.001419861102476716\n",
      "Iter: 77, loss: 0.001376406871713698\n",
      "Iter: 78, loss: 0.0015430378261953592\n",
      "Iter: 79, loss: 0.0014384539099410176\n",
      "Iter: 80, loss: 0.001393646583892405\n",
      "Iter: 81, loss: 0.001423181383870542\n",
      "Iter: 82, loss: 0.0014418920036405325\n",
      "Iter: 83, loss: 0.001289280829951167\n",
      "Iter: 84, loss: 0.0014858779031783342\n",
      "Iter: 85, loss: 0.0013870139373466372\n",
      "Iter: 86, loss: 0.0014856649795547128\n",
      "Iter: 87, loss: 0.0014411676675081253\n",
      "Iter: 88, loss: 0.001358333509415388\n",
      "Iter: 89, loss: 0.001445421134121716\n",
      "Iter: 90, loss: 0.00151486333925277\n",
      "Iter: 91, loss: 0.0013933108421042562\n",
      "Iter: 92, loss: 0.0014447536086663604\n",
      "Iter: 93, loss: 0.0014523416757583618\n",
      "Iter: 94, loss: 0.0014147120527923107\n",
      "Iter: 95, loss: 0.001456610974855721\n",
      "Iter: 96, loss: 0.001384333590976894\n",
      "Iter: 97, loss: 0.0013790844241157174\n",
      "Iter: 98, loss: 0.0015596050070598722\n",
      "Iter: 99, loss: 0.0015519310254603624\n",
      "Iter: 100, loss: 0.0013716132380068302\n",
      "Iter: 101, loss: 0.0014508923050016165\n",
      "Iter: 102, loss: 0.00136244622990489\n",
      "Iter: 103, loss: 0.001523476094007492\n",
      "Iter: 104, loss: 0.0014805274549871683\n",
      "Iter: 105, loss: 0.001346553210169077\n",
      "Iter: 106, loss: 0.0013799156295135617\n",
      "Iter: 107, loss: 0.0013889952097088099\n",
      "Iter: 108, loss: 0.0013510294957086444\n",
      "Iter: 109, loss: 0.0014259838499128819\n",
      "Iter: 110, loss: 0.0014201159356161952\n",
      "Iter: 111, loss: 0.0014048897428438067\n",
      "Iter: 112, loss: 0.0014395040925592184\n",
      "Iter: 113, loss: 0.0014540626434609294\n",
      "Iter: 114, loss: 0.001437804545275867\n",
      "Iter: 115, loss: 0.0014815723989158869\n",
      "Iter: 116, loss: 0.0013872450217604637\n",
      "Iter: 117, loss: 0.0015463740564882755\n",
      "Iter: 118, loss: 0.0014667232753708959\n",
      "Iter: 119, loss: 0.0014039365341886878\n",
      "Iter: 120, loss: 0.0013387047220021486\n",
      "Iter: 121, loss: 0.001314275898039341\n",
      "Iter: 122, loss: 0.0015806755982339382\n",
      "Iter: 123, loss: 0.0014125050511211157\n",
      "Iter: 124, loss: 0.0014985076850280166\n",
      "Epoch duration: 28.640769958496094\n",
      "Model saved to: mol_opt/output/model_gcn_54\n",
      "Epoch: 55\n",
      "Iter: 0, loss: 0.0014576488174498081\n",
      "Iter: 1, loss: 0.0015207150718197227\n",
      "Iter: 2, loss: 0.001266991370357573\n",
      "Iter: 3, loss: 0.001333423308096826\n",
      "Iter: 4, loss: 0.0014110695337876678\n",
      "Iter: 5, loss: 0.001442478853277862\n",
      "Iter: 6, loss: 0.0014753304421901703\n",
      "Iter: 7, loss: 0.0015585663495585322\n",
      "Iter: 8, loss: 0.0015303351683542132\n",
      "Iter: 9, loss: 0.0014227349311113358\n",
      "Iter: 10, loss: 0.0013637171359732747\n",
      "Iter: 11, loss: 0.001510299858637154\n",
      "Iter: 12, loss: 0.0014768869150429964\n",
      "Iter: 13, loss: 0.001634707790799439\n",
      "Iter: 14, loss: 0.0014186364132910967\n",
      "Iter: 15, loss: 0.0013746799668297172\n",
      "Iter: 16, loss: 0.0014612096128985286\n",
      "Iter: 17, loss: 0.0013723248848691583\n",
      "Iter: 18, loss: 0.0013357873540371656\n",
      "Iter: 19, loss: 0.0013103423407301307\n",
      "Iter: 20, loss: 0.0013411204563453794\n",
      "Iter: 21, loss: 0.001373739680275321\n",
      "Iter: 22, loss: 0.0013385133352130651\n",
      "Iter: 23, loss: 0.0013494256418198347\n",
      "Iter: 24, loss: 0.0014072112971916795\n",
      "Iter: 25, loss: 0.0014651078963652253\n",
      "Iter: 26, loss: 0.0013979894574731588\n",
      "Iter: 27, loss: 0.001408319571055472\n",
      "Iter: 28, loss: 0.0014856731286272407\n",
      "Iter: 29, loss: 0.001454887562431395\n",
      "Iter: 30, loss: 0.00144864188041538\n",
      "Iter: 31, loss: 0.0013357538264244795\n",
      "Iter: 32, loss: 0.0013756649568676949\n",
      "Iter: 33, loss: 0.0015334467170760036\n",
      "Iter: 34, loss: 0.0013684853911399841\n",
      "Iter: 35, loss: 0.0014493721537292004\n",
      "Iter: 36, loss: 0.0013954995665699244\n",
      "Iter: 37, loss: 0.0014543015277013183\n",
      "Iter: 38, loss: 0.0014962895074859262\n",
      "Iter: 39, loss: 0.0014756345190107822\n",
      "Iter: 40, loss: 0.001481047598645091\n",
      "Iter: 41, loss: 0.00131663354113698\n",
      "Iter: 42, loss: 0.0013937613693997264\n",
      "Iter: 43, loss: 0.0014195297844707966\n",
      "Iter: 44, loss: 0.00136058300267905\n",
      "Iter: 45, loss: 0.0014028078876435757\n",
      "Iter: 46, loss: 0.0015123218763619661\n",
      "Iter: 47, loss: 0.001336542540229857\n",
      "Iter: 48, loss: 0.0013736741384491324\n",
      "Iter: 49, loss: 0.00143473653588444\n",
      "Iter: 50, loss: 0.0013174897758290172\n",
      "Iter: 51, loss: 0.0014251210959628224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 52, loss: 0.0013945946702733636\n",
      "Iter: 53, loss: 0.001372966798953712\n",
      "Iter: 54, loss: 0.0013984751421958208\n",
      "Iter: 55, loss: 0.001386793446727097\n",
      "Iter: 56, loss: 0.0013823668705299497\n",
      "Iter: 57, loss: 0.001311515225097537\n",
      "Iter: 58, loss: 0.0013607499422505498\n",
      "Iter: 59, loss: 0.0015097714494913816\n",
      "Iter: 60, loss: 0.0014180183643475175\n",
      "Iter: 61, loss: 0.0015045898035168648\n",
      "Iter: 62, loss: 0.001392840058542788\n",
      "Iter: 63, loss: 0.0015114311827346683\n",
      "Iter: 64, loss: 0.0014632610836997628\n",
      "Iter: 65, loss: 0.001384757924824953\n",
      "Iter: 66, loss: 0.001416324987076223\n",
      "Iter: 67, loss: 0.0014092711498960853\n",
      "Iter: 68, loss: 0.0013293430674821138\n",
      "Iter: 69, loss: 0.001438646111637354\n",
      "Iter: 70, loss: 0.0013979047071188688\n",
      "Iter: 71, loss: 0.001324303331784904\n",
      "Iter: 72, loss: 0.0014052157057449222\n",
      "Iter: 73, loss: 0.0013524742098525167\n",
      "Iter: 74, loss: 0.0013813837431371212\n",
      "Iter: 75, loss: 0.0013901236234232783\n",
      "Iter: 76, loss: 0.0014019499067217112\n",
      "Iter: 77, loss: 0.0013603412080556154\n",
      "Iter: 78, loss: 0.0015252182492986321\n",
      "Iter: 79, loss: 0.0014207933563739061\n",
      "Iter: 80, loss: 0.0013777558924630284\n",
      "Iter: 81, loss: 0.0014065399300307035\n",
      "Iter: 82, loss: 0.0014252415858209133\n",
      "Iter: 83, loss: 0.00127347931265831\n",
      "Iter: 84, loss: 0.0014685951173305511\n",
      "Iter: 85, loss: 0.0013696213718503714\n",
      "Iter: 86, loss: 0.0014675429556518793\n",
      "Iter: 87, loss: 0.0014242813922464848\n",
      "Iter: 88, loss: 0.0013416096335276961\n",
      "Iter: 89, loss: 0.0014273264678195119\n",
      "Iter: 90, loss: 0.0014968906762078404\n",
      "Iter: 91, loss: 0.0013758366694673896\n",
      "Iter: 92, loss: 0.0014275877038016915\n",
      "Iter: 93, loss: 0.0014364771777763963\n",
      "Iter: 94, loss: 0.0013979630311951041\n",
      "Iter: 95, loss: 0.0014402300585061312\n",
      "Iter: 96, loss: 0.0013676367234438658\n",
      "Iter: 97, loss: 0.001362635288387537\n",
      "Iter: 98, loss: 0.0015411007916554809\n",
      "Iter: 99, loss: 0.0015335340285673738\n",
      "Iter: 100, loss: 0.0013539566425606608\n",
      "Iter: 101, loss: 0.0014334062580019236\n",
      "Iter: 102, loss: 0.0013460807967931032\n",
      "Iter: 103, loss: 0.0015056205447763205\n",
      "Iter: 104, loss: 0.0014624560717493296\n",
      "Iter: 105, loss: 0.0013303437735885382\n",
      "Iter: 106, loss: 0.0013637150404974818\n",
      "Iter: 107, loss: 0.0013732985826209188\n",
      "Iter: 108, loss: 0.00133507139980793\n",
      "Iter: 109, loss: 0.0014087562449276447\n",
      "Iter: 110, loss: 0.0014032330363988876\n",
      "Iter: 111, loss: 0.0013878612080588937\n",
      "Iter: 112, loss: 0.0014222603058442473\n",
      "Iter: 113, loss: 0.001437376020476222\n",
      "Iter: 114, loss: 0.0014202960301190615\n",
      "Iter: 115, loss: 0.0014634848339483142\n",
      "Iter: 116, loss: 0.0013698148541152477\n",
      "Iter: 117, loss: 0.0015277680940926075\n",
      "Iter: 118, loss: 0.0014494251227006316\n",
      "Iter: 119, loss: 0.0013875856529921293\n",
      "Iter: 120, loss: 0.0013234398793429136\n",
      "Iter: 121, loss: 0.001298881834372878\n",
      "Iter: 122, loss: 0.0015617631142958999\n",
      "Iter: 123, loss: 0.0013947796542197466\n",
      "Iter: 124, loss: 0.0014809154672548175\n",
      "Epoch duration: 28.898289442062378\n",
      "Model saved to: mol_opt/output/model_gcn_55\n",
      "Epoch: 56\n",
      "Iter: 0, loss: 0.0014393009478226304\n",
      "Iter: 1, loss: 0.0015028653433546424\n",
      "Iter: 2, loss: 0.001251152134500444\n",
      "Iter: 3, loss: 0.001317245769314468\n",
      "Iter: 4, loss: 0.0013935987371951342\n",
      "Iter: 5, loss: 0.0014249682426452637\n",
      "Iter: 6, loss: 0.001457689912058413\n",
      "Iter: 7, loss: 0.001540413941256702\n",
      "Iter: 8, loss: 0.0015129869570955634\n",
      "Iter: 9, loss: 0.0014060541288927197\n",
      "Iter: 10, loss: 0.0013471725396811962\n",
      "Iter: 11, loss: 0.0014924637507647276\n",
      "Iter: 12, loss: 0.0014595389366149902\n",
      "Iter: 13, loss: 0.0016154180048033595\n",
      "Iter: 14, loss: 0.00140113546513021\n",
      "Iter: 15, loss: 0.0013578799553215504\n",
      "Iter: 16, loss: 0.0014436603523790836\n",
      "Iter: 17, loss: 0.001356914290226996\n",
      "Iter: 18, loss: 0.0013192737242206931\n",
      "Iter: 19, loss: 0.0012954483972862363\n",
      "Iter: 20, loss: 0.0013249315088614821\n",
      "Iter: 21, loss: 0.0013574932236224413\n",
      "Iter: 22, loss: 0.0013219433603808284\n",
      "Iter: 23, loss: 0.0013342667371034622\n",
      "Iter: 24, loss: 0.0013906634412705898\n",
      "Iter: 25, loss: 0.0014479795936495066\n",
      "Iter: 26, loss: 0.001380939967930317\n",
      "Iter: 27, loss: 0.001391835743561387\n",
      "Iter: 28, loss: 0.0014672648394480348\n",
      "Iter: 29, loss: 0.0014378329506143928\n",
      "Iter: 30, loss: 0.00143083231523633\n",
      "Iter: 31, loss: 0.0013195048086345196\n",
      "Iter: 32, loss: 0.001359140151180327\n",
      "Iter: 33, loss: 0.0015146654332056642\n",
      "Iter: 34, loss: 0.0013519772328436375\n",
      "Iter: 35, loss: 0.0014322801725938916\n",
      "Iter: 36, loss: 0.0013785508926957846\n",
      "Iter: 37, loss: 0.001436966354958713\n",
      "Iter: 38, loss: 0.0014792560832574964\n",
      "Iter: 39, loss: 0.0014576580142602324\n",
      "Iter: 40, loss: 0.001463114283978939\n",
      "Iter: 41, loss: 0.0013011094415560365\n",
      "Iter: 42, loss: 0.0013774170074611902\n",
      "Iter: 43, loss: 0.0014017492067068815\n",
      "Iter: 44, loss: 0.0013446072116494179\n",
      "Iter: 45, loss: 0.0013868719106540084\n",
      "Iter: 46, loss: 0.0014950792538002133\n",
      "Iter: 47, loss: 0.0013204519636929035\n",
      "Iter: 48, loss: 0.0013574426993727684\n",
      "Iter: 49, loss: 0.0014173489762470126\n",
      "Iter: 50, loss: 0.0013011139817535877\n",
      "Iter: 51, loss: 0.001407984527759254\n",
      "Iter: 52, loss: 0.0013776551932096481\n",
      "Iter: 53, loss: 0.0013568000867962837\n",
      "Iter: 54, loss: 0.0013809377560392022\n",
      "Iter: 55, loss: 0.0013705989113077521\n",
      "Iter: 56, loss: 0.0013657062081620097\n",
      "Iter: 57, loss: 0.0012959932209923863\n",
      "Iter: 58, loss: 0.0013440002221614122\n",
      "Iter: 59, loss: 0.0014924730639904737\n",
      "Iter: 60, loss: 0.0014014190528541803\n",
      "Iter: 61, loss: 0.001487045083194971\n",
      "Iter: 62, loss: 0.0013766508782282472\n",
      "Iter: 63, loss: 0.0014937997329980135\n",
      "Iter: 64, loss: 0.0014461284736171365\n",
      "Iter: 65, loss: 0.0013692561769858003\n",
      "Iter: 66, loss: 0.0013993424363434315\n",
      "Iter: 67, loss: 0.0013930278364568949\n",
      "Iter: 68, loss: 0.0013137842761352658\n",
      "Iter: 69, loss: 0.001421889872290194\n",
      "Iter: 70, loss: 0.0013812145916745067\n",
      "Iter: 71, loss: 0.0013082836521789432\n",
      "Iter: 72, loss: 0.0013884494546800852\n",
      "Iter: 73, loss: 0.001335919019766152\n",
      "Iter: 74, loss: 0.0013648547464981675\n",
      "Iter: 75, loss: 0.001374165411107242\n",
      "Iter: 76, loss: 0.0013842317275702953\n",
      "Iter: 77, loss: 0.00134478195104748\n",
      "Iter: 78, loss: 0.0015079131117090583\n",
      "Iter: 79, loss: 0.00140362826641649\n",
      "Iter: 80, loss: 0.0013622683472931385\n",
      "Iter: 81, loss: 0.001390253659337759\n",
      "Iter: 82, loss: 0.0014091046759858727\n",
      "Iter: 83, loss: 0.0012581389164552093\n",
      "Iter: 84, loss: 0.0014517272356897593\n",
      "Iter: 85, loss: 0.0013527267146855593\n",
      "Iter: 86, loss: 0.0014499701792374253\n",
      "Iter: 87, loss: 0.001407985226251185\n",
      "Iter: 88, loss: 0.001325066783465445\n",
      "Iter: 89, loss: 0.0014095642836764455\n",
      "Iter: 90, loss: 0.0014792592264711857\n",
      "Iter: 91, loss: 0.0013588323490694165\n",
      "Iter: 92, loss: 0.001410778728313744\n",
      "Iter: 93, loss: 0.0014212272362783551\n",
      "Iter: 94, loss: 0.0013815186684951186\n",
      "Iter: 95, loss: 0.0014245827915146947\n",
      "Iter: 96, loss: 0.0013513746671378613\n",
      "Iter: 97, loss: 0.0013464551884680986\n",
      "Iter: 98, loss: 0.0015232004225254059\n",
      "Iter: 99, loss: 0.001515691285021603\n",
      "Iter: 100, loss: 0.0013377309078350663\n",
      "Iter: 101, loss: 0.0014163714367896318\n",
      "Iter: 102, loss: 0.0013301926665008068\n",
      "Iter: 103, loss: 0.0014877822250127792\n",
      "Iter: 104, loss: 0.0014452553587034345\n",
      "Iter: 105, loss: 0.00131451606284827\n",
      "Iter: 106, loss: 0.001347687910310924\n",
      "Iter: 107, loss: 0.0013580331578850746\n",
      "Iter: 108, loss: 0.0013199342647567391\n",
      "Iter: 109, loss: 0.0013922507641837\n",
      "Iter: 110, loss: 0.001386844553053379\n",
      "Iter: 111, loss: 0.0013711652718484402\n",
      "Iter: 112, loss: 0.0014054762432351708\n",
      "Iter: 113, loss: 0.0014210640219971538\n",
      "Iter: 114, loss: 0.0014034804189577699\n",
      "Iter: 115, loss: 0.0014454638585448265\n",
      "Iter: 116, loss: 0.0013530865544453263\n",
      "Iter: 117, loss: 0.0015097239520400763\n",
      "Iter: 118, loss: 0.0014325804077088833\n",
      "Iter: 119, loss: 0.0013710842467844486\n",
      "Iter: 120, loss: 0.0013083795784041286\n",
      "Iter: 121, loss: 0.0012840041890740395\n",
      "Iter: 122, loss: 0.0015431155916303396\n",
      "Iter: 123, loss: 0.001377980224788189\n",
      "Iter: 124, loss: 0.0014638201100751758\n",
      "Epoch duration: 28.630626678466797\n",
      "Model saved to: mol_opt/output/model_gcn_56\n",
      "Epoch: 57\n",
      "Iter: 0, loss: 0.0014215303817763925\n",
      "Iter: 1, loss: 0.0014857256319373846\n",
      "Iter: 2, loss: 0.0012354932259768248\n",
      "Iter: 3, loss: 0.0013014262076467276\n",
      "Iter: 4, loss: 0.001376622123643756\n",
      "Iter: 5, loss: 0.0014079370303079486\n",
      "Iter: 6, loss: 0.001440340536646545\n",
      "Iter: 7, loss: 0.001522723468951881\n",
      "Iter: 8, loss: 0.0014963361900299788\n",
      "Iter: 9, loss: 0.0013893876457586884\n",
      "Iter: 10, loss: 0.0013311994262039661\n",
      "Iter: 11, loss: 0.001475268043577671\n",
      "Iter: 12, loss: 0.0014428499853238463\n",
      "Iter: 13, loss: 0.001596627989783883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 14, loss: 0.0013842449989169836\n",
      "Iter: 15, loss: 0.0013421218609437346\n",
      "Iter: 16, loss: 0.0014265502104535699\n",
      "Iter: 17, loss: 0.0013419602764770389\n",
      "Iter: 18, loss: 0.0013031678972765803\n",
      "Iter: 19, loss: 0.0012811803026124835\n",
      "Iter: 20, loss: 0.0013091959990561008\n",
      "Iter: 21, loss: 0.0013417351292446256\n",
      "Iter: 22, loss: 0.0013064382364973426\n",
      "Iter: 23, loss: 0.0013196403160691261\n",
      "Iter: 24, loss: 0.0013739675050601363\n",
      "Iter: 25, loss: 0.0014310695696622133\n",
      "Iter: 26, loss: 0.001363894552923739\n",
      "Iter: 27, loss: 0.0013756946427747607\n",
      "Iter: 28, loss: 0.0014493318740278482\n",
      "Iter: 29, loss: 0.0014212829992175102\n",
      "Iter: 30, loss: 0.0014134942321106791\n",
      "Iter: 31, loss: 0.0013037865282967687\n",
      "Iter: 32, loss: 0.001343313604593277\n",
      "Iter: 33, loss: 0.001496393233537674\n",
      "Iter: 34, loss: 0.0013355470728129148\n",
      "Iter: 35, loss: 0.0014161051949486136\n",
      "Iter: 36, loss: 0.001361941103823483\n",
      "Iter: 37, loss: 0.0014197221025824547\n",
      "Iter: 38, loss: 0.0014626448974013329\n",
      "Iter: 39, loss: 0.0014403447275981307\n",
      "Iter: 40, loss: 0.0014454297488555312\n",
      "Iter: 41, loss: 0.0012855102540925145\n",
      "Iter: 42, loss: 0.0013615958159789443\n",
      "Iter: 43, loss: 0.0013845313806086779\n",
      "Iter: 44, loss: 0.0013292300282046199\n",
      "Iter: 45, loss: 0.0013710956554859877\n",
      "Iter: 46, loss: 0.001478089950978756\n",
      "Iter: 47, loss: 0.0013048241380602121\n",
      "Iter: 48, loss: 0.0013415812281891704\n",
      "Iter: 49, loss: 0.0014005635166540742\n",
      "Iter: 50, loss: 0.0012852592626586556\n",
      "Iter: 51, loss: 0.0013914649607613683\n",
      "Iter: 52, loss: 0.0013623114209622145\n",
      "Iter: 53, loss: 0.0013405949575826526\n",
      "Iter: 54, loss: 0.0013647424057126045\n",
      "Iter: 55, loss: 0.0013551480369642377\n",
      "Iter: 56, loss: 0.0013491297140717506\n",
      "Iter: 57, loss: 0.0012808962492272258\n",
      "Iter: 58, loss: 0.001327737933024764\n",
      "Iter: 59, loss: 0.0014761871425434947\n",
      "Iter: 60, loss: 0.0013852646807208657\n",
      "Iter: 61, loss: 0.001469621667638421\n",
      "Iter: 62, loss: 0.0013609351590275764\n",
      "Iter: 63, loss: 0.001476630917750299\n",
      "Iter: 64, loss: 0.0014295451110228896\n",
      "Iter: 65, loss: 0.0013539764331653714\n",
      "Iter: 66, loss: 0.0013830559328198433\n",
      "Iter: 67, loss: 0.0013772975653409958\n",
      "Iter: 68, loss: 0.0012986742658540606\n",
      "Iter: 69, loss: 0.0014055893989279866\n",
      "Iter: 70, loss: 0.0013651058543473482\n",
      "Iter: 71, loss: 0.0012926701456308365\n",
      "Iter: 72, loss: 0.0013720367569476366\n",
      "Iter: 73, loss: 0.0013196419458836317\n",
      "Iter: 74, loss: 0.0013482833746820688\n",
      "Iter: 75, loss: 0.0013584543485194445\n",
      "Iter: 76, loss: 0.0013671454507857561\n",
      "Iter: 77, loss: 0.001329486258327961\n",
      "Iter: 78, loss: 0.001491048256866634\n",
      "Iter: 79, loss: 0.0013869808753952384\n",
      "Iter: 80, loss: 0.0013472900027409196\n",
      "Iter: 81, loss: 0.0013744330499321222\n",
      "Iter: 82, loss: 0.0013934809248894453\n",
      "Iter: 83, loss: 0.0012433124938979745\n",
      "Iter: 84, loss: 0.00143547507468611\n",
      "Iter: 85, loss: 0.001336297020316124\n",
      "Iter: 86, loss: 0.0014329110272228718\n",
      "Iter: 87, loss: 0.0013920085038989782\n",
      "Iter: 88, loss: 0.001309042563661933\n",
      "Iter: 89, loss: 0.0013923731166869402\n",
      "Iter: 90, loss: 0.0014621790032833815\n",
      "Iter: 91, loss: 0.0013422872871160507\n",
      "Iter: 92, loss: 0.0013944278471171856\n",
      "Iter: 93, loss: 0.0014061606489121914\n",
      "Iter: 94, loss: 0.00136566162109375\n",
      "Iter: 95, loss: 0.0014084314461797476\n",
      "Iter: 96, loss: 0.0013355512637645006\n",
      "Iter: 97, loss: 0.0013306982582435012\n",
      "Iter: 98, loss: 0.0015056731645017862\n",
      "Iter: 99, loss: 0.0014982928987592459\n",
      "Iter: 100, loss: 0.0013220235705375671\n",
      "Iter: 101, loss: 0.0013999277725815773\n",
      "Iter: 102, loss: 0.0013147116405889392\n",
      "Iter: 103, loss: 0.0014704200439155102\n",
      "Iter: 104, loss: 0.0014285650104284286\n",
      "Iter: 105, loss: 0.0012987978989258409\n",
      "Iter: 106, loss: 0.0013319476274773479\n",
      "Iter: 107, loss: 0.001342981355264783\n",
      "Iter: 108, loss: 0.0013050143606960773\n",
      "Iter: 109, loss: 0.0013761891750618815\n",
      "Iter: 110, loss: 0.0013704880839213729\n",
      "Iter: 111, loss: 0.001354925218038261\n",
      "Iter: 112, loss: 0.0013891495764255524\n",
      "Iter: 113, loss: 0.001405123621225357\n",
      "Iter: 114, loss: 0.0013870325637981296\n",
      "Iter: 115, loss: 0.0014284612843766809\n",
      "Iter: 116, loss: 0.0013377016875892878\n",
      "Iter: 117, loss: 0.0014922528062015772\n",
      "Iter: 118, loss: 0.0014160432619974017\n",
      "Iter: 119, loss: 0.0013550700386986136\n",
      "Iter: 120, loss: 0.0012934403494000435\n",
      "Iter: 121, loss: 0.0012693241005763412\n",
      "Iter: 122, loss: 0.0015251075383275747\n",
      "Iter: 123, loss: 0.0013618632219731808\n",
      "Iter: 124, loss: 0.0014471940230578184\n",
      "Epoch duration: 28.66850185394287\n",
      "Model saved to: mol_opt/output/model_gcn_57\n",
      "Epoch: 58\n",
      "Iter: 0, loss: 0.0014041102258488536\n",
      "Iter: 1, loss: 0.0014690090902149677\n",
      "Iter: 2, loss: 0.001220271224156022\n",
      "Iter: 3, loss: 0.0012863033916801214\n",
      "Iter: 4, loss: 0.0013601352693513036\n",
      "Iter: 5, loss: 0.0013913135044276714\n",
      "Iter: 6, loss: 0.001423470675945282\n",
      "Iter: 7, loss: 0.0015055470867082477\n",
      "Iter: 8, loss: 0.0014800071949139237\n",
      "Iter: 9, loss: 0.0013726598117500544\n",
      "Iter: 10, loss: 0.0013156570494174957\n",
      "Iter: 11, loss: 0.0014585069147869945\n",
      "Iter: 12, loss: 0.001426379894837737\n",
      "Iter: 13, loss: 0.0015783080598339438\n",
      "Iter: 14, loss: 0.0013677008682861924\n",
      "Iter: 15, loss: 0.001326672499999404\n",
      "Iter: 16, loss: 0.0014099428663030267\n",
      "Iter: 17, loss: 0.0013273503864184022\n",
      "Iter: 18, loss: 0.0012874846579506993\n",
      "Iter: 19, loss: 0.0012670985888689756\n",
      "Iter: 20, loss: 0.0012938983272761106\n",
      "Iter: 21, loss: 0.001325975521467626\n",
      "Iter: 22, loss: 0.00129110855050385\n",
      "Iter: 23, loss: 0.0013050076086074114\n",
      "Iter: 24, loss: 0.0013581285020336509\n",
      "Iter: 25, loss: 0.0014148415066301823\n",
      "Iter: 26, loss: 0.001347393961623311\n",
      "Iter: 27, loss: 0.0013599474914371967\n",
      "Iter: 28, loss: 0.0014318358153104782\n",
      "Iter: 29, loss: 0.0014050444588065147\n",
      "Iter: 30, loss: 0.0013967057457193732\n",
      "Iter: 31, loss: 0.0012884073657914996\n",
      "Iter: 32, loss: 0.0013277943944558501\n",
      "Iter: 33, loss: 0.0014785992680117488\n",
      "Iter: 34, loss: 0.0013195155188441277\n",
      "Iter: 35, loss: 0.0014001628151163459\n",
      "Iter: 36, loss: 0.0013458813773468137\n",
      "Iter: 37, loss: 0.0014034247724339366\n",
      "Iter: 38, loss: 0.0014462015824392438\n",
      "Iter: 39, loss: 0.0014233061810955405\n",
      "Iter: 40, loss: 0.0014282673364505172\n",
      "Iter: 41, loss: 0.001270678243599832\n",
      "Iter: 42, loss: 0.0013461720664054155\n",
      "Iter: 43, loss: 0.001367697841487825\n",
      "Iter: 44, loss: 0.001313878339715302\n",
      "Iter: 45, loss: 0.0013560277875512838\n",
      "Iter: 46, loss: 0.001461582607589662\n",
      "Iter: 47, loss: 0.0012896113330498338\n",
      "Iter: 48, loss: 0.0013262017164379358\n",
      "Iter: 49, loss: 0.0013844663044437766\n",
      "Iter: 50, loss: 0.0012697519268840551\n",
      "Iter: 51, loss: 0.0013757309643551707\n",
      "Iter: 52, loss: 0.0013471700949594378\n",
      "Iter: 53, loss: 0.001324902637861669\n",
      "Iter: 54, loss: 0.001349094440229237\n",
      "Iter: 55, loss: 0.0013398892479017377\n",
      "Iter: 56, loss: 0.0013333162060007453\n",
      "Iter: 57, loss: 0.0012662042863667011\n",
      "Iter: 58, loss: 0.0013119204668328166\n",
      "Iter: 59, loss: 0.0014597452245652676\n",
      "Iter: 60, loss: 0.0013695292873308063\n",
      "Iter: 61, loss: 0.0014527887105941772\n",
      "Iter: 62, loss: 0.0013457087334245443\n",
      "Iter: 63, loss: 0.0014597282279282808\n",
      "Iter: 64, loss: 0.0014133576769381762\n",
      "Iter: 65, loss: 0.0013390827225521207\n",
      "Iter: 66, loss: 0.0013672825880348682\n",
      "Iter: 67, loss: 0.001361894072033465\n",
      "Iter: 68, loss: 0.0012842660071328282\n",
      "Iter: 69, loss: 0.0013896228047087789\n",
      "Iter: 70, loss: 0.0013494003796949983\n",
      "Iter: 71, loss: 0.001276930794119835\n",
      "Iter: 72, loss: 0.001355803688056767\n",
      "Iter: 73, loss: 0.0013038915349170566\n",
      "Iter: 74, loss: 0.001332392799668014\n",
      "Iter: 75, loss: 0.0013425854267552495\n",
      "Iter: 76, loss: 0.0013503587106242776\n",
      "Iter: 77, loss: 0.001314605469815433\n",
      "Iter: 78, loss: 0.0014746269444003701\n",
      "Iter: 79, loss: 0.001370670273900032\n",
      "Iter: 80, loss: 0.0013325972249731421\n",
      "Iter: 81, loss: 0.0013587924186140299\n",
      "Iter: 82, loss: 0.0013781609013676643\n",
      "Iter: 83, loss: 0.001228536944836378\n",
      "Iter: 84, loss: 0.0014189756475389004\n",
      "Iter: 85, loss: 0.001320420647971332\n",
      "Iter: 86, loss: 0.001416246872395277\n",
      "Iter: 87, loss: 0.0013763459865003824\n",
      "Iter: 88, loss: 0.0012934455880895257\n",
      "Iter: 89, loss: 0.0013756705448031425\n",
      "Iter: 90, loss: 0.0014455470954999328\n",
      "Iter: 91, loss: 0.0013261925196275115\n",
      "Iter: 92, loss: 0.0013793952530249953\n",
      "Iter: 93, loss: 0.001391148311085999\n",
      "Iter: 94, loss: 0.0013500959612429142\n",
      "Iter: 95, loss: 0.0013927080435678363\n",
      "Iter: 96, loss: 0.001320128794759512\n",
      "Iter: 97, loss: 0.0013155261985957623\n",
      "Iter: 98, loss: 0.0014886169228702784\n",
      "Iter: 99, loss: 0.001481187529861927\n",
      "Iter: 100, loss: 0.001306741964071989\n",
      "Iter: 101, loss: 0.001383724040351808\n",
      "Iter: 102, loss: 0.001299659488722682\n",
      "Iter: 103, loss: 0.0014537310926243663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 104, loss: 0.0014123072614893317\n",
      "Iter: 105, loss: 0.0012834945227950811\n",
      "Iter: 106, loss: 0.001316447276622057\n",
      "Iter: 107, loss: 0.0013281991705298424\n",
      "Iter: 108, loss: 0.0012904596514999866\n",
      "Iter: 109, loss: 0.0013607438886538148\n",
      "Iter: 110, loss: 0.0013549142749980092\n",
      "Iter: 111, loss: 0.0013387174112722278\n",
      "Iter: 112, loss: 0.0013733472442254424\n",
      "Iter: 113, loss: 0.0013894977746531367\n",
      "Iter: 114, loss: 0.0013709331396967173\n",
      "Iter: 115, loss: 0.0014118703547865152\n",
      "Iter: 116, loss: 0.0013216000515967607\n",
      "Iter: 117, loss: 0.0014751506969332695\n",
      "Iter: 118, loss: 0.001399983768351376\n",
      "Iter: 119, loss: 0.0013394048437476158\n",
      "Iter: 120, loss: 0.0012790346518158913\n",
      "Iter: 121, loss: 0.0012551029212772846\n",
      "Iter: 122, loss: 0.0015075881965458393\n",
      "Iter: 123, loss: 0.0013459122274070978\n",
      "Iter: 124, loss: 0.001431008568033576\n",
      "Epoch duration: 28.39522886276245\n",
      "Model saved to: mol_opt/output/model_gcn_58\n",
      "Epoch: 59\n",
      "Iter: 0, loss: 0.001387080759741366\n",
      "Iter: 1, loss: 0.0014530736953020096\n",
      "Iter: 2, loss: 0.0012051534140482545\n",
      "Iter: 3, loss: 0.0012712718453258276\n",
      "Iter: 4, loss: 0.0013440108159556985\n",
      "Iter: 5, loss: 0.0013742967275902629\n",
      "Iter: 6, loss: 0.0014070570468902588\n",
      "Iter: 7, loss: 0.0014884305419400334\n",
      "Iter: 8, loss: 0.0014637750573456287\n",
      "Iter: 9, loss: 0.001356897410005331\n",
      "Iter: 10, loss: 0.0013004891807213426\n",
      "Iter: 11, loss: 0.0014420161023736\n",
      "Iter: 12, loss: 0.001410348922945559\n",
      "Iter: 13, loss: 0.0015608149114996195\n",
      "Iter: 14, loss: 0.0013515885220840573\n",
      "Iter: 15, loss: 0.0013116265181452036\n",
      "Iter: 16, loss: 0.0013937709154561162\n",
      "Iter: 17, loss: 0.001313112792558968\n",
      "Iter: 18, loss: 0.0012722760438919067\n",
      "Iter: 19, loss: 0.0012533841654658318\n",
      "Iter: 20, loss: 0.0012788978638127446\n",
      "Iter: 21, loss: 0.0013110011350363493\n",
      "Iter: 22, loss: 0.0012764333514496684\n",
      "Iter: 23, loss: 0.001290745916776359\n",
      "Iter: 24, loss: 0.0013426520163193345\n",
      "Iter: 25, loss: 0.0013986394042149186\n",
      "Iter: 26, loss: 0.0013315832475200295\n",
      "Iter: 27, loss: 0.0013443310745060444\n",
      "Iter: 28, loss: 0.0014148245099931955\n",
      "Iter: 29, loss: 0.0013892815914005041\n",
      "Iter: 30, loss: 0.0013802585890516639\n",
      "Iter: 31, loss: 0.001273367670364678\n",
      "Iter: 32, loss: 0.0013129201252013445\n",
      "Iter: 33, loss: 0.001460996107198298\n",
      "Iter: 34, loss: 0.0013041595229879022\n",
      "Iter: 35, loss: 0.0013846445363014936\n",
      "Iter: 36, loss: 0.001330275903455913\n",
      "Iter: 37, loss: 0.0013871741248294711\n",
      "Iter: 38, loss: 0.0014306140365079045\n",
      "Iter: 39, loss: 0.0014066682197153568\n",
      "Iter: 40, loss: 0.0014111060881987214\n",
      "Iter: 41, loss: 0.0012561866315081716\n",
      "Iter: 42, loss: 0.0013312215451151133\n",
      "Iter: 43, loss: 0.0013517943443730474\n",
      "Iter: 44, loss: 0.0012990485411137342\n",
      "Iter: 45, loss: 0.0013410665560513735\n",
      "Iter: 46, loss: 0.0014455277705565095\n",
      "Iter: 47, loss: 0.0012748294975608587\n",
      "Iter: 48, loss: 0.0013110029976814985\n",
      "Iter: 49, loss: 0.0013683937722817063\n",
      "Iter: 50, loss: 0.0012548137456178665\n",
      "Iter: 51, loss: 0.0013598967343568802\n",
      "Iter: 52, loss: 0.0013323071179911494\n",
      "Iter: 53, loss: 0.0013096266193315387\n",
      "Iter: 54, loss: 0.001333437510766089\n",
      "Iter: 55, loss: 0.0013250279007479548\n",
      "Iter: 56, loss: 0.0013178713852539659\n",
      "Iter: 57, loss: 0.0012519259471446276\n",
      "Iter: 58, loss: 0.0012963697081431746\n",
      "Iter: 59, loss: 0.001443679560907185\n",
      "Iter: 60, loss: 0.0013542770175263286\n",
      "Iter: 61, loss: 0.0014366814866662025\n",
      "Iter: 62, loss: 0.0013307662447914481\n",
      "Iter: 63, loss: 0.0014433333417400718\n",
      "Iter: 64, loss: 0.001397554180584848\n",
      "Iter: 65, loss: 0.0013246018206700683\n",
      "Iter: 66, loss: 0.0013518991181626916\n",
      "Iter: 67, loss: 0.0013468851102516055\n",
      "Iter: 68, loss: 0.0012697848724201322\n",
      "Iter: 69, loss: 0.0013732474762946367\n",
      "Iter: 70, loss: 0.0013338918797671795\n",
      "Iter: 71, loss: 0.0012620685156434774\n",
      "Iter: 72, loss: 0.001340209972113371\n",
      "Iter: 73, loss: 0.001288694329559803\n",
      "Iter: 74, loss: 0.0013170238817110658\n",
      "Iter: 75, loss: 0.0013276584213599563\n",
      "Iter: 76, loss: 0.0013349201763048768\n",
      "Iter: 77, loss: 0.0013001123443245888\n",
      "Iter: 78, loss: 0.0014587031910195947\n",
      "Iter: 79, loss: 0.0013548575807362795\n",
      "Iter: 80, loss: 0.0013181602116674185\n",
      "Iter: 81, loss: 0.0013437604065984488\n",
      "Iter: 82, loss: 0.0013635910581797361\n",
      "Iter: 83, loss: 0.0012144319480285048\n",
      "Iter: 84, loss: 0.0014032486360520124\n",
      "Iter: 85, loss: 0.001304985722526908\n",
      "Iter: 86, loss: 0.0013993605971336365\n",
      "Iter: 87, loss: 0.0013613123446702957\n",
      "Iter: 88, loss: 0.0012781318509951234\n",
      "Iter: 89, loss: 0.0013593962648883462\n",
      "Iter: 90, loss: 0.001429743249900639\n",
      "Iter: 91, loss: 0.001310553401708603\n",
      "Iter: 92, loss: 0.0013637773226946592\n",
      "Iter: 93, loss: 0.0013765101321041584\n",
      "Iter: 94, loss: 0.0013350509107112885\n",
      "Iter: 95, loss: 0.0013778138672932982\n",
      "Iter: 96, loss: 0.001305137062445283\n",
      "Iter: 97, loss: 0.001300075207836926\n",
      "Iter: 98, loss: 0.0014715675497427583\n",
      "Iter: 99, loss: 0.0014646692434325814\n",
      "Iter: 100, loss: 0.0012913071550428867\n",
      "Iter: 101, loss: 0.0013679935364052653\n",
      "Iter: 102, loss: 0.0012844862649217248\n",
      "Iter: 103, loss: 0.0014376331819221377\n",
      "Iter: 104, loss: 0.0013971435837447643\n",
      "Iter: 105, loss: 0.0012688669376075268\n",
      "Iter: 106, loss: 0.0013014553114771843\n",
      "Iter: 107, loss: 0.001313930144533515\n",
      "Iter: 108, loss: 0.0012759778182953596\n",
      "Iter: 109, loss: 0.001345470198430121\n",
      "Iter: 110, loss: 0.0013396625872701406\n",
      "Iter: 111, loss: 0.001323266071267426\n",
      "Iter: 112, loss: 0.0013578776270151138\n",
      "Iter: 113, loss: 0.0013742028968408704\n",
      "Iter: 114, loss: 0.0013544271932914853\n",
      "Iter: 115, loss: 0.0013956777984276414\n",
      "Iter: 116, loss: 0.0013061120407655835\n",
      "Iter: 117, loss: 0.001458407728932798\n",
      "Iter: 118, loss: 0.001384333474561572\n",
      "Iter: 119, loss: 0.0013242224231362343\n",
      "Iter: 120, loss: 0.0012651419965550303\n",
      "Iter: 121, loss: 0.001241409219801426\n",
      "Iter: 122, loss: 0.00149051402695477\n",
      "Iter: 123, loss: 0.0013303215382620692\n",
      "Iter: 124, loss: 0.001415227190591395\n",
      "Epoch duration: 28.11064076423645\n",
      "Model saved to: mol_opt/output/model_gcn_59\n",
      "Epoch: 60\n",
      "Iter: 0, loss: 0.0013710403582081199\n",
      "Iter: 1, loss: 0.0014371246797963977\n",
      "Iter: 2, loss: 0.0011907024309039116\n",
      "Iter: 3, loss: 0.001256520044989884\n",
      "Iter: 4, loss: 0.0013282279251143336\n",
      "Iter: 5, loss: 0.0013583565596491098\n",
      "Iter: 6, loss: 0.0013911263085901737\n",
      "Iter: 7, loss: 0.0014720604522153735\n",
      "Iter: 8, loss: 0.001448102411814034\n",
      "Iter: 9, loss: 0.0013417662121355534\n",
      "Iter: 10, loss: 0.0012857692781835794\n",
      "Iter: 11, loss: 0.0014261495089158416\n",
      "Iter: 12, loss: 0.0013947271509096026\n",
      "Iter: 13, loss: 0.0015433403896167874\n",
      "Iter: 14, loss: 0.0013357276329770684\n",
      "Iter: 15, loss: 0.0012970156967639923\n",
      "Iter: 16, loss: 0.001378009794279933\n",
      "Iter: 17, loss: 0.0012992549454793334\n",
      "Iter: 18, loss: 0.0012573602143675089\n",
      "Iter: 19, loss: 0.0012401422718539834\n",
      "Iter: 20, loss: 0.0012642801739275455\n",
      "Iter: 21, loss: 0.0012961943866685033\n",
      "Iter: 22, loss: 0.0012620291672647\n",
      "Iter: 23, loss: 0.0012767986627295613\n",
      "Iter: 24, loss: 0.0013278063852339983\n",
      "Iter: 25, loss: 0.001382871880196035\n",
      "Iter: 26, loss: 0.0013160581002011895\n",
      "Iter: 27, loss: 0.001329517224803567\n",
      "Iter: 28, loss: 0.0013983125099912286\n",
      "Iter: 29, loss: 0.0013740059221163392\n",
      "Iter: 30, loss: 0.0013640550896525383\n",
      "Iter: 31, loss: 0.0012587410164996982\n",
      "Iter: 32, loss: 0.001297612558118999\n",
      "Iter: 33, loss: 0.0014440141385421157\n",
      "Iter: 34, loss: 0.0012893467210233212\n",
      "Iter: 35, loss: 0.0013689259067177773\n",
      "Iter: 36, loss: 0.0013150278246030211\n",
      "Iter: 37, loss: 0.001371328835375607\n",
      "Iter: 38, loss: 0.0014149906346574426\n",
      "Iter: 39, loss: 0.001391035271808505\n",
      "Iter: 40, loss: 0.001394767314195633\n",
      "Iter: 41, loss: 0.0012431974755600095\n",
      "Iter: 42, loss: 0.0013160156086087227\n",
      "Iter: 43, loss: 0.0013358461437746882\n",
      "Iter: 44, loss: 0.0012846426106989384\n",
      "Iter: 45, loss: 0.0013264830922707915\n",
      "Iter: 46, loss: 0.0014298900496214628\n",
      "Iter: 47, loss: 0.001260407385416329\n",
      "Iter: 48, loss: 0.001295938272960484\n",
      "Iter: 49, loss: 0.0013532093726098537\n",
      "Iter: 50, loss: 0.0012402243446558714\n",
      "Iter: 51, loss: 0.0013445797376334667\n",
      "Iter: 52, loss: 0.001317873247899115\n",
      "Iter: 53, loss: 0.001294568763114512\n",
      "Iter: 54, loss: 0.001318124937824905\n",
      "Iter: 55, loss: 0.001310375751927495\n",
      "Iter: 56, loss: 0.0013027529930695891\n",
      "Iter: 57, loss: 0.0012381358537822962\n",
      "Iter: 58, loss: 0.0012813012581318617\n",
      "Iter: 59, loss: 0.0014281360199674964\n",
      "Iter: 60, loss: 0.0013394204434007406\n",
      "Iter: 61, loss: 0.0014208273496478796\n",
      "Iter: 62, loss: 0.0013164164265617728\n",
      "Iter: 63, loss: 0.0014273792039602995\n",
      "Iter: 64, loss: 0.0013822499895468354\n",
      "Iter: 65, loss: 0.0013104458339512348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 66, loss: 0.0013371665263548493\n",
      "Iter: 67, loss: 0.0013319236459210515\n",
      "Iter: 68, loss: 0.001255708048120141\n",
      "Iter: 69, loss: 0.0013579654041677713\n",
      "Iter: 70, loss: 0.0013191747711971402\n",
      "Iter: 71, loss: 0.0012473310343921185\n",
      "Iter: 72, loss: 0.0013250417541712523\n",
      "Iter: 73, loss: 0.0012734270421788096\n",
      "Iter: 74, loss: 0.0013019361067563295\n",
      "Iter: 75, loss: 0.0013130754232406616\n",
      "Iter: 76, loss: 0.0013190577737987041\n",
      "Iter: 77, loss: 0.0012860174756497145\n",
      "Iter: 78, loss: 0.0014430738519877195\n",
      "Iter: 79, loss: 0.0013394191628322005\n",
      "Iter: 80, loss: 0.0013042681384831667\n",
      "Iter: 81, loss: 0.0013291110517457128\n",
      "Iter: 82, loss: 0.0013489678967744112\n",
      "Iter: 83, loss: 0.0012006019242107868\n",
      "Iter: 84, loss: 0.0013878836762160063\n",
      "Iter: 85, loss: 0.0012897553388029337\n",
      "Iter: 86, loss: 0.0013835193822160363\n",
      "Iter: 87, loss: 0.0013466643868014216\n",
      "Iter: 88, loss: 0.0012632261496037245\n",
      "Iter: 89, loss: 0.0013431150000542402\n",
      "Iter: 90, loss: 0.001413792953826487\n",
      "Iter: 91, loss: 0.001295293215662241\n",
      "Iter: 92, loss: 0.0013484865194186568\n",
      "Iter: 93, loss: 0.0013623242266476154\n",
      "Iter: 94, loss: 0.0013205896830186248\n",
      "Iter: 95, loss: 0.0013630513567477465\n",
      "Iter: 96, loss: 0.0012905102921649814\n",
      "Iter: 97, loss: 0.0012851832434535027\n",
      "Iter: 98, loss: 0.0014553951332345605\n",
      "Iter: 99, loss: 0.0014486555010080338\n",
      "Iter: 100, loss: 0.001276800176128745\n",
      "Iter: 101, loss: 0.001352792140096426\n",
      "Iter: 102, loss: 0.001270344015210867\n",
      "Iter: 103, loss: 0.0014217854477465153\n",
      "Iter: 104, loss: 0.0013815934071317315\n",
      "Iter: 105, loss: 0.0012537692673504353\n",
      "Iter: 106, loss: 0.001286366255953908\n",
      "Iter: 107, loss: 0.0012999885948374867\n",
      "Iter: 108, loss: 0.001262090401723981\n",
      "Iter: 109, loss: 0.001330869854427874\n",
      "Iter: 110, loss: 0.0013253752840682864\n",
      "Iter: 111, loss: 0.0013081423239782453\n",
      "Iter: 112, loss: 0.001342826639302075\n",
      "Iter: 113, loss: 0.0013587267603725195\n",
      "Iter: 114, loss: 0.0013392835389822721\n",
      "Iter: 115, loss: 0.0013798747677356005\n",
      "Iter: 116, loss: 0.0012909478973597288\n",
      "Iter: 117, loss: 0.0014421187806874514\n",
      "Iter: 118, loss: 0.001369055244140327\n",
      "Iter: 119, loss: 0.0013093615416437387\n",
      "Iter: 120, loss: 0.0012518770527094603\n",
      "Iter: 121, loss: 0.0012279781512916088\n",
      "Iter: 122, loss: 0.0014735400909557939\n",
      "Iter: 123, loss: 0.001315124100074172\n",
      "Iter: 124, loss: 0.001399003784172237\n",
      "Epoch duration: 28.38718032836914\n",
      "Model saved to: mol_opt/output/model_gcn_60\n",
      "Epoch: 61\n",
      "Iter: 0, loss: 0.0013551756273955107\n",
      "Iter: 1, loss: 0.0014215856790542603\n",
      "Iter: 2, loss: 0.0011764006922021508\n",
      "Iter: 3, loss: 0.0012419625418260694\n",
      "Iter: 4, loss: 0.0013128904392942786\n",
      "Iter: 5, loss: 0.0013430090621113777\n",
      "Iter: 6, loss: 0.0013755125692114234\n",
      "Iter: 7, loss: 0.0014561560237780213\n",
      "Iter: 8, loss: 0.0014329282566905022\n",
      "Iter: 9, loss: 0.001327093574218452\n",
      "Iter: 10, loss: 0.0012706744018942118\n",
      "Iter: 11, loss: 0.0014105250593274832\n",
      "Iter: 12, loss: 0.001379266381263733\n",
      "Iter: 13, loss: 0.00152637402061373\n",
      "Iter: 14, loss: 0.0013206357834860682\n",
      "Iter: 15, loss: 0.0012827084865421057\n",
      "Iter: 16, loss: 0.001362759037874639\n",
      "Iter: 17, loss: 0.0012855642708018422\n",
      "Iter: 18, loss: 0.0012428259942680597\n",
      "Iter: 19, loss: 0.001226932741701603\n",
      "Iter: 20, loss: 0.0012500322191044688\n",
      "Iter: 21, loss: 0.0012818966060876846\n",
      "Iter: 22, loss: 0.0012476475676521659\n",
      "Iter: 23, loss: 0.0012628193944692612\n",
      "Iter: 24, loss: 0.0013130448060110211\n",
      "Iter: 25, loss: 0.0013674537185579538\n",
      "Iter: 26, loss: 0.0013008470414206386\n",
      "Iter: 27, loss: 0.0013151870807632804\n",
      "Iter: 28, loss: 0.0013821833999827504\n",
      "Iter: 29, loss: 0.0013592676259577274\n",
      "Iter: 30, loss: 0.00134814262855798\n",
      "Iter: 31, loss: 0.0012445091269910336\n",
      "Iter: 32, loss: 0.0012832427164539695\n",
      "Iter: 33, loss: 0.0014275276334956288\n",
      "Iter: 34, loss: 0.0012747724540531635\n",
      "Iter: 35, loss: 0.0013540730578824878\n",
      "Iter: 36, loss: 0.0013002479681745172\n",
      "Iter: 37, loss: 0.0013561638770624995\n",
      "Iter: 38, loss: 0.001399814267642796\n",
      "Iter: 39, loss: 0.0013752045342698693\n",
      "Iter: 40, loss: 0.0013787116622552276\n",
      "Iter: 41, loss: 0.0012289452133700252\n",
      "Iter: 42, loss: 0.001301900134421885\n",
      "Iter: 43, loss: 0.0013201312394812703\n",
      "Iter: 44, loss: 0.0012705986155197024\n",
      "Iter: 45, loss: 0.0013121527153998613\n",
      "Iter: 46, loss: 0.0014148030895739794\n",
      "Iter: 47, loss: 0.0012463591992855072\n",
      "Iter: 48, loss: 0.0012815746013075113\n",
      "Iter: 49, loss: 0.0013379312586039305\n",
      "Iter: 50, loss: 0.001225904212333262\n",
      "Iter: 51, loss: 0.0013295832322910428\n",
      "Iter: 52, loss: 0.0013037660392001271\n",
      "Iter: 53, loss: 0.001279940945096314\n",
      "Iter: 54, loss: 0.0013033177237957716\n",
      "Iter: 55, loss: 0.0012962444452568889\n",
      "Iter: 56, loss: 0.001288143452256918\n",
      "Iter: 57, loss: 0.001224071835167706\n",
      "Iter: 58, loss: 0.0012661414220929146\n",
      "Iter: 59, loss: 0.0014127384638413787\n",
      "Iter: 60, loss: 0.0013248823815956712\n",
      "Iter: 61, loss: 0.0014054212952032685\n",
      "Iter: 62, loss: 0.0013021975755691528\n",
      "Iter: 63, loss: 0.001411947188898921\n",
      "Iter: 64, loss: 0.0013671600027009845\n",
      "Iter: 65, loss: 0.0012962704058736563\n",
      "Iter: 66, loss: 0.0013229105388745666\n",
      "Iter: 67, loss: 0.0013176554348319769\n",
      "Iter: 68, loss: 0.0012419781414791942\n",
      "Iter: 69, loss: 0.0013430605176836252\n",
      "Iter: 70, loss: 0.0013043376384302974\n",
      "Iter: 71, loss: 0.0012332298792898655\n",
      "Iter: 72, loss: 0.001310173305682838\n",
      "Iter: 73, loss: 0.0012588222743943334\n",
      "Iter: 74, loss: 0.001287123654037714\n",
      "Iter: 75, loss: 0.0012988614616915584\n",
      "Iter: 76, loss: 0.0013035462470725179\n",
      "Iter: 77, loss: 0.0012720623053610325\n",
      "Iter: 78, loss: 0.0014278286835178733\n",
      "Iter: 79, loss: 0.0013243925059214234\n",
      "Iter: 80, loss: 0.0012905893381685019\n",
      "Iter: 81, loss: 0.001315486035309732\n",
      "Iter: 82, loss: 0.001334693981334567\n",
      "Iter: 83, loss: 0.0011877182405442\n",
      "Iter: 84, loss: 0.0013723766896873713\n",
      "Iter: 85, loss: 0.0012749629095196724\n",
      "Iter: 86, loss: 0.0013680187985301018\n",
      "Iter: 87, loss: 0.0013321966398507357\n",
      "Iter: 88, loss: 0.0012487709755077958\n",
      "Iter: 89, loss: 0.0013275553938001394\n",
      "Iter: 90, loss: 0.0013982955133542418\n",
      "Iter: 91, loss: 0.0012801314005628228\n",
      "Iter: 92, loss: 0.001333599560894072\n",
      "Iter: 93, loss: 0.0013488098047673702\n",
      "Iter: 94, loss: 0.0013061159988865256\n",
      "Iter: 95, loss: 0.0013487156247720122\n",
      "Iter: 96, loss: 0.0012762609403580427\n",
      "Iter: 97, loss: 0.0012707429705187678\n",
      "Iter: 98, loss: 0.0014392697485163808\n",
      "Iter: 99, loss: 0.001432818011380732\n",
      "Iter: 100, loss: 0.0012626011157408357\n",
      "Iter: 101, loss: 0.001337681314907968\n",
      "Iter: 102, loss: 0.0012564980424940586\n",
      "Iter: 103, loss: 0.00140609301161021\n",
      "Iter: 104, loss: 0.001366418320685625\n",
      "Iter: 105, loss: 0.0012398804537951946\n",
      "Iter: 106, loss: 0.0012720613740384579\n",
      "Iter: 107, loss: 0.0012871306389570236\n",
      "Iter: 108, loss: 0.001248558983206749\n",
      "Iter: 109, loss: 0.001316254842095077\n",
      "Iter: 110, loss: 0.0013108582934364676\n",
      "Iter: 111, loss: 0.001293384120799601\n",
      "Iter: 112, loss: 0.0013280973071232438\n",
      "Iter: 113, loss: 0.0013441415503621101\n",
      "Iter: 114, loss: 0.0013245298760011792\n",
      "Iter: 115, loss: 0.0013644698774442077\n",
      "Iter: 116, loss: 0.001276169321499765\n",
      "Iter: 117, loss: 0.001426143804565072\n",
      "Iter: 118, loss: 0.001354136154986918\n",
      "Iter: 119, loss: 0.0012949506053701043\n",
      "Iter: 120, loss: 0.0012385506415739655\n",
      "Iter: 121, loss: 0.0012148075038567185\n",
      "Iter: 122, loss: 0.0014572694199159741\n",
      "Iter: 123, loss: 0.0012998506426811218\n",
      "Iter: 124, loss: 0.0013839403400197625\n",
      "Epoch duration: 28.300234079360962\n",
      "Model saved to: mol_opt/output/model_gcn_61\n",
      "Epoch: 62\n",
      "Iter: 0, loss: 0.0013395026326179504\n",
      "Iter: 1, loss: 0.0014065082650631666\n",
      "Iter: 2, loss: 0.0011626359773799777\n",
      "Iter: 3, loss: 0.0012278862996026874\n",
      "Iter: 4, loss: 0.001297996030189097\n",
      "Iter: 5, loss: 0.001328081707470119\n",
      "Iter: 6, loss: 0.0013597110519185662\n",
      "Iter: 7, loss: 0.0014404549729079008\n",
      "Iter: 8, loss: 0.0014179558493196964\n",
      "Iter: 9, loss: 0.0013126741396263242\n",
      "Iter: 10, loss: 0.001256596646271646\n",
      "Iter: 11, loss: 0.0013952594017609954\n",
      "Iter: 12, loss: 0.0013642475241795182\n",
      "Iter: 13, loss: 0.0015099102165549994\n",
      "Iter: 14, loss: 0.001305552665144205\n",
      "Iter: 15, loss: 0.001268745050765574\n",
      "Iter: 16, loss: 0.001347710145637393\n",
      "Iter: 17, loss: 0.0012722507817670703\n",
      "Iter: 18, loss: 0.0012286703567951918\n",
      "Iter: 19, loss: 0.0012137943413108587\n",
      "Iter: 20, loss: 0.001236165757291019\n",
      "Iter: 21, loss: 0.0012677637860178947\n",
      "Iter: 22, loss: 0.0012338998494669795\n",
      "Iter: 23, loss: 0.0012495119590312243\n",
      "Iter: 24, loss: 0.001298382761888206\n",
      "Iter: 25, loss: 0.0013524037785828114\n",
      "Iter: 26, loss: 0.001285735983401537\n",
      "Iter: 27, loss: 0.0013008253881707788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 28, loss: 0.001366420998238027\n",
      "Iter: 29, loss: 0.001344447024166584\n",
      "Iter: 30, loss: 0.0013327503111213446\n",
      "Iter: 31, loss: 0.0012306104181334376\n",
      "Iter: 32, loss: 0.001268883584998548\n",
      "Iter: 33, loss: 0.00141122005879879\n",
      "Iter: 34, loss: 0.0012605716474354267\n",
      "Iter: 35, loss: 0.001339639537036419\n",
      "Iter: 36, loss: 0.001285793841816485\n",
      "Iter: 37, loss: 0.0013413054402917624\n",
      "Iter: 38, loss: 0.001384994015097618\n",
      "Iter: 39, loss: 0.0013596395729109645\n",
      "Iter: 40, loss: 0.001363184885121882\n",
      "Iter: 41, loss: 0.0012159005273133516\n",
      "Iter: 42, loss: 0.001287736464291811\n",
      "Iter: 43, loss: 0.001304713194258511\n",
      "Iter: 44, loss: 0.0012568903621286154\n",
      "Iter: 45, loss: 0.0012982385233044624\n",
      "Iter: 46, loss: 0.0013998515205457807\n",
      "Iter: 47, loss: 0.0012326480355113745\n",
      "Iter: 48, loss: 0.001267777755856514\n",
      "Iter: 49, loss: 0.0013230155454948545\n",
      "Iter: 50, loss: 0.0012119279708713293\n",
      "Iter: 51, loss: 0.0013151419116184115\n",
      "Iter: 52, loss: 0.001290082698687911\n",
      "Iter: 53, loss: 0.00126592256128788\n",
      "Iter: 54, loss: 0.0012887249467894435\n"
     ]
    }
   ],
   "source": [
    "molopt = main(args, data_loader = data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try out some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data_loader:\n",
    "    X = (MolGraph(i[0]))\n",
    "    Y = (MolGraph(i[1]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_embedding, x_delta = molopt.forward(X)\n",
    "y_embedding = molopt.encode(Y)\n",
    "y_aligned = molopt.align(x_embedding, X, y_embedding, Y)\n",
    "xhat_delta = molopt.delta(x_embedding, y_aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1041, 50])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0329,  0.0609, -0.0998,  ...,  0.0075,  0.0955,  0.0417],\n",
       "        [-0.0989,  0.0230, -0.0382,  ...,  0.0112,  0.0739,  0.0399],\n",
       "        [-0.0075,  0.0569, -0.0903,  ..., -0.0105,  0.1205,  0.0362],\n",
       "        ...,\n",
       "        [-0.0320,  0.0622, -0.1089,  ...,  0.0226,  0.0922,  0.0389],\n",
       "        [-0.0710,  0.0140, -0.0761,  ..., -0.0365,  0.0978,  0.0615],\n",
       "        [-0.0357,  0.0151, -0.0686,  ..., -0.0102,  0.0744,  0.0069]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.3743e-02,  4.4010e-02, -3.0419e-02,  ..., -3.1573e-02,\n",
       "          3.3295e-02,  5.1158e-02],\n",
       "        [-1.7625e-01,  1.2730e-01, -6.4805e-02,  ...,  5.5192e-02,\n",
       "          1.4270e-01, -6.1634e-03],\n",
       "        [-7.5377e-02, -1.6244e-02,  5.7163e-02,  ...,  5.2657e-02,\n",
       "          6.2681e-02,  2.1037e-02],\n",
       "        ...,\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [-1.9689e-01, -5.8483e-02,  9.4051e-03,  ...,  1.4330e-01,\n",
       "          3.4291e-01,  1.2687e-01],\n",
       "        [-8.1267e-03,  1.4335e-04, -5.4330e-04,  ..., -4.7268e-03,\n",
       "          1.3943e-02, -8.6641e-03]], device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xhat_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0209,  0.0169, -0.0694,  ...,  0.0390,  0.0622, -0.0094],\n",
       "        [ 0.0773, -0.1043,  0.0266,  ..., -0.0440, -0.0688,  0.0460],\n",
       "        [ 0.0679,  0.0732, -0.1474,  ..., -0.0631,  0.0578,  0.0152],\n",
       "        ...,\n",
       "        [-0.0320,  0.0622, -0.1089,  ...,  0.0226,  0.0922,  0.0389],\n",
       "        [ 0.1259,  0.0725, -0.0856,  ..., -0.1798, -0.2451, -0.0654],\n",
       "        [-0.0276,  0.0149, -0.0681,  ..., -0.0054,  0.0605,  0.0156]],\n",
       "       device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_delta - xhat_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0016, -0.0741, -0.0745,  ...,  0.0228, -0.0114, -0.0270],\n",
       "        [ 0.0706, -0.2145, -0.2110,  ...,  0.3129,  0.1174, -0.0711],\n",
       "        [-0.1300, -0.2984,  0.1859,  ..., -0.0819,  0.1519, -0.2849],\n",
       "        ...,\n",
       "        [ 0.2385, -0.1195, -0.1575,  ...,  0.0702, -0.1347,  0.0785],\n",
       "        [ 0.0352,  0.1964, -0.0075,  ..., -0.0421,  0.1371, -0.1475],\n",
       "        [-0.0727,  0.1660,  0.0481,  ...,  0.0867,  0.1587, -0.0863]],\n",
       "       device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "molopt.delta(x_embedding, y) - xhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<otgnn.graph.mol_graph.Molecule at 0x7fec37dd4f60>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.mols[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
