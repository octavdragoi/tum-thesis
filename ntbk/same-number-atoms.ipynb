{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/octav/gitrepos/tum-thesis\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if \"ntbk\" in os.getcwd():\n",
    "    os.chdir(\"..\")\n",
    "print(os.getcwd())\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), \"otgnn\"))\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from otgnn.models import GCN\n",
    "from otgnn.graph import MolGraph\n",
    "\n",
    "from mol_opt.data_mol_opt import MolOptDataset\n",
    "from mol_opt.data_mol_opt import get_loader\n",
    "from mol_opt.arguments import get_args\n",
    "from mol_opt.train_mol_opt import main\n",
    "\n",
    "from rdkit.Chem import MolFromSmiles\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/octav/anaconda3/envs/mol_ot/lib/python36.zip',\n",
       " '/home/octav/anaconda3/envs/mol_ot/lib/python3.6',\n",
       " '/home/octav/anaconda3/envs/mol_ot/lib/python3.6/lib-dynload',\n",
       " '',\n",
       " '/home/octav/anaconda3/envs/mol_ot/lib/python3.6/site-packages',\n",
       " '/home/octav/anaconda3/envs/mol_ot/lib/python3.6/site-packages/IPython/extensions',\n",
       " '/home/octav/.ipython',\n",
       " '/home/octav/gitrepos/tum-thesis/otgnn']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = get_loader(\"iclr19-graph2graph/data/qed\", 48, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data_loader:\n",
    "    X = (MolGraph(i[0]))\n",
    "    Y = (MolGraph(i[1]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = [\"\", \"-cuda\"]\n",
    "args = get_args()\n",
    "# args.device = \"cpu\"\n",
    "gcn = GCN(args).to(args.device)\n",
    "opt = torch.nn.Linear(50, 50).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(agg_func='sum', batch_norm=False, cuda=True, device='cuda:0', dropout_ffn=0.0, dropout_gcn=0.0, ffn_activation='LeakyReLU', linear_out=False, n_epochs=10, n_ffn_hidden=100, n_hidden=50, n_labels=1, n_layers=5, pc_hidden=50)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn = GCN(args).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (W_message_i): Linear(in_features=100, out_features=50, bias=False)\n",
       "  (W_message_h): Linear(in_features=50, out_features=50, bias=False)\n",
       "  (W_message_o): Linear(in_features=143, out_features=50, bias=True)\n",
       "  (W_mol_h): Linear(in_features=50, out_features=100, bias=True)\n",
       "  (W_mol_o): Linear(in_features=100, out_features=1, bias=True)\n",
       "  (dropout_gcn): Dropout(p=0.0, inplace=False)\n",
       "  (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (W_message_i): Linear(in_features=100, out_features=50, bias=False)\n",
       "  (W_message_h): Linear(in_features=50, out_features=50, bias=False)\n",
       "  (W_message_o): Linear(in_features=143, out_features=50, bias=True)\n",
       "  (W_mol_h): Linear(in_features=50, out_features=100, bias=True)\n",
       "  (W_mol_o): Linear(in_features=100, out_features=1, bias=True)\n",
       "  (dropout_gcn): Dropout(p=0.0, inplace=False)\n",
       "  (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1131, -0.0804, -0.0672,  ..., -0.1228,  0.2296, -0.0391],\n",
       "        [-0.1317,  0.0174, -0.0672,  ..., -0.0214,  0.3100, -0.0278],\n",
       "        [-0.1112,  0.0032, -0.0945,  ..., -0.0932,  0.1757,  0.0186],\n",
       "        ...,\n",
       "        [-0.1082,  0.0235, -0.0526,  ..., -0.0472,  0.3220, -0.0061],\n",
       "        [-0.1707, -0.0715, -0.1234,  ..., -0.1141,  0.1511, -0.0728],\n",
       "        [-0.1173,  0.0305, -0.1945,  ..., -0.1928,  0.1837,  0.0267]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt(gcn.forward(Y)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1041, 50])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcn.forward(X)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  22   40   61   79  100  120  144  164  186  205  226  251  276  298\n",
      "  321  340  360  386  411  433  455  476  500  523  542  566  586  612\n",
      "  635  659  679  695  719  738  762  781  799  822  844  866  890  914\n",
      "  938  958  978  998 1021 1041]\n"
     ]
    }
   ],
   "source": [
    "print (np.cumsum(([len(x.atoms) for x in X.mols])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X.mols[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MolOpt(\n",
       "  (GCN): GCN(\n",
       "    (W_message_i): Linear(in_features=100, out_features=50, bias=False)\n",
       "    (W_message_h): Linear(in_features=50, out_features=50, bias=False)\n",
       "    (W_message_o): Linear(in_features=143, out_features=50, bias=True)\n",
       "    (W_mol_h): Linear(in_features=50, out_features=100, bias=True)\n",
       "    (W_mol_o): Linear(in_features=100, out_features=1, bias=True)\n",
       "    (dropout_gcn): Dropout(p=0.0, inplace=False)\n",
       "    (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (opt0): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (opt1): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (delta_loss): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mol_opt import mol_opt\n",
    "molopt = mol_opt.MolOpt(args).to(device = args.device)\n",
    "molopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3370,  0.1239, -0.0026,  ..., -0.0845,  0.1382, -0.0048],\n",
       "        [-0.3119,  0.0086,  0.1595,  ..., -0.0298,  0.0852,  0.3029],\n",
       "        [-0.2207, -0.0153,  0.0735,  ..., -0.0120,  0.2595,  0.2476],\n",
       "        ...,\n",
       "        [-0.3421,  0.1246,  0.0272,  ..., -0.0622,  0.1342, -0.0053],\n",
       "        [-0.3372,  0.0805, -0.0215,  ...,  0.0689,  0.2914,  0.3123],\n",
       "        [-0.2457, -0.0502,  0.0285,  ...,  0.1159,  0.1260,  0.3469]],\n",
       "       device='cuda:0', grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "molopt.align(molopt.encode(X), X, molopt.encode(Y), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3370,  0.1239, -0.0026,  ..., -0.0845,  0.1382, -0.0048],\n",
       "        [-0.2441, -0.0096, -0.0594,  ..., -0.0485,  0.1602,  0.1980],\n",
       "        [-0.0878,  0.1209, -0.0024,  ..., -0.1630, -0.0446,  0.1149],\n",
       "        ...,\n",
       "        [-0.2650,  0.0218, -0.0695,  ...,  0.1347,  0.3265,  0.3682],\n",
       "        [-0.2774,  0.2852, -0.1069,  ...,  0.0081, -0.0275,  0.0836],\n",
       "        [-0.2457, -0.0502,  0.0285,  ...,  0.1159,  0.1260,  0.3469]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "molopt.encode(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0155, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "molopt.forward_train(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, loss: 0.015534802339971066\n",
      "Iter: 1, loss: 0.015233045443892479\n",
      "Iter: 2, loss: 0.014541471377015114\n",
      "Iter: 3, loss: 0.014296744018793106\n",
      "Iter: 4, loss: 0.014862217009067535\n",
      "Iter: 5, loss: 0.014735095202922821\n",
      "Iter: 6, loss: 0.014787458814680576\n",
      "Iter: 7, loss: 0.0148368114605546\n",
      "Iter: 8, loss: 0.014837524853646755\n",
      "Iter: 9, loss: 0.014857822097837925\n",
      "Iter: 10, loss: 0.014195257797837257\n",
      "Iter: 11, loss: 0.01448412798345089\n",
      "Iter: 12, loss: 0.01481739804148674\n",
      "Iter: 13, loss: 0.01585579290986061\n",
      "Iter: 14, loss: 0.015088125132024288\n",
      "Iter: 15, loss: 0.014189892448484898\n",
      "Iter: 16, loss: 0.014590233564376831\n",
      "Iter: 17, loss: 0.014030560851097107\n",
      "Iter: 18, loss: 0.014501072466373444\n",
      "Iter: 19, loss: 0.013706115074455738\n",
      "Iter: 20, loss: 0.014175619930028915\n",
      "Iter: 21, loss: 0.014129242859780788\n",
      "Iter: 22, loss: 0.013829604722559452\n",
      "Iter: 23, loss: 0.013918042182922363\n",
      "Iter: 24, loss: 0.01380153838545084\n",
      "Iter: 25, loss: 0.014579208567738533\n",
      "Iter: 26, loss: 0.014408345334231853\n",
      "Iter: 27, loss: 0.013757500797510147\n",
      "Iter: 28, loss: 0.014586374163627625\n",
      "Iter: 29, loss: 0.014030367136001587\n",
      "Iter: 30, loss: 0.014972575008869171\n",
      "Iter: 31, loss: 0.013838625513017178\n",
      "Iter: 32, loss: 0.014057839289307594\n",
      "Iter: 33, loss: 0.014723531901836395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/octav/anaconda3/envs/mol_ot/lib/python3.6/site-packages/ot/lp/__init__.py:276: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  result_code_string = check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 34, loss: 0.014029206708073616\n",
      "Iter: 35, loss: 0.013968406245112419\n",
      "Iter: 36, loss: 0.014084894210100174\n",
      "Iter: 37, loss: 0.014040748588740826\n",
      "Iter: 38, loss: 0.014291472733020782\n",
      "Iter: 39, loss: 0.014438503421843052\n",
      "Iter: 40, loss: 0.01469825953245163\n",
      "Iter: 41, loss: 0.013522089459002018\n",
      "Iter: 42, loss: 0.013808383606374264\n",
      "Iter: 43, loss: 0.014365884475409985\n",
      "Iter: 44, loss: 0.013690410181879997\n",
      "Iter: 45, loss: 0.01380215585231781\n",
      "Iter: 46, loss: 0.013775057159364223\n",
      "Iter: 47, loss: 0.013525346294045448\n",
      "Iter: 48, loss: 0.013605396263301373\n",
      "Iter: 49, loss: 0.013963649980723858\n",
      "Iter: 50, loss: 0.013937383890151978\n",
      "Iter: 51, loss: 0.013569599017500877\n",
      "Iter: 52, loss: 0.013187862001359463\n",
      "Iter: 53, loss: 0.013646628707647324\n",
      "Iter: 54, loss: 0.013621378690004349\n",
      "Iter: 55, loss: 0.013175789266824722\n",
      "Iter: 56, loss: 0.013652865774929523\n",
      "Iter: 57, loss: 0.01333098765462637\n",
      "Iter: 58, loss: 0.013325710780918598\n",
      "Iter: 59, loss: 0.013774197548627853\n",
      "Iter: 60, loss: 0.013378186151385307\n",
      "Iter: 61, loss: 0.013967080041766167\n",
      "Iter: 62, loss: 0.013819216750562191\n",
      "Iter: 63, loss: 0.013878257013857365\n",
      "Iter: 64, loss: 0.013549194671213627\n",
      "Iter: 65, loss: 0.013185909017920494\n",
      "Iter: 66, loss: 0.013645118102431297\n",
      "Iter: 67, loss: 0.013439894653856754\n",
      "Iter: 68, loss: 0.013092362321913242\n",
      "Iter: 69, loss: 0.013488792814314365\n",
      "Iter: 70, loss: 0.013175700791180134\n",
      "Iter: 71, loss: 0.013136427849531174\n",
      "Iter: 72, loss: 0.013679327443242073\n",
      "Iter: 73, loss: 0.013437462039291859\n",
      "Iter: 74, loss: 0.013416965492069721\n",
      "Iter: 75, loss: 0.013332183472812176\n",
      "Iter: 76, loss: 0.013342847116291523\n",
      "Iter: 77, loss: 0.012805301696062088\n",
      "Iter: 78, loss: 0.013922800309956074\n",
      "Iter: 79, loss: 0.013392427936196327\n",
      "Iter: 80, loss: 0.013061540201306343\n",
      "Iter: 81, loss: 0.013268196024000645\n",
      "Iter: 82, loss: 0.0130225894972682\n",
      "Iter: 83, loss: 0.013009865768253803\n",
      "Iter: 84, loss: 0.013326502405107021\n",
      "Iter: 85, loss: 0.013354449532926083\n",
      "Iter: 86, loss: 0.013951610773801804\n",
      "Iter: 87, loss: 0.012694334611296654\n",
      "Iter: 88, loss: 0.013131109066307545\n",
      "Iter: 89, loss: 0.013477041386067867\n",
      "Iter: 90, loss: 0.01290386263281107\n",
      "Iter: 91, loss: 0.01305649895220995\n",
      "Iter: 92, loss: 0.013091722503304482\n",
      "Iter: 93, loss: 0.012810495682060719\n",
      "Iter: 94, loss: 0.013209645636379719\n",
      "Iter: 95, loss: 0.012732145376503468\n",
      "Iter: 96, loss: 0.01311113964766264\n",
      "Iter: 97, loss: 0.012965523637831211\n",
      "Iter: 98, loss: 0.013695057481527328\n",
      "Iter: 99, loss: 0.013150534592568874\n",
      "Iter: 100, loss: 0.012929410673677921\n",
      "Iter: 101, loss: 0.013197200372815132\n",
      "Iter: 102, loss: 0.012924632988870144\n",
      "Iter: 103, loss: 0.012940888293087482\n",
      "Iter: 104, loss: 0.013044989667832851\n",
      "Iter: 105, loss: 0.012706185691058636\n",
      "Iter: 106, loss: 0.012751391157507896\n",
      "Iter: 107, loss: 0.012302065268158913\n",
      "Iter: 108, loss: 0.01226859726011753\n",
      "Iter: 109, loss: 0.01288294605910778\n",
      "Iter: 110, loss: 0.01291897613555193\n",
      "Iter: 111, loss: 0.013150287792086601\n",
      "Iter: 112, loss: 0.012742201797664165\n",
      "Iter: 113, loss: 0.012524046003818512\n",
      "Iter: 114, loss: 0.012817999348044395\n",
      "Iter: 115, loss: 0.01327043678611517\n",
      "Iter: 116, loss: 0.012598437257111073\n",
      "Iter: 117, loss: 0.013395052403211594\n",
      "Iter: 118, loss: 0.012979235500097275\n",
      "Iter: 119, loss: 0.012825231067836285\n",
      "Iter: 120, loss: 0.012201842851936817\n",
      "Iter: 121, loss: 0.012268640100955963\n",
      "Iter: 122, loss: 0.012919865548610687\n",
      "Iter: 123, loss: 0.012532426044344902\n",
      "Iter: 124, loss: 0.012841210700571537\n",
      "29.537330150604248\n",
      "Iter: 0, loss: 0.013314880430698395\n",
      "Iter: 1, loss: 0.013072304427623749\n",
      "Iter: 2, loss: 0.012425108812749386\n",
      "Iter: 3, loss: 0.012226542457938194\n",
      "Iter: 4, loss: 0.012742315419018269\n",
      "Iter: 5, loss: 0.012629089877009392\n",
      "Iter: 6, loss: 0.012709146365523338\n",
      "Iter: 7, loss: 0.012753823772072792\n",
      "Iter: 8, loss: 0.012764287181198597\n",
      "Iter: 9, loss: 0.012713998556137085\n",
      "Iter: 10, loss: 0.012158598750829697\n",
      "Iter: 11, loss: 0.012460031546652317\n",
      "Iter: 12, loss: 0.012747890315949917\n",
      "Iter: 13, loss: 0.013656259514391422\n",
      "Iter: 14, loss: 0.012969165109097958\n",
      "Iter: 15, loss: 0.012183734215795994\n",
      "Iter: 16, loss: 0.012538234703242779\n",
      "Iter: 17, loss: 0.012031381018459797\n",
      "Iter: 18, loss: 0.012443693354725838\n",
      "Iter: 19, loss: 0.011741459369659424\n",
      "Iter: 20, loss: 0.012172562070190907\n",
      "Iter: 21, loss: 0.012155761942267418\n",
      "Iter: 22, loss: 0.011884421110153198\n",
      "Iter: 23, loss: 0.011965585872530937\n",
      "Iter: 24, loss: 0.01185271330177784\n",
      "Iter: 25, loss: 0.01257330272346735\n",
      "Iter: 26, loss: 0.012399480678141117\n",
      "Iter: 27, loss: 0.01184318121522665\n",
      "Iter: 28, loss: 0.012596805579960346\n",
      "Iter: 29, loss: 0.012090994045138359\n",
      "Iter: 30, loss: 0.012908052653074265\n",
      "Iter: 31, loss: 0.011919578537344933\n",
      "Iter: 32, loss: 0.012113019824028015\n",
      "Iter: 33, loss: 0.012709660455584526\n",
      "Iter: 34, loss: 0.012093188241124153\n",
      "Iter: 35, loss: 0.012045026756823063\n",
      "Iter: 36, loss: 0.012131157331168652\n",
      "Iter: 37, loss: 0.012106908485293388\n",
      "Iter: 38, loss: 0.012340560555458069\n",
      "Iter: 39, loss: 0.01249023899435997\n",
      "Iter: 40, loss: 0.012681416235864162\n",
      "Iter: 41, loss: 0.011644751764833927\n",
      "Iter: 42, loss: 0.011913170106709003\n",
      "Iter: 43, loss: 0.012414944358170033\n",
      "Iter: 44, loss: 0.011810775846242905\n",
      "Iter: 45, loss: 0.011921463534235954\n",
      "Iter: 46, loss: 0.011929511092603207\n",
      "Iter: 47, loss: 0.011690777726471424\n",
      "Iter: 48, loss: 0.011735009029507637\n",
      "Iter: 49, loss: 0.01206178031861782\n",
      "Iter: 50, loss: 0.012021519243717194\n",
      "Iter: 51, loss: 0.011763849295675755\n",
      "Iter: 52, loss: 0.01140161044895649\n",
      "Iter: 53, loss: 0.011802662163972855\n",
      "Iter: 54, loss: 0.011788253672420979\n",
      "Iter: 55, loss: 0.011401958763599396\n",
      "Iter: 56, loss: 0.011835065670311451\n",
      "Iter: 57, loss: 0.011510658077895641\n",
      "Iter: 58, loss: 0.011539905332028866\n",
      "Iter: 59, loss: 0.011929831467568874\n",
      "Iter: 60, loss: 0.011599373072385788\n",
      "Iter: 61, loss: 0.012119518592953682\n",
      "Iter: 62, loss: 0.011971121653914452\n",
      "Iter: 63, loss: 0.012051412835717201\n",
      "Iter: 64, loss: 0.011755188927054405\n",
      "Iter: 65, loss: 0.011448003351688385\n",
      "Iter: 66, loss: 0.011816784739494324\n",
      "Iter: 67, loss: 0.011647269129753113\n",
      "Iter: 68, loss: 0.011348815634846687\n",
      "Iter: 69, loss: 0.011701228097081184\n",
      "Iter: 70, loss: 0.011422358453273773\n",
      "Iter: 71, loss: 0.011390525847673416\n",
      "Iter: 72, loss: 0.011878710240125656\n",
      "Iter: 73, loss: 0.011663753539323807\n",
      "Iter: 74, loss: 0.01164185255765915\n",
      "Iter: 75, loss: 0.011557336896657944\n",
      "Iter: 76, loss: 0.011599934659898281\n",
      "Iter: 77, loss: 0.011114384047687054\n",
      "Iter: 78, loss: 0.01211998425424099\n",
      "Iter: 79, loss: 0.011645146645605564\n",
      "Iter: 80, loss: 0.011326998472213745\n",
      "Iter: 81, loss: 0.011548752896487713\n",
      "Iter: 82, loss: 0.011325119994580746\n",
      "Iter: 83, loss: 0.01129138097167015\n",
      "Iter: 84, loss: 0.011611188761889935\n",
      "Iter: 85, loss: 0.011603065766394138\n",
      "Iter: 86, loss: 0.012159885838627815\n",
      "Iter: 87, loss: 0.011043562553822994\n",
      "Iter: 88, loss: 0.011423371732234955\n",
      "Iter: 89, loss: 0.011760825291275978\n",
      "Iter: 90, loss: 0.011246079578995705\n",
      "Iter: 91, loss: 0.011364420875906944\n",
      "Iter: 92, loss: 0.011403114534914494\n",
      "Iter: 93, loss: 0.01116891484707594\n",
      "Iter: 94, loss: 0.011508312076330185\n",
      "Iter: 95, loss: 0.011088106781244278\n",
      "Iter: 96, loss: 0.011393163353204727\n",
      "Iter: 97, loss: 0.01128936093300581\n",
      "Iter: 98, loss: 0.011958283372223377\n",
      "Iter: 99, loss: 0.01149334292858839\n",
      "Iter: 100, loss: 0.011270086281001568\n",
      "Iter: 101, loss: 0.011518120765686035\n",
      "Iter: 102, loss: 0.011262554675340652\n",
      "Iter: 103, loss: 0.01129167154431343\n",
      "Iter: 104, loss: 0.01140816044062376\n",
      "Iter: 105, loss: 0.01107370387762785\n",
      "Iter: 106, loss: 0.011119581758975983\n",
      "Iter: 107, loss: 0.01071028970181942\n",
      "Iter: 108, loss: 0.010692915879189968\n",
      "Iter: 109, loss: 0.01124150212854147\n",
      "Iter: 110, loss: 0.011276218108832836\n",
      "Iter: 111, loss: 0.011487564072012901\n",
      "Iter: 112, loss: 0.011135484091937542\n",
      "Iter: 113, loss: 0.010949484072625637\n",
      "Iter: 114, loss: 0.011207520961761475\n",
      "Iter: 115, loss: 0.011620252393186092\n",
      "Iter: 116, loss: 0.011009725742042065\n",
      "Iter: 117, loss: 0.01173393428325653\n",
      "Iter: 118, loss: 0.01136074960231781\n",
      "Iter: 119, loss: 0.011219275183975697\n",
      "Iter: 120, loss: 0.010645907372236252\n",
      "Iter: 121, loss: 0.010703658685088158\n",
      "Iter: 122, loss: 0.011349334381520748\n",
      "Iter: 123, loss: 0.010967321693897247\n",
      "Iter: 124, loss: 0.011241091415286064\n",
      "29.446598529815674\n",
      "Iter: 0, loss: 0.011667764745652676\n",
      "Iter: 1, loss: 0.011460945941507816\n",
      "Iter: 2, loss: 0.010861878283321857\n",
      "Iter: 3, loss: 0.01068966556340456\n",
      "Iter: 4, loss: 0.0111648915335536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 5, loss: 0.011062366887927055\n",
      "Iter: 6, loss: 0.011161036789417267\n",
      "Iter: 7, loss: 0.011199872009456158\n",
      "Iter: 8, loss: 0.01121568400412798\n",
      "Iter: 9, loss: 0.011119906790554523\n",
      "Iter: 10, loss: 0.010642554610967636\n",
      "Iter: 11, loss: 0.010950086638331413\n",
      "Iter: 12, loss: 0.011198685504496098\n",
      "Iter: 13, loss: 0.012002616189420223\n",
      "Iter: 14, loss: 0.011378305032849312\n",
      "Iter: 15, loss: 0.010684579610824585\n",
      "Iter: 16, loss: 0.010998154990375042\n",
      "Iter: 17, loss: 0.010535567998886108\n",
      "Iter: 18, loss: 0.010907185263931751\n",
      "Iter: 19, loss: 0.010277491994202137\n",
      "Iter: 20, loss: 0.010675954632461071\n",
      "Iter: 21, loss: 0.010673644952476025\n",
      "Iter: 22, loss: 0.010425758548080921\n",
      "Iter: 23, loss: 0.010501904413104057\n",
      "Iter: 24, loss: 0.010392596945166588\n",
      "Iter: 25, loss: 0.011063323356211185\n",
      "Iter: 26, loss: 0.010888702236115932\n",
      "Iter: 27, loss: 0.01040828786790371\n",
      "Iter: 28, loss: 0.011097054928541183\n",
      "Iter: 29, loss: 0.010628379881381989\n",
      "Iter: 30, loss: 0.011344282887876034\n",
      "Iter: 31, loss: 0.010476179420948029\n",
      "Iter: 32, loss: 0.010651170276105404\n",
      "Iter: 33, loss: 0.011188444681465626\n",
      "Iter: 34, loss: 0.010629061609506607\n",
      "Iter: 35, loss: 0.010597018525004387\n",
      "Iter: 36, loss: 0.01066096592694521\n",
      "Iter: 37, loss: 0.010648626834154129\n",
      "Iter: 38, loss: 0.010862833820283413\n",
      "Iter: 39, loss: 0.011015648953616619\n",
      "Iter: 40, loss: 0.011154345236718655\n",
      "Iter: 41, loss: 0.010229545645415783\n",
      "Iter: 42, loss: 0.010479368269443512\n",
      "Iter: 43, loss: 0.010930841788649559\n",
      "Iter: 44, loss: 0.010392403230071068\n",
      "Iter: 45, loss: 0.010490423999726772\n",
      "Iter: 46, loss: 0.010525756515562534\n",
      "Iter: 47, loss: 0.010301548056304455\n",
      "Iter: 48, loss: 0.010320812463760376\n",
      "Iter: 49, loss: 0.010620756074786186\n",
      "Iter: 50, loss: 0.010565743781626225\n",
      "Iter: 51, loss: 0.010383694432675838\n",
      "Iter: 52, loss: 0.010044017806649208\n",
      "Iter: 53, loss: 0.010402665473520756\n",
      "Iter: 54, loss: 0.010395064018666744\n",
      "Iter: 55, loss: 0.010052090510725975\n",
      "Iter: 56, loss: 0.010448195040225983\n",
      "Iter: 57, loss: 0.010127217508852482\n",
      "Iter: 58, loss: 0.010172169655561447\n",
      "Iter: 59, loss: 0.010522423312067986\n",
      "Iter: 60, loss: 0.010242179036140442\n",
      "Iter: 61, loss: 0.01070042047649622\n",
      "Iter: 62, loss: 0.010553056374192238\n",
      "Iter: 63, loss: 0.010649528354406357\n",
      "Iter: 64, loss: 0.010387469083070755\n",
      "Iter: 65, loss: 0.010120544582605362\n",
      "Iter: 66, loss: 0.010422086343169212\n",
      "Iter: 67, loss: 0.010274081490933895\n",
      "Iter: 68, loss: 0.010013371706008911\n",
      "Iter: 69, loss: 0.010328566655516624\n",
      "Iter: 70, loss: 0.010073760524392128\n",
      "Iter: 71, loss: 0.010047375224530697\n",
      "Iter: 72, loss: 0.010494791902601719\n",
      "Iter: 73, loss: 0.010300103574991226\n",
      "Iter: 74, loss: 0.010273915715515614\n",
      "Iter: 75, loss: 0.010197915136814117\n",
      "Iter: 76, loss: 0.010256855748593807\n",
      "Iter: 77, loss: 0.009818772785365582\n",
      "Iter: 78, loss: 0.010727137327194214\n",
      "Iter: 79, loss: 0.010296415537595749\n",
      "Iter: 80, loss: 0.009991670958697796\n",
      "Iter: 81, loss: 0.01022023893892765\n",
      "Iter: 82, loss: 0.010018112137913704\n",
      "Iter: 83, loss: 0.00996632780879736\n",
      "Iter: 84, loss: 0.010290625505149364\n",
      "Iter: 85, loss: 0.010249576531350613\n",
      "Iter: 86, loss: 0.010774509981274605\n",
      "Iter: 87, loss: 0.009768865071237087\n",
      "Iter: 88, loss: 0.010106134228408337\n",
      "Iter: 89, loss: 0.010428542271256447\n",
      "Iter: 90, loss: 0.009965385310351849\n",
      "Iter: 91, loss: 0.010054094716906548\n",
      "Iter: 92, loss: 0.01010038610547781\n",
      "Iter: 93, loss: 0.00989636778831482\n",
      "Iter: 94, loss: 0.010188418440520763\n",
      "Iter: 95, loss: 0.009816889651119709\n",
      "Iter: 96, loss: 0.010063408873975277\n",
      "Iter: 97, loss: 0.009988554753363132\n",
      "Iter: 98, loss: 0.010609881952404976\n",
      "Iter: 99, loss: 0.010209561325609684\n",
      "Iter: 100, loss: 0.009977388195693493\n",
      "Iter: 101, loss: 0.010211776942014694\n",
      "Iter: 102, loss: 0.009973909705877304\n",
      "Iter: 103, loss: 0.01001246739178896\n",
      "Iter: 104, loss: 0.010130491107702255\n",
      "Iter: 105, loss: 0.009803611785173416\n",
      "Iter: 106, loss: 0.009846883825957775\n",
      "Iter: 107, loss: 0.009474975988268852\n",
      "Iter: 108, loss: 0.009464679285883904\n",
      "Iter: 109, loss: 0.009964065626263618\n",
      "Iter: 110, loss: 0.009992717765271664\n",
      "Iter: 111, loss: 0.010191707871854305\n",
      "Iter: 112, loss: 0.009876231662929058\n",
      "Iter: 113, loss: 0.009720338508486748\n",
      "Iter: 114, loss: 0.009946377016603947\n",
      "Iter: 115, loss: 0.01033253688365221\n",
      "Iter: 116, loss: 0.009766650386154652\n",
      "Iter: 117, loss: 0.010431382805109024\n",
      "Iter: 118, loss: 0.010092190466821194\n",
      "Iter: 119, loss: 0.009966534562408924\n",
      "Iter: 120, loss: 0.009434216655790806\n",
      "Iter: 121, loss: 0.009483857080340385\n",
      "Iter: 122, loss: 0.010118857026100159\n",
      "Iter: 123, loss: 0.009743839502334595\n",
      "Iter: 124, loss: 0.009989193640649319\n",
      "29.29327630996704\n",
      "Iter: 0, loss: 0.010371473617851734\n",
      "Iter: 1, loss: 0.010188944637775421\n",
      "Iter: 2, loss: 0.009635228663682938\n",
      "Iter: 3, loss: 0.009484739042818546\n",
      "Iter: 4, loss: 0.009919742122292519\n",
      "Iter: 5, loss: 0.009832247160375118\n",
      "Iter: 6, loss: 0.00994191039353609\n",
      "Iter: 7, loss: 0.009978990070521832\n",
      "Iter: 8, loss: 0.0100009236484766\n",
      "Iter: 9, loss: 0.009866639040410519\n",
      "Iter: 10, loss: 0.009455544874072075\n",
      "Iter: 11, loss: 0.009758294560015202\n",
      "Iter: 12, loss: 0.009975739754736423\n",
      "Iter: 13, loss: 0.01069493405520916\n",
      "Iter: 14, loss: 0.010121027007699013\n",
      "Iter: 15, loss: 0.009500079788267612\n",
      "Iter: 16, loss: 0.009786417707800865\n",
      "Iter: 17, loss: 0.009354590438306332\n",
      "Iter: 18, loss: 0.009693102911114693\n",
      "Iter: 19, loss: 0.00912026222795248\n",
      "Iter: 20, loss: 0.009495128877460957\n",
      "Iter: 21, loss: 0.009505455382168293\n",
      "Iter: 22, loss: 0.009276050142943859\n",
      "Iter: 23, loss: 0.009345192462205887\n",
      "Iter: 24, loss: 0.00924364011734724\n",
      "Iter: 25, loss: 0.00986273493617773\n",
      "Iter: 26, loss: 0.009693327359855175\n",
      "Iter: 27, loss: 0.009277323260903358\n",
      "Iter: 28, loss: 0.009908698499202728\n",
      "Iter: 29, loss: 0.009469646029174328\n",
      "Iter: 30, loss: 0.010103556327521801\n",
      "Iter: 31, loss: 0.009336178191006184\n",
      "Iter: 32, loss: 0.009492945857346058\n",
      "Iter: 33, loss: 0.009978214278817177\n",
      "Iter: 34, loss: 0.009465750306844711\n",
      "Iter: 35, loss: 0.009444743394851685\n",
      "Iter: 36, loss: 0.009496851824223995\n",
      "Iter: 37, loss: 0.009492343291640282\n",
      "Iter: 38, loss: 0.009686349891126156\n",
      "Iter: 39, loss: 0.009842488914728165\n",
      "Iter: 40, loss: 0.009942649863660336\n",
      "Iter: 41, loss: 0.0091019868850708\n",
      "Iter: 42, loss: 0.009338763542473316\n",
      "Iter: 43, loss: 0.00974807795137167\n",
      "Iter: 44, loss: 0.009267347864806652\n",
      "Iter: 45, loss: 0.009351657703518867\n",
      "Iter: 46, loss: 0.009413403458893299\n",
      "Iter: 47, loss: 0.009195818565785885\n",
      "Iter: 48, loss: 0.009197277948260307\n",
      "Iter: 49, loss: 0.009471569210290909\n",
      "Iter: 50, loss: 0.009408380836248398\n",
      "Iter: 51, loss: 0.009286561980843544\n",
      "Iter: 52, loss: 0.008962288498878479\n",
      "Iter: 53, loss: 0.009291836060583591\n",
      "Iter: 54, loss: 0.009282526560127735\n",
      "Iter: 55, loss: 0.008977053686976433\n",
      "Iter: 56, loss: 0.009341159835457802\n",
      "Iter: 57, loss: 0.009023759514093399\n",
      "Iter: 58, loss: 0.009081550873816013\n",
      "Iter: 59, loss: 0.00939715001732111\n",
      "Iter: 60, loss: 0.009157619439065456\n",
      "Iter: 61, loss: 0.009567983448505402\n",
      "Iter: 62, loss: 0.00941977184265852\n",
      "Iter: 63, loss: 0.009524518623948097\n",
      "Iter: 64, loss: 0.009289608336985111\n",
      "Iter: 65, loss: 0.009058235213160515\n",
      "Iter: 66, loss: 0.009305517189204693\n",
      "Iter: 67, loss: 0.009176819585263729\n",
      "Iter: 68, loss: 0.008945056237280369\n",
      "Iter: 69, loss: 0.009228195995092392\n",
      "Iter: 70, loss: 0.008995162323117256\n",
      "Iter: 71, loss: 0.008968501351773739\n",
      "Iter: 72, loss: 0.009385326877236366\n",
      "Iter: 73, loss: 0.009202869608998299\n",
      "Iter: 74, loss: 0.009178762324154377\n",
      "Iter: 75, loss: 0.009112196043133736\n",
      "Iter: 76, loss: 0.009176941588521004\n",
      "Iter: 77, loss: 0.008779761381447315\n",
      "Iter: 78, loss: 0.009609107859432697\n",
      "Iter: 79, loss: 0.00921250693500042\n",
      "Iter: 80, loss: 0.008922142907977104\n",
      "Iter: 81, loss: 0.009153042919933796\n",
      "Iter: 82, loss: 0.00896687712520361\n",
      "Iter: 83, loss: 0.00889440905302763\n",
      "Iter: 84, loss: 0.009227767586708069\n",
      "Iter: 85, loss: 0.00916348397731781\n",
      "Iter: 86, loss: 0.009655955247581005\n",
      "Iter: 87, loss: 0.008742458187043667\n",
      "Iter: 88, loss: 0.009048267267644405\n",
      "Iter: 89, loss: 0.009349824860692024\n",
      "Iter: 90, loss: 0.00893372017890215\n",
      "Iter: 91, loss: 0.008997487835586071\n",
      "Iter: 92, loss: 0.009052049368619919\n",
      "Iter: 93, loss: 0.008869816549122334\n",
      "Iter: 94, loss: 0.009126110002398491\n",
      "Iter: 95, loss: 0.008793085813522339\n",
      "Iter: 96, loss: 0.008992972783744335\n",
      "Iter: 97, loss: 0.008936102502048016\n",
      "Iter: 98, loss: 0.00951679889112711\n",
      "Iter: 99, loss: 0.00917506031692028\n",
      "Iter: 100, loss: 0.00893212016671896\n",
      "Iter: 101, loss: 0.009156395681202412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 102, loss: 0.0089333551004529\n",
      "Iter: 103, loss: 0.008979018777608871\n",
      "Iter: 104, loss: 0.009097672067582607\n",
      "Iter: 105, loss: 0.008776803500950336\n",
      "Iter: 106, loss: 0.00881857518106699\n",
      "Iter: 107, loss: 0.008477690629661083\n",
      "Iter: 108, loss: 0.008470491506159306\n",
      "Iter: 109, loss: 0.008933167904615402\n",
      "Iter: 110, loss: 0.008951665833592415\n",
      "Iter: 111, loss: 0.00914007518440485\n",
      "Iter: 112, loss: 0.008857402950525284\n",
      "Iter: 113, loss: 0.008729237131774426\n",
      "Iter: 114, loss: 0.008920636959373951\n",
      "Iter: 115, loss: 0.009286978282034397\n",
      "Iter: 116, loss: 0.008761647157371044\n",
      "Iter: 117, loss: 0.00937346089631319\n",
      "Iter: 118, loss: 0.009057339280843735\n",
      "Iter: 119, loss: 0.008951058611273766\n",
      "Iter: 120, loss: 0.008448668755590916\n",
      "Iter: 121, loss: 0.008493450470268726\n",
      "Iter: 122, loss: 0.009120439179241657\n",
      "Iter: 123, loss: 0.008751928806304932\n",
      "Iter: 124, loss: 0.00897366926074028\n",
      "29.838321924209595\n",
      "Iter: 0, loss: 0.009316830895841122\n",
      "Iter: 1, loss: 0.009152702987194061\n",
      "Iter: 2, loss: 0.008635593578219414\n",
      "Iter: 3, loss: 0.008508560247719288\n",
      "Iter: 4, loss: 0.00891126599162817\n",
      "Iter: 5, loss: 0.008829684928059578\n",
      "Iter: 6, loss: 0.008945103734731674\n",
      "Iter: 7, loss: 0.008985408581793308\n",
      "Iter: 8, loss: 0.009009350091218948\n",
      "Iter: 9, loss: 0.008846933953464031\n",
      "Iter: 10, loss: 0.008492414839565754\n",
      "Iter: 11, loss: 0.008785629644989967\n",
      "Iter: 12, loss: 0.008977428078651428\n",
      "Iter: 13, loss: 0.009628365747630596\n",
      "Iter: 14, loss: 0.009097907692193985\n",
      "Iter: 15, loss: 0.008532878011465073\n",
      "Iter: 16, loss: 0.00879878830164671\n",
      "Iter: 17, loss: 0.008397297002375126\n",
      "Iter: 18, loss: 0.008702792227268219\n",
      "Iter: 19, loss: 0.008174949325621128\n",
      "Iter: 20, loss: 0.008527033030986786\n",
      "Iter: 21, loss: 0.00855162087827921\n",
      "Iter: 22, loss: 0.008337544277310371\n",
      "Iter: 23, loss: 0.00839995127171278\n",
      "Iter: 24, loss: 0.008304189890623093\n",
      "Iter: 25, loss: 0.008880122564733028\n",
      "Iter: 26, loss: 0.008713947609066963\n",
      "Iter: 27, loss: 0.008352432399988174\n",
      "Iter: 28, loss: 0.00893036276102066\n",
      "Iter: 29, loss: 0.008525214157998562\n",
      "Iter: 30, loss: 0.00908681657165289\n",
      "Iter: 31, loss: 0.008403888903558254\n",
      "Iter: 32, loss: 0.008543004281818867\n",
      "Iter: 33, loss: 0.008988572284579277\n",
      "Iter: 34, loss: 0.008513711392879486\n",
      "Iter: 35, loss: 0.008503813296556473\n",
      "Iter: 36, loss: 0.008544988930225372\n",
      "Iter: 37, loss: 0.008544067852199078\n",
      "Iter: 38, loss: 0.008723342791199684\n",
      "Iter: 39, loss: 0.008882912807166576\n",
      "Iter: 40, loss: 0.008948694914579391\n",
      "Iter: 41, loss: 0.00817838218063116\n",
      "Iter: 42, loss: 0.008404916152358055\n",
      "Iter: 43, loss: 0.008777182549238205\n",
      "Iter: 44, loss: 0.008347677066922188\n",
      "Iter: 45, loss: 0.008413812145590782\n",
      "Iter: 46, loss: 0.00849919207394123\n",
      "Iter: 47, loss: 0.00829186849296093\n",
      "Iter: 48, loss: 0.008274434134364128\n",
      "Iter: 49, loss: 0.008530855178833008\n",
      "Iter: 50, loss: 0.008454767055809498\n",
      "Iter: 51, loss: 0.00838245265185833\n",
      "Iter: 52, loss: 0.008069214411079884\n",
      "Iter: 53, loss: 0.008379213511943817\n",
      "Iter: 54, loss: 0.008368582464754581\n",
      "Iter: 55, loss: 0.008091633208096027\n",
      "Iter: 56, loss: 0.008430084213614464\n",
      "Iter: 57, loss: 0.00811934657394886\n",
      "Iter: 58, loss: 0.00818270817399025\n",
      "Iter: 59, loss: 0.008472094312310219\n",
      "Iter: 60, loss: 0.008262766525149345\n",
      "Iter: 61, loss: 0.008636623620986938\n",
      "Iter: 62, loss: 0.008489010855555534\n",
      "Iter: 63, loss: 0.008599553257226944\n",
      "Iter: 64, loss: 0.008389833383262157\n",
      "Iter: 65, loss: 0.008181569166481495\n",
      "Iter: 66, loss: 0.00838919822126627\n",
      "Iter: 67, loss: 0.008275119587779045\n",
      "Iter: 68, loss: 0.008064991794526577\n",
      "Iter: 69, loss: 0.008322891779243946\n",
      "Iter: 70, loss: 0.00810831505805254\n",
      "Iter: 71, loss: 0.00807802565395832\n",
      "Iter: 72, loss: 0.008468758314847946\n",
      "Iter: 73, loss: 0.00829724594950676\n",
      "Iter: 74, loss: 0.008276456966996193\n",
      "Iter: 75, loss: 0.008218718692660332\n",
      "Iter: 76, loss: 0.008284180425107479\n",
      "Iter: 77, loss: 0.007921918295323849\n",
      "Iter: 78, loss: 0.00868632085621357\n",
      "Iter: 79, loss: 0.008314577862620354\n",
      "Iter: 80, loss: 0.008041257038712502\n",
      "Iter: 81, loss: 0.008272749371826649\n",
      "Iter: 82, loss: 0.008096463046967983\n",
      "Iter: 83, loss: 0.008014188148081303\n",
      "Iter: 84, loss: 0.008349176496267319\n",
      "Iter: 85, loss: 0.008265646174550056\n",
      "Iter: 86, loss: 0.008732395246624947\n",
      "Iter: 87, loss: 0.007894445210695267\n",
      "Iter: 88, loss: 0.008173095062375069\n",
      "Iter: 89, loss: 0.008457157760858536\n",
      "Iter: 90, loss: 0.008081306703388691\n",
      "Iter: 91, loss: 0.008121746592223644\n",
      "Iter: 92, loss: 0.008185038343071938\n",
      "Iter: 93, loss: 0.008018202148377895\n",
      "Iter: 94, loss: 0.008243510499596596\n",
      "Iter: 95, loss: 0.007945507764816284\n",
      "Iter: 96, loss: 0.008109032176434994\n",
      "Iter: 97, loss: 0.008063276298344135\n",
      "Iter: 98, loss: 0.00861044880002737\n",
      "Iter: 99, loss: 0.008314959704875946\n",
      "Iter: 100, loss: 0.008067814633250237\n",
      "Iter: 101, loss: 0.008280683308839798\n",
      "Iter: 102, loss: 0.00807051733136177\n",
      "Iter: 103, loss: 0.008124709129333496\n",
      "Iter: 104, loss: 0.008242852985858917\n",
      "Iter: 105, loss: 0.007924414239823818\n",
      "Iter: 106, loss: 0.007966598495841026\n",
      "Iter: 107, loss: 0.0076532126404345036\n",
      "Iter: 108, loss: 0.0076480102725327015\n",
      "Iter: 109, loss: 0.00807920005172491\n",
      "Iter: 110, loss: 0.008088226430118084\n",
      "Iter: 111, loss: 0.008266828022897243\n",
      "Iter: 112, loss: 0.008010362274944782\n",
      "Iter: 113, loss: 0.007905607111752033\n",
      "Iter: 114, loss: 0.008069537580013275\n",
      "Iter: 115, loss: 0.008417828939855099\n",
      "Iter: 116, loss: 0.007924949750304222\n",
      "Iter: 117, loss: 0.008494368754327297\n",
      "Iter: 118, loss: 0.008198495022952557\n",
      "Iter: 119, loss: 0.00810861587524414\n",
      "Iter: 120, loss: 0.0076327575370669365\n",
      "Iter: 121, loss: 0.007668778300285339\n",
      "Iter: 122, loss: 0.008286044001579285\n",
      "Iter: 123, loss: 0.00792170874774456\n",
      "Iter: 124, loss: 0.008130456320941448\n",
      "28.85675859451294\n",
      "Iter: 0, loss: 0.008437815122306347\n",
      "Iter: 1, loss: 0.008288823999464512\n",
      "Iter: 2, loss: 0.007805409841239452\n",
      "Iter: 3, loss: 0.007696966640651226\n",
      "Iter: 4, loss: 0.008070419542491436\n",
      "Iter: 5, loss: 0.007996401749551296\n",
      "Iter: 6, loss: 0.00811783131211996\n",
      "Iter: 7, loss: 0.008158355951309204\n",
      "Iter: 8, loss: 0.008183851838111877\n",
      "Iter: 9, loss: 0.008000238798558712\n",
      "Iter: 10, loss: 0.007689017336815596\n",
      "Iter: 11, loss: 0.007975516840815544\n",
      "Iter: 12, loss: 0.00814067292958498\n",
      "Iter: 13, loss: 0.008736792951822281\n",
      "Iter: 14, loss: 0.00824093259871006\n",
      "Iter: 15, loss: 0.007724041119217873\n",
      "Iter: 16, loss: 0.007972732186317444\n",
      "Iter: 17, loss: 0.007596449926495552\n",
      "Iter: 18, loss: 0.007876357063651085\n",
      "Iter: 19, loss: 0.007386018522083759\n",
      "Iter: 20, loss: 0.007721141446381807\n",
      "Iter: 21, loss: 0.007757623679935932\n",
      "Iter: 22, loss: 0.0075547052547335625\n",
      "Iter: 23, loss: 0.007611276116222143\n",
      "Iter: 24, loss: 0.007519904524087906\n",
      "Iter: 25, loss: 0.008059588260948658\n",
      "Iter: 26, loss: 0.007894929498434067\n",
      "Iter: 27, loss: 0.007581198588013649\n",
      "Iter: 28, loss: 0.008110779337584972\n",
      "Iter: 29, loss: 0.00773449894040823\n",
      "Iter: 30, loss: 0.008235331624746323\n",
      "Iter: 31, loss: 0.007623498793691397\n",
      "Iter: 32, loss: 0.007750425487756729\n",
      "Iter: 33, loss: 0.00816422887146473\n",
      "Iter: 34, loss: 0.0077127693220973015\n",
      "Iter: 35, loss: 0.0077162450179457664\n",
      "Iter: 36, loss: 0.007748651783913374\n",
      "Iter: 37, loss: 0.007750900462269783\n",
      "Iter: 38, loss: 0.007917221635580063\n",
      "Iter: 39, loss: 0.00807971227914095\n",
      "Iter: 40, loss: 0.00811953004449606\n",
      "Iter: 41, loss: 0.007405691780149937\n",
      "Iter: 42, loss: 0.00762251066043973\n",
      "Iter: 43, loss: 0.00796250719577074\n",
      "Iter: 44, loss: 0.0075799524784088135\n",
      "Iter: 45, loss: 0.007628589868545532\n",
      "Iter: 46, loss: 0.007732892408967018\n",
      "Iter: 47, loss: 0.0075278934091329575\n",
      "Iter: 48, loss: 0.0075021847151219845\n",
      "Iter: 49, loss: 0.007741663604974747\n",
      "Iter: 50, loss: 0.00765622965991497\n",
      "Iter: 51, loss: 0.007623642683029175\n",
      "Iter: 52, loss: 0.0073213279247283936\n",
      "Iter: 53, loss: 0.00761177483946085\n",
      "Iter: 54, loss: 0.007602898869663477\n",
      "Iter: 55, loss: 0.007351026404649019\n",
      "Iter: 56, loss: 0.007666017394512892\n",
      "Iter: 57, loss: 0.007360025309026241\n",
      "Iter: 58, loss: 0.007430371828377247\n",
      "Iter: 59, loss: 0.007695453241467476\n",
      "Iter: 60, loss: 0.007513268385082483\n",
      "Iter: 61, loss: 0.007853292860090733\n",
      "Iter: 62, loss: 0.007706144824624062\n",
      "Iter: 63, loss: 0.007824688218533993\n",
      "Iter: 64, loss: 0.0076288217678666115\n",
      "Iter: 65, loss: 0.007445179857313633\n",
      "Iter: 66, loss: 0.007621122989803553\n",
      "Iter: 67, loss: 0.007512621581554413\n",
      "Iter: 68, loss: 0.007324607111513615\n",
      "Iter: 69, loss: 0.0075626675970852375\n",
      "Iter: 70, loss: 0.007361899595707655\n",
      "Iter: 71, loss: 0.007330442778766155\n",
      "Iter: 72, loss: 0.007697668392211199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 73, loss: 0.007536970544606447\n",
      "Iter: 74, loss: 0.007517963647842407\n",
      "Iter: 75, loss: 0.007469389121979475\n",
      "Iter: 76, loss: 0.0075345225632190704\n",
      "Iter: 77, loss: 0.007199797313660383\n",
      "Iter: 78, loss: 0.007907253690063953\n",
      "Iter: 79, loss: 0.007559444755315781\n",
      "Iter: 80, loss: 0.007300992496311665\n",
      "Iter: 81, loss: 0.007529605645686388\n",
      "Iter: 82, loss: 0.00736561045050621\n",
      "Iter: 83, loss: 0.007269708905369043\n",
      "Iter: 84, loss: 0.0076103536412119865\n",
      "Iter: 85, loss: 0.007507013622671366\n",
      "Iter: 86, loss: 0.007951815612614155\n",
      "Iter: 87, loss: 0.007180726155638695\n",
      "Iter: 88, loss: 0.007435615174472332\n",
      "Iter: 89, loss: 0.007705459836870432\n",
      "Iter: 90, loss: 0.0073601980693638325\n",
      "Iter: 91, loss: 0.0073839761316776276\n",
      "Iter: 92, loss: 0.0074530672281980515\n",
      "Iter: 93, loss: 0.007299473974853754\n",
      "Iter: 94, loss: 0.007499695755541325\n",
      "Iter: 95, loss: 0.007230495568364859\n",
      "Iter: 96, loss: 0.007363138720393181\n",
      "Iter: 97, loss: 0.007327426690608263\n",
      "Iter: 98, loss: 0.00784387905150652\n",
      "Iter: 99, loss: 0.007590943947434425\n",
      "Iter: 100, loss: 0.007337769027799368\n",
      "Iter: 101, loss: 0.007541488390415907\n",
      "Iter: 102, loss: 0.007343053352087736\n",
      "Iter: 103, loss: 0.007403324358165264\n",
      "Iter: 104, loss: 0.007520151790231466\n",
      "Iter: 105, loss: 0.007206809241324663\n",
      "Iter: 106, loss: 0.007246650289744139\n",
      "Iter: 107, loss: 0.00695708068087697\n",
      "Iter: 108, loss: 0.006954506505280733\n",
      "Iter: 109, loss: 0.007357629016041756\n",
      "Iter: 110, loss: 0.007359635084867477\n",
      "Iter: 111, loss: 0.007528137881308794\n",
      "Iter: 112, loss: 0.007294313050806522\n",
      "Iter: 113, loss: 0.007211612071841955\n",
      "Iter: 114, loss: 0.007349670398980379\n",
      "Iter: 115, loss: 0.007682415656745434\n",
      "Iter: 116, loss: 0.007219148334115744\n",
      "Iter: 117, loss: 0.007750747725367546\n",
      "Iter: 118, loss: 0.007472805678844452\n",
      "Iter: 119, loss: 0.007395237684249878\n",
      "Iter: 120, loss: 0.006942263338714838\n",
      "Iter: 121, loss: 0.006973007693886757\n",
      "Iter: 122, loss: 0.007579068187624216\n",
      "Iter: 123, loss: 0.0072206659242510796\n",
      "Iter: 124, loss: 0.007416360080242157\n",
      "29.160040855407715\n",
      "Iter: 0, loss: 0.0076905181631445885\n",
      "Iter: 1, loss: 0.007560637779533863\n",
      "Iter: 2, loss: 0.0071031744591891766\n",
      "Iter: 3, loss: 0.007010100409388542\n",
      "Iter: 4, loss: 0.007358725648373365\n",
      "Iter: 5, loss: 0.007290873676538467\n",
      "Iter: 6, loss: 0.007414945401251316\n",
      "Iter: 7, loss: 0.007455278653651476\n",
      "Iter: 8, loss: 0.007484257221221924\n",
      "Iter: 9, loss: 0.007284163497388363\n",
      "Iter: 10, loss: 0.007003454957157373\n",
      "Iter: 11, loss: 0.0072883134707808495\n",
      "Iter: 12, loss: 0.007431205827742815\n",
      "Iter: 13, loss: 0.007983972318470478\n",
      "Iter: 14, loss: 0.007514472585171461\n",
      "Iter: 15, loss: 0.007036710623651743\n",
      "Iter: 16, loss: 0.007273678667843342\n",
      "Iter: 17, loss: 0.006920421961694956\n",
      "Iter: 18, loss: 0.0071741328574717045\n",
      "Iter: 19, loss: 0.00671799574047327\n",
      "Iter: 20, loss: 0.00703766942024231\n",
      "Iter: 21, loss: 0.007083529140800238\n",
      "Iter: 22, loss: 0.006889968179166317\n",
      "Iter: 23, loss: 0.0069421869702637196\n",
      "Iter: 24, loss: 0.006858048960566521\n",
      "Iter: 25, loss: 0.007362771779298782\n",
      "Iter: 26, loss: 0.00720049487426877\n",
      "Iter: 27, loss: 0.00692515866830945\n",
      "Iter: 28, loss: 0.007413456682115793\n",
      "Iter: 29, loss: 0.007063603959977627\n",
      "Iter: 30, loss: 0.007512650452554226\n",
      "Iter: 31, loss: 0.006959171034395695\n",
      "Iter: 32, loss: 0.007074200082570314\n",
      "Iter: 33, loss: 0.0074626081623137\n",
      "Iter: 34, loss: 0.007030067034065723\n",
      "Iter: 35, loss: 0.007048070430755615\n",
      "Iter: 36, loss: 0.0070705721154809\n",
      "Iter: 37, loss: 0.007080083712935448\n",
      "Iter: 38, loss: 0.007231018040329218\n",
      "Iter: 39, loss: 0.007395229768007994\n",
      "Iter: 40, loss: 0.007412537932395935\n",
      "Iter: 41, loss: 0.006745623890310526\n",
      "Iter: 42, loss: 0.006956878118216991\n",
      "Iter: 43, loss: 0.007272546179592609\n",
      "Iter: 44, loss: 0.006925971247255802\n",
      "Iter: 45, loss: 0.006960156373679638\n",
      "Iter: 46, loss: 0.007079537492245436\n",
      "Iter: 47, loss: 0.006873434875160456\n",
      "Iter: 48, loss: 0.006846209987998009\n",
      "Iter: 49, loss: 0.007071749307215214\n",
      "Iter: 50, loss: 0.006977998651564121\n",
      "Iter: 51, loss: 0.006976080592721701\n",
      "Iter: 52, loss: 0.006684828549623489\n",
      "Iter: 53, loss: 0.006960072088986635\n",
      "Iter: 54, loss: 0.006951329298317432\n",
      "Iter: 55, loss: 0.006721362471580505\n",
      "Iter: 56, loss: 0.007013471331447363\n",
      "Iter: 57, loss: 0.006712885107845068\n",
      "Iter: 58, loss: 0.0067873927764594555\n",
      "Iter: 59, loss: 0.007033827248960733\n",
      "Iter: 60, loss: 0.006874999962747097\n",
      "Iter: 61, loss: 0.007181695196777582\n",
      "Iter: 62, loss: 0.007039396092295647\n",
      "Iter: 63, loss: 0.007164578419178724\n",
      "Iter: 64, loss: 0.006979808676987886\n",
      "Iter: 65, loss: 0.006815957836806774\n",
      "Iter: 66, loss: 0.0069662462919950485\n",
      "Iter: 67, loss: 0.006865784991532564\n",
      "Iter: 68, loss: 0.006693494971841574\n",
      "Iter: 69, loss: 0.0069131809286773205\n",
      "Iter: 70, loss: 0.0067269220016896725\n",
      "Iter: 71, loss: 0.0066919405944645405\n",
      "Iter: 72, loss: 0.007041616830974817\n",
      "Iter: 73, loss: 0.006887971423566341\n",
      "Iter: 74, loss: 0.0068695335648953915\n",
      "Iter: 75, loss: 0.006830703932791948\n",
      "Iter: 76, loss: 0.006894935853779316\n",
      "Iter: 77, loss: 0.0065861111506819725\n",
      "Iter: 78, loss: 0.007245111744850874\n",
      "Iter: 79, loss: 0.006915149744600058\n",
      "Iter: 80, loss: 0.006669932976365089\n",
      "Iter: 81, loss: 0.006899621803313494\n",
      "Iter: 82, loss: 0.006740964017808437\n",
      "Iter: 83, loss: 0.0066369338892400265\n",
      "Iter: 84, loss: 0.006978064309805632\n",
      "Iter: 85, loss: 0.006863586138933897\n",
      "Iter: 86, loss: 0.007283201906830072\n",
      "Iter: 87, loss: 0.006569864694029093\n",
      "Iter: 88, loss: 0.0068029784597456455\n",
      "Iter: 89, loss: 0.007060057483613491\n",
      "Iter: 90, loss: 0.006744980346411467\n",
      "Iter: 91, loss: 0.006754082627594471\n",
      "Iter: 92, loss: 0.006827081087976694\n",
      "Iter: 93, loss: 0.006684495136141777\n",
      "Iter: 94, loss: 0.0068621509708464146\n",
      "Iter: 95, loss: 0.006619374267756939\n",
      "Iter: 96, loss: 0.006728768348693848\n",
      "Iter: 97, loss: 0.006696347147226334\n",
      "Iter: 98, loss: 0.0071915495209395885\n",
      "Iter: 99, loss: 0.006969521287828684\n",
      "Iter: 100, loss: 0.006715096533298492\n",
      "Iter: 101, loss: 0.006908046547323465\n",
      "Iter: 102, loss: 0.006723381578922272\n",
      "Iter: 103, loss: 0.006787415593862534\n",
      "Iter: 104, loss: 0.006901140324771404\n",
      "Iter: 105, loss: 0.006594090722501278\n",
      "Iter: 106, loss: 0.006632352713495493\n",
      "Iter: 107, loss: 0.006363198161125183\n",
      "Iter: 108, loss: 0.006361565552651882\n",
      "Iter: 109, loss: 0.006739174015820026\n",
      "Iter: 110, loss: 0.006739113479852676\n",
      "Iter: 111, loss: 0.0068975770846009254\n",
      "Iter: 112, loss: 0.006681131664663553\n",
      "Iter: 113, loss: 0.006615942809730768\n",
      "Iter: 114, loss: 0.006732241716235876\n",
      "Iter: 115, loss: 0.007052959408611059\n",
      "Iter: 116, loss: 0.0066098119132220745\n",
      "Iter: 117, loss: 0.007112004328519106\n",
      "Iter: 118, loss: 0.006850554142147303\n",
      "Iter: 119, loss: 0.0067845797166228294\n",
      "Iter: 120, loss: 0.006354121491312981\n",
      "Iter: 121, loss: 0.0063782655633986\n",
      "Iter: 122, loss: 0.006973568815737963\n",
      "Iter: 123, loss: 0.006620162166655064\n",
      "Iter: 124, loss: 0.00680500315502286\n",
      "29.40696930885315\n",
      "Iter: 0, loss: 0.007051272317767143\n",
      "Iter: 1, loss: 0.006934717763215303\n",
      "Iter: 2, loss: 0.006500390823930502\n",
      "Iter: 3, loss: 0.006422548554837704\n",
      "Iter: 4, loss: 0.0067466129548847675\n",
      "Iter: 5, loss: 0.006685012485831976\n",
      "Iter: 6, loss: 0.006813008803874254\n",
      "Iter: 7, loss: 0.0068541779182851315\n",
      "Iter: 8, loss: 0.0068831355310976505\n",
      "Iter: 9, loss: 0.006670526694506407\n",
      "Iter: 10, loss: 0.00641764048486948\n",
      "Iter: 11, loss: 0.006697849836200476\n",
      "Iter: 12, loss: 0.006823830772191286\n",
      "Iter: 13, loss: 0.007336599752306938\n",
      "Iter: 14, loss: 0.00689372094348073\n",
      "Iter: 15, loss: 0.0064459326677024364\n",
      "Iter: 16, loss: 0.006671973969787359\n",
      "Iter: 17, loss: 0.006341203581541777\n",
      "Iter: 18, loss: 0.006572366692125797\n",
      "Iter: 19, loss: 0.006144368089735508\n",
      "Iter: 20, loss: 0.006451982073485851\n",
      "Iter: 21, loss: 0.006502846721559763\n",
      "Iter: 22, loss: 0.006317504215985537\n",
      "Iter: 23, loss: 0.006369202397763729\n",
      "Iter: 24, loss: 0.006290020886808634\n",
      "Iter: 25, loss: 0.006764495279639959\n",
      "Iter: 26, loss: 0.00660310871899128\n",
      "Iter: 27, loss: 0.006361955311149359\n",
      "Iter: 28, loss: 0.00681457482278347\n",
      "Iter: 29, loss: 0.006489080376923084\n",
      "Iter: 30, loss: 0.006889862474054098\n",
      "Iter: 31, loss: 0.006387343630194664\n",
      "Iter: 32, loss: 0.0064939772710204124\n",
      "Iter: 33, loss: 0.00686216913163662\n",
      "Iter: 34, loss: 0.006444510538130999\n",
      "Iter: 35, loss: 0.006474385038018227\n",
      "Iter: 36, loss: 0.006488482002168894\n",
      "Iter: 37, loss: 0.006501645781099796\n",
      "Iter: 38, loss: 0.006642138585448265\n",
      "Iter: 39, loss: 0.00680554797872901\n",
      "Iter: 40, loss: 0.0068051451817154884\n",
      "Iter: 41, loss: 0.006180365104228258\n",
      "Iter: 42, loss: 0.006386001594364643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 43, loss: 0.006678000092506409\n",
      "Iter: 44, loss: 0.006363663822412491\n",
      "Iter: 45, loss: 0.006386504042893648\n",
      "Iter: 46, loss: 0.006521744187921286\n",
      "Iter: 47, loss: 0.006312240846455097\n",
      "Iter: 48, loss: 0.006283583119511604\n",
      "Iter: 49, loss: 0.006495081819593906\n",
      "Iter: 50, loss: 0.00639758724719286\n",
      "Iter: 51, loss: 0.0064171007834374905\n",
      "Iter: 52, loss: 0.006138720083981752\n",
      "Iter: 53, loss: 0.006400314625352621\n",
      "Iter: 54, loss: 0.006390412803739309\n",
      "Iter: 55, loss: 0.006179599557071924\n",
      "Iter: 56, loss: 0.006450003478676081\n",
      "Iter: 57, loss: 0.006157366558909416\n",
      "Iter: 58, loss: 0.006233284715563059\n",
      "Iter: 59, loss: 0.0064674667082726955\n",
      "Iter: 60, loss: 0.00632502930238843\n",
      "Iter: 61, loss: 0.00660732202231884\n",
      "Iter: 62, loss: 0.006467925850301981\n",
      "Iter: 63, loss: 0.006596527993679047\n",
      "Iter: 64, loss: 0.0064226905815303326\n",
      "Iter: 65, loss: 0.0062757087871432304\n",
      "Iter: 66, loss: 0.006404914893209934\n",
      "Iter: 67, loss: 0.006311205215752125\n",
      "Iter: 68, loss: 0.006149180233478546\n",
      "Iter: 69, loss: 0.006356267724186182\n",
      "Iter: 70, loss: 0.006181239616125822\n",
      "Iter: 71, loss: 0.006143074948340654\n",
      "Iter: 72, loss: 0.006475519388914108\n",
      "Iter: 73, loss: 0.00632780697196722\n",
      "Iter: 74, loss: 0.0063138664700090885\n",
      "Iter: 75, loss: 0.00627986341714859\n",
      "Iter: 76, loss: 0.006342405453324318\n",
      "Iter: 77, loss: 0.006057613994926214\n",
      "Iter: 78, loss: 0.006673470605164766\n",
      "Iter: 79, loss: 0.006359160412102938\n",
      "Iter: 80, loss: 0.006128700450062752\n",
      "Iter: 81, loss: 0.006354519631713629\n",
      "Iter: 82, loss: 0.0062054977752268314\n",
      "Iter: 83, loss: 0.0060914792120456696\n",
      "Iter: 84, loss: 0.006433733273297548\n",
      "Iter: 85, loss: 0.006309061776846647\n",
      "Iter: 86, loss: 0.006705349311232567\n",
      "Iter: 87, loss: 0.006043260917067528\n",
      "Iter: 88, loss: 0.0062597570940852165\n",
      "Iter: 89, loss: 0.006503962446004152\n",
      "Iter: 90, loss: 0.006215665489435196\n",
      "Iter: 91, loss: 0.006212877109646797\n",
      "Iter: 92, loss: 0.006289838347584009\n",
      "Iter: 93, loss: 0.006154686212539673\n",
      "Iter: 94, loss: 0.006312736310064793\n",
      "Iter: 95, loss: 0.006092939991503954\n",
      "Iter: 96, loss: 0.006181316915899515\n",
      "Iter: 97, loss: 0.00615350529551506\n",
      "Iter: 98, loss: 0.006627783179283142\n",
      "Iter: 99, loss: 0.00643443688750267\n",
      "Iter: 100, loss: 0.006178425159305334\n",
      "Iter: 101, loss: 0.00636454951018095\n",
      "Iter: 102, loss: 0.006187858060002327\n",
      "Iter: 103, loss: 0.00625674519687891\n",
      "Iter: 104, loss: 0.00636714743450284\n",
      "Iter: 105, loss: 0.0060635823756456375\n",
      "Iter: 106, loss: 0.006103195250034332\n",
      "Iter: 107, loss: 0.0058531248942017555\n",
      "Iter: 108, loss: 0.005852121394127607\n",
      "Iter: 109, loss: 0.00620795413851738\n",
      "Iter: 110, loss: 0.006199023686349392\n",
      "Iter: 111, loss: 0.0063548157922923565\n",
      "Iter: 112, loss: 0.006152115296572447\n",
      "Iter: 113, loss: 0.00610195379704237\n",
      "Iter: 114, loss: 0.0061990381218492985\n",
      "Iter: 115, loss: 0.00650860695168376\n",
      "Iter: 116, loss: 0.0060863192193210125\n",
      "Iter: 117, loss: 0.006561729125678539\n",
      "Iter: 118, loss: 0.006314686965197325\n",
      "Iter: 119, loss: 0.0062567731365561485\n",
      "Iter: 120, loss: 0.0058461083099246025\n",
      "Iter: 121, loss: 0.005867845378816128\n",
      "Iter: 122, loss: 0.006449825596064329\n",
      "Iter: 123, loss: 0.006101988255977631\n",
      "Iter: 124, loss: 0.006277063861489296\n",
      "29.009713649749756\n",
      "Iter: 0, loss: 0.006499267183244228\n",
      "Iter: 1, loss: 0.0063937073573470116\n",
      "Iter: 2, loss: 0.005980042740702629\n",
      "Iter: 3, loss: 0.005915358662605286\n",
      "Iter: 4, loss: 0.006219843402504921\n",
      "Iter: 5, loss: 0.006161412689834833\n",
      "Iter: 6, loss: 0.006291058845818043\n",
      "Iter: 7, loss: 0.006334052886813879\n",
      "Iter: 8, loss: 0.00636331457644701\n",
      "Iter: 9, loss: 0.006141887046396732\n",
      "Iter: 10, loss: 0.005912667140364647\n",
      "Iter: 11, loss: 0.006186882499605417\n",
      "Iter: 12, loss: 0.0062977890484035015\n",
      "Iter: 13, loss: 0.006776700261980295\n",
      "Iter: 14, loss: 0.00635484280064702\n",
      "Iter: 15, loss: 0.005937161855399609\n",
      "Iter: 16, loss: 0.006151089910417795\n",
      "Iter: 17, loss: 0.005840770900249481\n",
      "Iter: 18, loss: 0.006049380172044039\n",
      "Iter: 19, loss: 0.005648026242852211\n",
      "Iter: 20, loss: 0.0059438711032271385\n",
      "Iter: 21, loss: 0.006002781447023153\n",
      "Iter: 22, loss: 0.005823799408972263\n",
      "Iter: 23, loss: 0.0058729867450892925\n",
      "Iter: 24, loss: 0.00579956267029047\n",
      "Iter: 25, loss: 0.006245660595595837\n",
      "Iter: 26, loss: 0.006086348090320826\n",
      "Iter: 27, loss: 0.005874252412468195\n",
      "Iter: 28, loss: 0.006296343635767698\n",
      "Iter: 29, loss: 0.005991634912788868\n",
      "Iter: 30, loss: 0.0063533056527376175\n",
      "Iter: 31, loss: 0.005894188303500414\n",
      "Iter: 32, loss: 0.00599200464785099\n",
      "Iter: 33, loss: 0.006342490203678608\n",
      "Iter: 34, loss: 0.005938517861068249\n",
      "Iter: 35, loss: 0.005975349806249142\n",
      "Iter: 36, loss: 0.005986754782497883\n",
      "Iter: 37, loss: 0.006001496687531471\n",
      "Iter: 38, loss: 0.006132508162409067\n",
      "Iter: 39, loss: 0.00629465002566576\n",
      "Iter: 40, loss: 0.006280031055212021\n",
      "Iter: 41, loss: 0.0056919073686003685\n",
      "Iter: 42, loss: 0.005892303306609392\n",
      "Iter: 43, loss: 0.006162640638649464\n",
      "Iter: 44, loss: 0.005876464769244194\n",
      "Iter: 45, loss: 0.005890449974685907\n",
      "Iter: 46, loss: 0.006035393103957176\n",
      "Iter: 47, loss: 0.005826212465763092\n",
      "Iter: 48, loss: 0.00579698896035552\n",
      "Iter: 49, loss: 0.005997378379106522\n",
      "Iter: 50, loss: 0.00589203555136919\n",
      "Iter: 51, loss: 0.00593359163030982\n",
      "Iter: 52, loss: 0.005667148623615503\n",
      "Iter: 53, loss: 0.005913907662034035\n",
      "Iter: 54, loss: 0.005904868710786104\n",
      "Iter: 55, loss: 0.005710079800337553\n",
      "Iter: 56, loss: 0.005961187183856964\n",
      "Iter: 57, loss: 0.005677307024598122\n",
      "Iter: 58, loss: 0.005753585137426853\n",
      "Iter: 59, loss: 0.005974579136818647\n",
      "Iter: 60, loss: 0.005847262218594551\n",
      "Iter: 61, loss: 0.006107916124165058\n",
      "Iter: 62, loss: 0.005971620324999094\n",
      "Iter: 63, loss: 0.006102332845330238\n",
      "Iter: 64, loss: 0.005939398426562548\n",
      "Iter: 65, loss: 0.005806729197502136\n",
      "Iter: 66, loss: 0.005917751230299473\n",
      "Iter: 67, loss: 0.005831719376146793\n",
      "Iter: 68, loss: 0.005676671396940947\n",
      "Iter: 69, loss: 0.0058719078078866005\n",
      "Iter: 70, loss: 0.005708134733140469\n",
      "Iter: 71, loss: 0.0056682247668504715\n",
      "Iter: 72, loss: 0.005982298869639635\n",
      "Iter: 73, loss: 0.005841868929564953\n",
      "Iter: 74, loss: 0.005831912159919739\n",
      "Iter: 75, loss: 0.005804962012916803\n",
      "Iter: 76, loss: 0.005864774342626333\n",
      "Iter: 77, loss: 0.00559783773496747\n",
      "Iter: 78, loss: 0.006176736205816269\n",
      "Iter: 79, loss: 0.005877943709492683\n",
      "Iter: 80, loss: 0.005660699680447578\n",
      "Iter: 81, loss: 0.005881903227418661\n",
      "Iter: 82, loss: 0.005738984793424606\n",
      "Iter: 83, loss: 0.00561881810426712\n",
      "Iter: 84, loss: 0.005961017217487097\n",
      "Iter: 85, loss: 0.0058267866261303425\n",
      "Iter: 86, loss: 0.0062051317654550076\n",
      "Iter: 87, loss: 0.005586104933172464\n",
      "Iter: 88, loss: 0.005788056645542383\n",
      "Iter: 89, loss: 0.006019212771207094\n",
      "Iter: 90, loss: 0.0057548838667571545\n",
      "Iter: 91, loss: 0.005739323329180479\n",
      "Iter: 92, loss: 0.005823980085551739\n",
      "Iter: 93, loss: 0.005695715546607971\n",
      "Iter: 94, loss: 0.005835589487105608\n",
      "Iter: 95, loss: 0.005636370740830898\n",
      "Iter: 96, loss: 0.005707304924726486\n",
      "Iter: 97, loss: 0.00568404421210289\n",
      "Iter: 98, loss: 0.006139267235994339\n",
      "Iter: 99, loss: 0.005970019847154617\n",
      "Iter: 100, loss: 0.005711353849619627\n",
      "Iter: 101, loss: 0.005889452528208494\n",
      "Iter: 102, loss: 0.005722957197576761\n",
      "Iter: 103, loss: 0.005794895347207785\n",
      "Iter: 104, loss: 0.005903509445488453\n",
      "Iter: 105, loss: 0.005603006575256586\n",
      "Iter: 106, loss: 0.005642992909997702\n",
      "Iter: 107, loss: 0.0054105473682284355\n",
      "Iter: 108, loss: 0.005408015567809343\n",
      "Iter: 109, loss: 0.005746112205088139\n",
      "Iter: 110, loss: 0.005731952376663685\n",
      "Iter: 111, loss: 0.005884623154997826\n",
      "Iter: 112, loss: 0.005692607723176479\n",
      "Iter: 113, loss: 0.005655630491673946\n",
      "Iter: 114, loss: 0.005736439023166895\n",
      "Iter: 115, loss: 0.006035706494003534\n",
      "Iter: 116, loss: 0.005630049854516983\n",
      "Iter: 117, loss: 0.006082815583795309\n",
      "Iter: 118, loss: 0.005849451757967472\n",
      "Iter: 119, loss: 0.005798587575554848\n",
      "Iter: 120, loss: 0.005406423471868038\n",
      "Iter: 121, loss: 0.005423628259450197\n",
      "Iter: 122, loss: 0.005993493366986513\n",
      "Iter: 123, loss: 0.005652297288179398\n",
      "Iter: 124, loss: 0.005817847792059183\n",
      "29.460012197494507\n",
      "Iter: 0, loss: 0.006018606945872307\n",
      "Iter: 1, loss: 0.005924977827817202\n",
      "Iter: 2, loss: 0.005527430679649115\n",
      "Iter: 3, loss: 0.005475078709423542\n",
      "Iter: 4, loss: 0.005760825704783201\n",
      "Iter: 5, loss: 0.005707684438675642\n",
      "Iter: 6, loss: 0.005836028140038252\n",
      "Iter: 7, loss: 0.005881187971681356\n",
      "Iter: 8, loss: 0.005909009836614132\n",
      "Iter: 9, loss: 0.005681444890797138\n",
      "Iter: 10, loss: 0.005475127138197422\n",
      "Iter: 11, loss: 0.005742795765399933\n",
      "Iter: 12, loss: 0.0058385953307151794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 13, loss: 0.006290254648774862\n",
      "Iter: 14, loss: 0.005886174738407135\n",
      "Iter: 15, loss: 0.005493386182934046\n",
      "Iter: 16, loss: 0.0056996108032763\n",
      "Iter: 17, loss: 0.005404071882367134\n",
      "Iter: 18, loss: 0.005594437941908836\n",
      "Iter: 19, loss: 0.005218156613409519\n",
      "Iter: 20, loss: 0.005501819308847189\n",
      "Iter: 21, loss: 0.005564944818615913\n",
      "Iter: 22, loss: 0.005394055973738432\n",
      "Iter: 23, loss: 0.005439731292426586\n",
      "Iter: 24, loss: 0.00537204509600997\n",
      "Iter: 25, loss: 0.005793604534119368\n",
      "Iter: 26, loss: 0.00563765550032258\n",
      "Iter: 27, loss: 0.0054502300918102264\n",
      "Iter: 28, loss: 0.005842233542352915\n",
      "Iter: 29, loss: 0.005558673292398453\n",
      "Iter: 30, loss: 0.005884532816708088\n",
      "Iter: 31, loss: 0.005464375484734774\n",
      "Iter: 32, loss: 0.0055547053925693035\n",
      "Iter: 33, loss: 0.00588854169473052\n",
      "Iter: 34, loss: 0.0054968721233308315\n",
      "Iter: 35, loss: 0.005542881321161985\n",
      "Iter: 36, loss: 0.0055511388927698135\n",
      "Iter: 37, loss: 0.005565284751355648\n",
      "Iter: 38, loss: 0.005689182318747044\n",
      "Iter: 39, loss: 0.0058484990149736404\n",
      "Iter: 40, loss: 0.005823461804538965\n",
      "Iter: 41, loss: 0.005268503446131945\n",
      "Iter: 42, loss: 0.00546289561316371\n",
      "Iter: 43, loss: 0.005713258404284716\n",
      "Iter: 44, loss: 0.0054527632892131805\n",
      "Iter: 45, loss: 0.00545890536159277\n",
      "Iter: 46, loss: 0.005611885339021683\n",
      "Iter: 47, loss: 0.005403625313192606\n",
      "Iter: 48, loss: 0.005373061168938875\n",
      "Iter: 49, loss: 0.0055619534105062485\n",
      "Iter: 50, loss: 0.005452718585729599\n",
      "Iter: 51, loss: 0.0055129267275333405\n",
      "Iter: 52, loss: 0.005255925469100475\n",
      "Iter: 53, loss: 0.005490623414516449\n",
      "Iter: 54, loss: 0.005481385160237551\n",
      "Iter: 55, loss: 0.005299525335431099\n",
      "Iter: 56, loss: 0.005533766467124224\n",
      "Iter: 57, loss: 0.005257788579910994\n",
      "Iter: 58, loss: 0.005336526781320572\n",
      "Iter: 59, loss: 0.005545525345951319\n",
      "Iter: 60, loss: 0.005431264638900757\n",
      "Iter: 61, loss: 0.005674462299793959\n",
      "Iter: 62, loss: 0.0055381618440151215\n",
      "Iter: 63, loss: 0.005671593360602856\n",
      "Iter: 64, loss: 0.005517779849469662\n",
      "Iter: 65, loss: 0.0053963493555784225\n",
      "Iter: 66, loss: 0.005493691191077232\n",
      "Iter: 67, loss: 0.005412270314991474\n",
      "Iter: 68, loss: 0.005265443120151758\n",
      "Iter: 69, loss: 0.005449475254863501\n",
      "Iter: 70, loss: 0.005297175608575344\n",
      "Iter: 71, loss: 0.005253824405372143\n",
      "Iter: 72, loss: 0.005553855560719967\n",
      "Iter: 73, loss: 0.005417913664132357\n",
      "Iter: 74, loss: 0.005410084035247564\n",
      "Iter: 75, loss: 0.0053912014700472355\n",
      "Iter: 76, loss: 0.005446462891995907\n",
      "Iter: 77, loss: 0.005199009086936712\n",
      "Iter: 78, loss: 0.005744675174355507\n",
      "Iter: 79, loss: 0.005456574261188507\n",
      "Iter: 80, loss: 0.005251377355307341\n",
      "Iter: 81, loss: 0.005468784831464291\n",
      "Iter: 82, loss: 0.0053339870646595955\n",
      "Iter: 83, loss: 0.005206992384046316\n",
      "Iter: 84, loss: 0.005548766348510981\n",
      "Iter: 85, loss: 0.005407286342233419\n",
      "Iter: 86, loss: 0.005768615752458572\n",
      "Iter: 87, loss: 0.005189691204577684\n",
      "Iter: 88, loss: 0.005375328473746777\n",
      "Iter: 89, loss: 0.005597698036581278\n",
      "Iter: 90, loss: 0.005353420041501522\n",
      "Iter: 91, loss: 0.005327398888766766\n",
      "Iter: 92, loss: 0.005416322033852339\n",
      "Iter: 93, loss: 0.005295770708471537\n",
      "Iter: 94, loss: 0.005418093875050545\n",
      "Iter: 95, loss: 0.0052387909963727\n",
      "Iter: 96, loss: 0.0052935476414859295\n",
      "Iter: 97, loss: 0.005273627582937479\n",
      "Iter: 98, loss: 0.005710655357688665\n",
      "Iter: 99, loss: 0.0055634560994803905\n",
      "Iter: 100, loss: 0.00530390627682209\n",
      "Iter: 101, loss: 0.005475562531501055\n",
      "Iter: 102, loss: 0.005315687507390976\n",
      "Iter: 103, loss: 0.005393311381340027\n",
      "Iter: 104, loss: 0.005498659797012806\n",
      "Iter: 105, loss: 0.005202246829867363\n",
      "Iter: 106, loss: 0.005240566097199917\n",
      "Iter: 107, loss: 0.005024014040827751\n",
      "Iter: 108, loss: 0.0050206296145915985\n",
      "Iter: 109, loss: 0.005343073047697544\n",
      "Iter: 110, loss: 0.005323814693838358\n",
      "Iter: 111, loss: 0.005471399985253811\n",
      "Iter: 112, loss: 0.0052910055965185165\n",
      "Iter: 113, loss: 0.005267793778330088\n",
      "Iter: 114, loss: 0.005329673178493977\n",
      "Iter: 115, loss: 0.005619577132165432\n",
      "Iter: 116, loss: 0.005230452865362167\n",
      "Iter: 117, loss: 0.005662268493324518\n",
      "Iter: 118, loss: 0.00544060580432415\n",
      "Iter: 119, loss: 0.005396762862801552\n",
      "Iter: 120, loss: 0.005022610537707806\n",
      "Iter: 121, loss: 0.005035632289946079\n",
      "Iter: 122, loss: 0.005592850968241692\n",
      "Iter: 123, loss: 0.005257697775959969\n",
      "Iter: 124, loss: 0.005415511783212423\n",
      "29.4804265499115\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.n_epochs):\n",
    "    start = time.time()\n",
    "    for idx, i in enumerate(data_loader):\n",
    "        X = (MolGraph(i[0]))\n",
    "        Y = (MolGraph(i[1]))\n",
    "\n",
    "        # create your optimizer\n",
    "        optimizer = torch.optim.SGD(molopt.parameters(), lr=0.01)\n",
    "\n",
    "        # in your training loop:\n",
    "        optimizer.zero_grad()   # zero the gradient buffers\n",
    "        loss = molopt.forward_train(X, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()    # Does the update\n",
    "\n",
    "        print (\"Iter: {}, loss: {}\".format(idx, loss.item()))\n",
    "    end = time.time()\n",
    "    print(\"Time for epoch {}: {}\", epoch, end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 22),\n",
       " (22, 18),\n",
       " (40, 21),\n",
       " (61, 18),\n",
       " (79, 21),\n",
       " (100, 20),\n",
       " (120, 24),\n",
       " (144, 20),\n",
       " (164, 22),\n",
       " (186, 19),\n",
       " (205, 21),\n",
       " (226, 25),\n",
       " (251, 25),\n",
       " (276, 22),\n",
       " (298, 23),\n",
       " (321, 19),\n",
       " (340, 20),\n",
       " (360, 26),\n",
       " (386, 25),\n",
       " (411, 22),\n",
       " (433, 22),\n",
       " (455, 21),\n",
       " (476, 24),\n",
       " (500, 23),\n",
       " (523, 19),\n",
       " (542, 24),\n",
       " (566, 20),\n",
       " (586, 26),\n",
       " (612, 23),\n",
       " (635, 24),\n",
       " (659, 20),\n",
       " (679, 16),\n",
       " (695, 24),\n",
       " (719, 19),\n",
       " (738, 24),\n",
       " (762, 19),\n",
       " (781, 18),\n",
       " (799, 23),\n",
       " (822, 22),\n",
       " (844, 22),\n",
       " (866, 24),\n",
       " (890, 24),\n",
       " (914, 24),\n",
       " (938, 20),\n",
       " (958, 20),\n",
       " (978, 20),\n",
       " (998, 23),\n",
       " (1021, 20)]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0455,  0.0447,  0.0261,  ..., -0.0160,  0.0526,  0.0204],\n",
       "        [-0.0465,  0.0597,  0.0177,  ..., -0.1785, -0.0618,  0.0922],\n",
       "        [ 0.0161,  0.0673, -0.1091,  ...,  0.0081, -0.0677,  0.1738],\n",
       "        ...,\n",
       "        [ 0.0850, -0.0669, -0.0290,  ..., -0.0392, -0.0300,  0.0311],\n",
       "        [ 0.1073, -0.0826, -0.0462,  ..., -0.0227, -0.0654,  0.0300],\n",
       "        [ 0.0796,  0.0683,  0.0362,  ..., -0.0448, -0.0863,  0.0600]],\n",
       "       device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_embedding.narrow(0, 0, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 50])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_embedding[0:22,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-9.2475e-02,  4.4592e-02,  4.1157e-02, -6.2958e-02, -2.6449e-02,\n",
       "          5.1672e-03, -4.6384e-02, -5.3797e-02, -7.3464e-02, -7.3005e-02,\n",
       "         -1.7704e-02,  8.3490e-02, -1.3573e-01,  8.2092e-03, -5.4852e-02,\n",
       "         -1.0005e-01,  1.0265e-01,  8.8467e-02,  1.8191e-02,  8.6351e-02,\n",
       "          1.7423e-01,  2.4751e-02,  6.3913e-03,  3.5624e-03,  8.4073e-03,\n",
       "         -2.3538e-02,  1.3579e-01,  4.8769e-02,  1.5200e-01, -2.5218e-02,\n",
       "          2.4999e-01, -7.4404e-03,  6.2257e-02, -1.2847e-01, -1.4130e-01,\n",
       "          1.4734e-01, -2.1842e-03,  8.3792e-02, -8.7801e-02,  4.2352e-02,\n",
       "          2.5093e-01, -3.2312e-02,  7.7363e-02,  6.2747e-02,  1.1126e-01,\n",
       "          6.4715e-02, -3.6868e-02,  1.1756e-03,  7.7242e-03,  7.6595e-03],\n",
       "        [ 2.0119e-03, -1.1420e-01, -6.6094e-03, -6.3875e-02,  8.7873e-02,\n",
       "          1.7288e-02,  9.8871e-02, -1.4052e-01, -5.1341e-02, -1.6861e-01,\n",
       "         -7.1023e-02,  3.3994e-02,  2.1864e-02, -9.5770e-03,  4.5321e-02,\n",
       "          1.1468e-01,  1.7144e-01,  1.0275e-01,  6.4650e-02,  6.7548e-02,\n",
       "          1.1096e-01, -1.7862e-01, -1.1572e-01,  3.9683e-03,  2.1316e-02,\n",
       "          5.1599e-02,  1.9089e-02, -9.7457e-02, -5.7705e-02, -3.2013e-02,\n",
       "          1.1510e-01, -1.3428e-01,  7.6023e-02,  1.1371e-01, -1.2995e-01,\n",
       "          1.5539e-01,  6.3551e-02,  6.3039e-02, -3.3870e-02,  2.1248e-01,\n",
       "         -5.7164e-03,  5.5832e-02, -1.1985e-01,  3.5071e-02, -5.7318e-02,\n",
       "         -6.9915e-02, -1.2773e-02,  2.2984e-02, -1.9392e-01,  5.6128e-02],\n",
       "        [-7.4831e-02,  1.9202e-02,  2.6232e-02, -4.9086e-02,  1.6536e-01,\n",
       "          1.1132e-02,  2.0409e-02, -1.3349e-01,  1.0190e-02, -1.5394e-01,\n",
       "         -6.3496e-02, -6.5835e-02,  1.8784e-02,  1.0282e-02,  1.2250e-01,\n",
       "          3.5327e-02,  9.5941e-02,  9.9225e-02,  1.0245e-01,  1.0312e-01,\n",
       "          1.3071e-01, -3.9658e-02, -1.2481e-01,  2.3864e-02,  4.8434e-02,\n",
       "          2.9407e-02, -7.0972e-02, -9.2338e-02, -2.1639e-02,  1.9514e-02,\n",
       "          9.3547e-02, -9.2258e-02,  1.4259e-01,  1.0263e-01, -1.0799e-01,\n",
       "          3.9730e-02, -4.9214e-03,  8.1520e-03,  5.2861e-02,  1.0459e-01,\n",
       "          3.7120e-02,  5.9990e-02, -7.3661e-02,  4.6982e-02,  1.8996e-02,\n",
       "         -1.5189e-02,  5.7292e-02, -1.1482e-01, -1.5137e-01,  4.1276e-02],\n",
       "        [ 1.1107e-01, -1.1086e-01, -6.8608e-02, -7.9348e-02, -9.8589e-03,\n",
       "         -3.3475e-02,  2.5361e-02, -1.0676e-01, -3.0541e-02, -1.3399e-01,\n",
       "          7.1472e-02,  1.2067e-02, -1.1203e-01, -6.5732e-02,  3.8560e-02,\n",
       "         -3.7062e-02,  5.2307e-03, -7.6133e-04,  1.2335e-01,  1.3654e-01,\n",
       "          5.2324e-02,  1.1503e-02, -3.3225e-02,  4.9500e-02, -3.9982e-03,\n",
       "         -1.1418e-01,  9.5772e-02, -2.7959e-02,  5.9731e-02, -7.4483e-02,\n",
       "          1.1130e-01, -5.3144e-02,  4.5784e-02,  7.8103e-02, -9.3262e-02,\n",
       "          1.4370e-01,  3.0391e-02,  1.0941e-02, -6.8041e-02,  1.2573e-01,\n",
       "          8.3403e-02, -1.9750e-02,  4.3076e-02,  5.5770e-02, -1.0587e-04,\n",
       "          1.9467e-02, -4.1530e-02, -5.3311e-02, -6.1343e-02,  5.2183e-02],\n",
       "        [ 1.4965e-01, -4.3767e-02, -1.6036e-02, -4.3057e-02, -1.3387e-02,\n",
       "         -4.2743e-02,  3.6037e-02, -1.3668e-01,  7.7198e-03, -1.7244e-01,\n",
       "          4.1195e-02,  7.0311e-02, -1.4857e-01, -1.4191e-01,  5.1213e-02,\n",
       "         -7.0471e-02,  7.4260e-02,  2.3955e-02,  4.7956e-02,  1.9671e-01,\n",
       "         -2.9506e-02, -7.5589e-02, -4.6484e-02, -9.1569e-02,  1.0470e-01,\n",
       "          6.0248e-03,  4.1826e-02,  5.8838e-02,  6.5742e-03,  8.4379e-02,\n",
       "         -4.3867e-02, -1.1369e-02,  1.2448e-01, -2.8557e-02,  5.5058e-02,\n",
       "          1.8164e-01,  6.6553e-03,  9.0238e-02, -2.0040e-01,  1.6113e-01,\n",
       "          1.0416e-01,  9.9796e-02, -4.6932e-02, -2.1982e-02, -1.3103e-01,\n",
       "          5.8650e-02,  1.5278e-02,  8.2953e-02, -1.0486e-01,  1.0686e-01],\n",
       "        [-1.3811e-01, -1.0962e-02,  1.5299e-01, -1.5090e-01,  4.2050e-02,\n",
       "         -7.9550e-02,  1.4646e-01, -3.8062e-02,  1.0760e-02, -9.9155e-02,\n",
       "         -1.3323e-01,  1.3856e-01, -6.6188e-02, -3.5772e-02,  1.2616e-03,\n",
       "         -5.8006e-02,  5.4338e-02,  3.4116e-02,  3.9887e-02, -2.6394e-02,\n",
       "         -3.1722e-02, -7.0152e-02, -1.0378e-01, -6.5871e-02, -2.9889e-02,\n",
       "         -4.4786e-02, -3.9086e-02,  1.8439e-02,  1.9467e-01,  1.3721e-01,\n",
       "          8.1558e-04, -2.5913e-02,  3.7295e-03, -1.0363e-01, -1.8557e-01,\n",
       "          9.6840e-02, -7.1866e-02,  1.1633e-01,  3.3533e-02, -8.6680e-02,\n",
       "          9.4380e-02,  3.4461e-02, -5.2381e-02,  7.9040e-02,  3.4064e-02,\n",
       "          1.6273e-01,  1.3577e-02, -1.4633e-01, -8.3325e-02,  1.2317e-01],\n",
       "        [ 1.3637e-01, -4.4353e-02, -4.4834e-02, -3.8192e-02, -1.4762e-02,\n",
       "         -6.8501e-02,  5.0284e-02, -1.4969e-01, -3.9461e-03, -1.5554e-01,\n",
       "          2.2196e-02,  9.1206e-02, -2.2257e-01, -9.7455e-02,  3.4719e-02,\n",
       "          5.4771e-03,  6.0597e-02,  4.3464e-02,  8.3396e-02,  1.6212e-01,\n",
       "          5.4218e-02, -9.5843e-02, -8.6093e-02, -6.3744e-02,  9.9415e-02,\n",
       "          2.2308e-02,  1.7616e-02,  2.0128e-02,  3.0082e-02,  8.9772e-02,\n",
       "         -1.0687e-02, -8.5193e-02,  1.4852e-01, -1.3911e-02,  7.9518e-02,\n",
       "          2.1588e-01,  2.2159e-02,  5.5309e-02, -2.2926e-01,  1.1593e-01,\n",
       "          1.5672e-01,  7.2799e-02, -5.9888e-04, -4.4272e-02, -7.3670e-02,\n",
       "          1.2101e-01,  6.7118e-03,  4.4005e-02, -7.9488e-02,  1.4179e-01],\n",
       "        [ 5.0304e-04, -4.7265e-02, -6.8680e-02, -1.8989e-02, -4.3069e-02,\n",
       "          1.2028e-02,  3.0058e-02, -1.1381e-01, -4.9087e-03, -9.1327e-02,\n",
       "          4.2059e-02,  3.9389e-02, -1.3250e-01, -4.4926e-02,  2.9262e-02,\n",
       "          9.3829e-03,  6.7096e-02, -1.2767e-02,  1.2730e-01,  1.1479e-01,\n",
       "         -1.2171e-03, -7.9559e-02,  1.3363e-02, -3.9249e-02,  1.0448e-01,\n",
       "         -4.9952e-02,  5.7367e-02,  7.9590e-03,  7.2742e-02,  1.9224e-02,\n",
       "         -5.6658e-02, -7.9287e-02,  3.9887e-02,  6.7840e-02,  1.2043e-01,\n",
       "          2.2017e-01, -2.5548e-02,  6.9413e-02, -1.2256e-01,  1.0479e-01,\n",
       "          8.6722e-02,  1.2362e-01, -3.2506e-02,  1.9682e-02, -4.7247e-02,\n",
       "          1.0296e-01, -2.9497e-02, -6.7464e-02, -5.0787e-02,  2.1128e-01],\n",
       "        [-8.4101e-02, -1.2952e-01,  4.6561e-03,  2.5336e-02,  8.9385e-02,\n",
       "         -3.3152e-02, -6.2429e-02, -1.1558e-01,  3.9478e-02,  7.0717e-04,\n",
       "          1.9344e-03, -1.9591e-03, -4.0175e-02, -1.8285e-01, -2.4802e-02,\n",
       "         -1.2054e-01,  4.9930e-02,  2.6806e-02,  1.5114e-02,  3.2357e-02,\n",
       "          3.1959e-02,  3.0837e-02, -1.8034e-01, -4.4719e-02, -1.4276e-02,\n",
       "         -4.3431e-03, -1.8570e-02,  1.0516e-01,  1.9579e-02,  8.9768e-02,\n",
       "          2.8135e-02, -3.8529e-02,  3.8870e-02, -6.8335e-02, -1.2181e-01,\n",
       "          2.4701e-01, -5.1530e-02,  1.2441e-01, -5.6590e-02,  1.2005e-01,\n",
       "          1.3201e-01, -2.4858e-02,  8.6666e-02, -4.5921e-02,  9.3157e-02,\n",
       "          6.3326e-02,  3.9900e-02, -1.4168e-02, -2.9439e-02,  8.9053e-02],\n",
       "        [-1.1275e-01,  1.2943e-03, -5.4671e-02, -6.5457e-02, -7.1936e-02,\n",
       "         -1.3738e-02, -3.0450e-02,  1.2147e-01,  9.9804e-02, -1.1622e-01,\n",
       "          1.0904e-01,  9.0601e-02, -8.1999e-02, -3.5589e-02,  2.1940e-01,\n",
       "          1.2496e-02, -5.0089e-02, -8.2836e-02,  3.4453e-02, -6.3917e-03,\n",
       "          9.5113e-02,  4.1443e-02,  8.1753e-02,  3.9420e-02, -5.5499e-02,\n",
       "         -8.2504e-02,  7.4723e-02, -1.4665e-01,  7.0595e-02, -3.6533e-02,\n",
       "          8.3312e-02, -8.9807e-02, -3.5077e-02,  1.9412e-01,  1.8954e-02,\n",
       "          2.2515e-01,  3.8089e-02, -5.1842e-02,  8.1517e-02, -4.1861e-03,\n",
       "          2.3012e-02,  1.4055e-02,  3.7473e-02, -3.2812e-02, -1.2586e-02,\n",
       "         -4.8062e-02, -4.3280e-02, -2.8999e-02, -2.0492e-01,  2.0290e-01],\n",
       "        [-3.6591e-02,  4.6016e-03, -7.6796e-02, -1.8898e-02, -4.2509e-02,\n",
       "          5.2543e-02, -9.9672e-03, -1.1597e-01,  1.5406e-02, -3.6737e-02,\n",
       "          6.9244e-02,  1.5420e-02, -1.1354e-01, -5.1621e-02,  5.0807e-02,\n",
       "          2.1520e-02,  5.6531e-02,  2.4925e-02,  1.6732e-01,  9.6959e-02,\n",
       "          7.0073e-03, -7.5256e-02, -4.8453e-02, -2.2980e-02,  9.7129e-02,\n",
       "          6.9751e-02,  6.9503e-02,  1.0228e-02,  1.1353e-01,  3.1293e-02,\n",
       "         -2.1551e-03, -9.0327e-02,  8.6456e-02,  7.5242e-02,  6.5324e-02,\n",
       "          2.2482e-01, -4.8340e-02,  7.8806e-02, -8.1247e-02,  1.1624e-01,\n",
       "          1.5895e-01,  9.4216e-02, -4.1416e-02,  4.3831e-02, -7.5481e-02,\n",
       "          1.1918e-01, -6.2028e-02, -4.3639e-02, -4.4378e-02,  1.7389e-01],\n",
       "        [-5.7643e-02,  9.0985e-03, -2.9833e-02,  4.5143e-03, -9.8575e-03,\n",
       "         -5.3548e-02,  1.5851e-02, -1.0652e-01,  6.3350e-03, -2.0842e-03,\n",
       "          5.1616e-02,  4.2332e-02,  2.1081e-03, -1.9878e-01,  8.5597e-02,\n",
       "         -1.6425e-01, -5.8760e-03,  3.3283e-02, -1.0720e-01,  1.4510e-01,\n",
       "          2.2784e-02,  4.8351e-02, -1.7794e-01,  1.0362e-02, -6.6807e-02,\n",
       "          2.6328e-02,  4.2177e-02,  4.3030e-02,  4.8680e-02,  1.0780e-01,\n",
       "          1.0842e-01, -6.3398e-02,  1.0684e-02,  3.4621e-02, -3.8505e-02,\n",
       "          9.0589e-02,  3.8153e-02,  1.3906e-01, -1.6519e-02,  4.2114e-02,\n",
       "          1.2325e-01, -7.1997e-02, -3.0775e-02,  6.2293e-02, -3.6598e-02,\n",
       "         -6.2821e-02,  8.2474e-03, -8.8951e-02,  5.3630e-02, -1.4264e-02],\n",
       "        [-7.8254e-02, -8.2792e-02,  1.8675e-02, -3.5932e-02,  5.6793e-02,\n",
       "          5.2942e-02,  7.4303e-02, -3.8158e-02, -8.5450e-02, -1.1032e-01,\n",
       "         -4.6928e-02, -3.1212e-03,  8.4089e-02,  4.6139e-02,  5.1325e-02,\n",
       "          1.7325e-01,  1.2102e-01,  7.5074e-02,  1.6370e-01, -4.7505e-02,\n",
       "          1.2629e-01, -2.2183e-01, -8.4482e-02,  6.4749e-02, -9.5201e-03,\n",
       "         -1.6193e-02,  1.7835e-02, -1.0828e-01,  5.5775e-02, -1.0897e-01,\n",
       "          5.6066e-03, -1.4808e-01, -3.7711e-02,  2.1916e-01, -1.1704e-01,\n",
       "          1.4177e-01,  3.2967e-02,  9.5712e-02, -3.4131e-03,  2.3827e-01,\n",
       "         -1.3770e-02,  3.3080e-02, -1.6552e-01,  9.0251e-02,  1.5492e-02,\n",
       "         -2.8265e-02, -4.8025e-02, -1.1888e-01, -1.7046e-01,  1.3865e-01],\n",
       "        [ 3.4114e-02,  1.0204e-01, -9.0270e-02,  2.8761e-02, -3.7776e-02,\n",
       "         -1.2258e-02, -1.4382e-01,  4.7977e-02,  4.9830e-02,  1.5700e-02,\n",
       "          1.3497e-01,  6.7086e-02, -1.2683e-01, -4.5996e-02,  1.0811e-01,\n",
       "          1.4485e-02,  1.6109e-02,  4.6190e-02,  5.7579e-02,  7.8621e-02,\n",
       "          1.5290e-01, -7.5824e-03,  3.9437e-03, -1.0607e-02,  2.2816e-02,\n",
       "         -8.0109e-02,  7.4569e-02, -3.1358e-02,  9.5366e-02,  3.2356e-03,\n",
       "         -4.7866e-02,  8.0441e-02,  1.0015e-01,  2.1060e-02,  5.2461e-02,\n",
       "          6.3485e-02,  1.0180e-03, -5.4314e-02, -3.3123e-03,  3.4116e-02,\n",
       "          7.1588e-02,  1.3165e-01,  1.0242e-02,  1.3733e-02, -2.7791e-02,\n",
       "         -9.8141e-03, -5.0253e-02,  1.8194e-02, -9.1925e-02,  1.7552e-01],\n",
       "        [ 3.7890e-02,  7.0168e-02, -9.0855e-02,  6.6424e-02, -4.5429e-02,\n",
       "         -5.5615e-02, -1.0286e-01,  9.2074e-03, -1.1636e-02, -2.2935e-02,\n",
       "          8.6468e-02,  2.9843e-02, -1.0275e-01, -3.1200e-02,  5.9485e-02,\n",
       "          8.5958e-03, -1.5521e-02,  1.9868e-02,  8.7264e-02,  8.1197e-02,\n",
       "          1.2720e-01, -2.3281e-02, -1.1109e-02, -2.8612e-05,  8.5473e-03,\n",
       "         -9.9978e-02,  8.3859e-02, -4.0947e-03,  8.3056e-02, -1.4108e-02,\n",
       "         -9.8817e-02,  1.2138e-01,  7.1917e-02, -3.6474e-03,  3.2560e-02,\n",
       "          3.9896e-02,  5.0349e-04, -3.1494e-02, -4.4667e-02,  5.3413e-02,\n",
       "          3.8768e-02,  1.0952e-01,  2.6022e-02, -1.3302e-02, -3.4250e-02,\n",
       "         -1.2320e-02, -2.2463e-02, -1.0446e-03, -5.0203e-02,  2.0068e-01],\n",
       "        [ 3.0044e-03,  9.2889e-02,  4.0899e-02,  6.2183e-02,  8.6703e-02,\n",
       "         -1.2006e-02,  5.7864e-03,  1.6809e-02,  1.0045e-01, -8.5322e-03,\n",
       "          9.9359e-02, -3.2882e-02, -1.6531e-02, -2.0355e-01,  1.4233e-01,\n",
       "         -4.0833e-02, -6.8080e-02,  2.4784e-02,  5.1234e-02,  6.8301e-02,\n",
       "         -6.0353e-02, -5.3563e-02, -7.3836e-02,  4.4007e-02,  6.5564e-02,\n",
       "         -6.5315e-02,  2.8460e-02, -2.5883e-02,  1.4184e-02,  8.0203e-02,\n",
       "         -7.9824e-02, -9.4156e-02,  5.4985e-02,  7.5851e-02, -3.3991e-02,\n",
       "          2.1319e-02, -3.2957e-02,  1.0702e-01, -2.6242e-03,  1.0723e-01,\n",
       "          3.0318e-02,  4.1110e-03, -9.3678e-02, -2.2851e-02, -2.4640e-02,\n",
       "         -7.2871e-02,  7.6433e-02, -1.9022e-01, -3.6434e-02,  1.3717e-01],\n",
       "        [ 3.7890e-02,  7.0168e-02, -9.0855e-02,  6.6424e-02, -4.5429e-02,\n",
       "         -5.5615e-02, -1.0286e-01,  9.2074e-03, -1.1636e-02, -2.2935e-02,\n",
       "          8.6468e-02,  2.9843e-02, -1.0275e-01, -3.1200e-02,  5.9485e-02,\n",
       "          8.5958e-03, -1.5521e-02,  1.9868e-02,  8.7264e-02,  8.1197e-02,\n",
       "          1.2720e-01, -2.3281e-02, -1.1109e-02, -2.8612e-05,  8.5473e-03,\n",
       "         -9.9978e-02,  8.3859e-02, -4.0947e-03,  8.3056e-02, -1.4108e-02,\n",
       "         -9.8817e-02,  1.2138e-01,  7.1917e-02, -3.6474e-03,  3.2560e-02,\n",
       "          3.9896e-02,  5.0349e-04, -3.1494e-02, -4.4667e-02,  5.3413e-02,\n",
       "          3.8768e-02,  1.0952e-01,  2.6022e-02, -1.3302e-02, -3.4250e-02,\n",
       "         -1.2320e-02, -2.2463e-02, -1.0446e-03, -5.0203e-02,  2.0068e-01],\n",
       "        [ 3.4114e-02,  1.0204e-01, -9.0270e-02,  2.8761e-02, -3.7776e-02,\n",
       "         -1.2258e-02, -1.4382e-01,  4.7977e-02,  4.9830e-02,  1.5700e-02,\n",
       "          1.3497e-01,  6.7086e-02, -1.2683e-01, -4.5996e-02,  1.0811e-01,\n",
       "          1.4485e-02,  1.6109e-02,  4.6190e-02,  5.7579e-02,  7.8621e-02,\n",
       "          1.5290e-01, -7.5824e-03,  3.9437e-03, -1.0607e-02,  2.2816e-02,\n",
       "         -8.0109e-02,  7.4569e-02, -3.1358e-02,  9.5366e-02,  3.2356e-03,\n",
       "         -4.7866e-02,  8.0441e-02,  1.0015e-01,  2.1060e-02,  5.2461e-02,\n",
       "          6.3485e-02,  1.0180e-03, -5.4314e-02, -3.3123e-03,  3.4116e-02,\n",
       "          7.1588e-02,  1.3165e-01,  1.0242e-02,  1.3733e-02, -2.7791e-02,\n",
       "         -9.8141e-03, -5.0253e-02,  1.8194e-02, -9.1925e-02,  1.7552e-01]],\n",
       "       device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_embedding.narrow(0, 22, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.0229, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "1 tensor(0.0221, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "2 tensor(0.0220, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "3 tensor(0.0217, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "4 tensor(0.0227, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "5 tensor(0.0220, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "6 tensor(0.0214, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "7 tensor(0.0216, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "8 tensor(0.0213, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "9 tensor(0.0206, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "10 tensor(0.0207, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "11 tensor(0.0208, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "12 tensor(0.0221, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "13 tensor(0.0214, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "14 tensor(0.0210, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "15 tensor(0.0206, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "16 tensor(0.0205, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "17 tensor(0.0206, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "18 tensor(0.0203, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "19 tensor(0.0203, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "20 tensor(0.0204, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "21 tensor(0.0202, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "22 tensor(0.0207, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "23 tensor(0.0198, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "24 tensor(0.0198, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "25 tensor(0.0207, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "26 tensor(0.0202, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "27 tensor(0.0202, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "28 tensor(0.0196, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "29 tensor(0.0196, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "30 tensor(0.0197, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "31 tensor(0.0189, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "32 tensor(0.0191, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "33 tensor(0.0195, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "34 tensor(0.0194, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "35 tensor(0.0191, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "36 tensor(0.0191, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "37 tensor(0.0187, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "38 tensor(0.0192, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "39 tensor(0.0184, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "40 tensor(0.0188, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "41 tensor(0.0193, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "42 tensor(0.0184, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "43 tensor(0.0189, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "44 tensor(0.0183, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "45 tensor(0.0190, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "46 tensor(0.0187, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "47 tensor(0.0187, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "48 tensor(0.0178, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "49 tensor(0.0184, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "50 tensor(0.0188, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "51 tensor(0.0178, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "52 tensor(0.0177, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "53 tensor(0.0184, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "54 tensor(0.0185, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "55 tensor(0.0176, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "56 tensor(0.0178, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "57 tensor(0.0175, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "58 tensor(0.0180, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "59 tensor(0.0176, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "60 tensor(0.0176, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "61 tensor(0.0174, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "62 tensor(0.0174, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "63 tensor(0.0172, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "64 tensor(0.0173, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "65 tensor(0.0171, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "66 tensor(0.0169, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "67 tensor(0.0170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "68 tensor(0.0175, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "69 tensor(0.0170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "70 tensor(0.0171, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "71 tensor(0.0167, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "72 tensor(0.0172, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "73 tensor(0.0172, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "74 tensor(0.0163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "75 tensor(0.0163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "76 tensor(0.0171, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "77 tensor(0.0163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "78 tensor(0.0171, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "79 tensor(0.0161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "80 tensor(0.0157, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "81 tensor(0.0167, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "82 tensor(0.0164, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "83 tensor(0.0168, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "84 tensor(0.0163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "85 tensor(0.0165, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "86 tensor(0.0163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "87 tensor(0.0163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "88 tensor(0.0159, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "89 tensor(0.0163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "90 tensor(0.0161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "91 tensor(0.0159, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "92 tensor(0.0158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "93 tensor(0.0159, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "94 tensor(0.0163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "95 tensor(0.0155, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "96 tensor(0.0153, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "97 tensor(0.0152, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "98 tensor(0.0160, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "99 tensor(0.0150, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "100 tensor(0.0152, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "101 tensor(0.0159, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "102 tensor(0.0154, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "103 tensor(0.0156, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "104 tensor(0.0158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "105 tensor(0.0152, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "106 tensor(0.0163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "107 tensor(0.0152, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "108 tensor(0.0153, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "109 tensor(0.0152, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "110 tensor(0.0152, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "111 tensor(0.0150, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "112 tensor(0.0151, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "113 tensor(0.0150, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "114 tensor(0.0148, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "115 tensor(0.0149, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "116 tensor(0.0148, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "117 tensor(0.0149, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "118 tensor(0.0145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "119 tensor(0.0149, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "120 tensor(0.0148, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "121 tensor(0.0151, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "122 tensor(0.0144, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "123 tensor(0.0141, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "124 tensor(0.0133, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "31.26579213142395\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for idx, i in enumerate(data_loader):\n",
    "    X = (MolGraph(i[0]))\n",
    "    Y = (MolGraph(i[1]))\n",
    "    \n",
    "    # create your optimizer\n",
    "    optimizer = torch.optim.SGD(molopt.parameters(), lr=0.01)\n",
    "\n",
    "    # in your training loop:\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    loss = molopt.forward_train(X, Y)\n",
    "    loss.backward()\n",
    "    optimizer.step()    # Does the update\n",
    "\n",
    "    print (idx, loss)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run the implemented function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(agg_func='sum', batch_norm=False, cuda=True, device='cuda:0', dropout_ffn=0.0, dropout_gcn=0.0, ffn_activation='LeakyReLU', linear_out=False, n_epochs=10, n_ffn_hidden=100, n_hidden=50, n_labels=1, n_layers=5, pc_hidden=50)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = get_args()\n",
    "args.n_epochs = 10\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Iter: 0, loss: 0.018722232431173325\n",
      "Iter: 1, loss: 0.017757058143615723\n",
      "Iter: 2, loss: 0.018040090799331665\n",
      "Iter: 3, loss: 0.01792227476835251\n",
      "Iter: 4, loss: 0.018236974254250526\n",
      "Iter: 5, loss: 0.018043730407953262\n",
      "Iter: 6, loss: 0.018231509253382683\n",
      "Iter: 7, loss: 0.01780024543404579\n",
      "Iter: 8, loss: 0.017768017947673798\n",
      "Iter: 9, loss: 0.016536850482225418\n",
      "Iter: 10, loss: 0.01756877638399601\n",
      "Iter: 11, loss: 0.017272545024752617\n",
      "Iter: 12, loss: 0.018287181854248047\n",
      "Iter: 13, loss: 0.017411379143595695\n",
      "Iter: 14, loss: 0.01773667335510254\n",
      "Iter: 15, loss: 0.01723826862871647\n",
      "Iter: 16, loss: 0.016550524160265923\n",
      "Iter: 17, loss: 0.017243877053260803\n",
      "Iter: 18, loss: 0.016688283532857895\n",
      "Iter: 19, loss: 0.017896555364131927\n",
      "Iter: 20, loss: 0.016977950930595398\n",
      "Iter: 21, loss: 0.016956424340605736\n",
      "Iter: 22, loss: 0.017922870814800262\n",
      "Iter: 23, loss: 0.017049843445420265\n",
      "Iter: 24, loss: 0.017049923539161682\n",
      "Iter: 25, loss: 0.017336567863821983\n",
      "Iter: 26, loss: 0.017349278554320335\n",
      "Iter: 27, loss: 0.017400089651346207\n",
      "Iter: 28, loss: 0.01704513281583786\n",
      "Iter: 29, loss: 0.016948997974395752\n",
      "Iter: 30, loss: 0.01652940921485424\n",
      "Iter: 31, loss: 0.01677509769797325\n",
      "Iter: 32, loss: 0.01651483215391636\n",
      "Iter: 33, loss: 0.01657988131046295\n",
      "Iter: 34, loss: 0.016322506591677666\n",
      "Iter: 35, loss: 0.01648543030023575\n",
      "Iter: 36, loss: 0.0163273848593235\n",
      "Iter: 37, loss: 0.01625530607998371\n",
      "Iter: 38, loss: 0.016727209091186523\n",
      "Iter: 39, loss: 0.01593111827969551\n",
      "Iter: 40, loss: 0.016077572479844093\n",
      "Iter: 41, loss: 0.017142076045274734\n",
      "Iter: 42, loss: 0.015818100422620773\n",
      "Iter: 43, loss: 0.016852444037795067\n",
      "Iter: 44, loss: 0.016551876440644264\n",
      "Iter: 45, loss: 0.015925632789731026\n",
      "Iter: 46, loss: 0.01667685993015766\n",
      "Iter: 47, loss: 0.016677167266607285\n",
      "Iter: 48, loss: 0.01566160097718239\n",
      "Iter: 49, loss: 0.016149863600730896\n",
      "Iter: 50, loss: 0.015990739688277245\n",
      "Iter: 51, loss: 0.0166202113032341\n",
      "Iter: 52, loss: 0.015762552618980408\n",
      "Iter: 53, loss: 0.016316968947649002\n",
      "Iter: 54, loss: 0.01662231795489788\n",
      "Iter: 55, loss: 0.015523619018495083\n",
      "Iter: 56, loss: 0.016051659360527992\n",
      "Iter: 57, loss: 0.015641694888472557\n",
      "Iter: 58, loss: 0.016191391274333\n",
      "Iter: 59, loss: 0.015530397184193134\n",
      "Iter: 60, loss: 0.015624336898326874\n",
      "Iter: 61, loss: 0.015736734494566917\n",
      "Iter: 62, loss: 0.015474141575396061\n",
      "Iter: 63, loss: 0.015347589738667011\n",
      "Iter: 64, loss: 0.015568855218589306\n",
      "Iter: 65, loss: 0.01530027762055397\n",
      "Iter: 66, loss: 0.014843747019767761\n",
      "Iter: 67, loss: 0.015493648126721382\n",
      "Iter: 68, loss: 0.01566789485514164\n",
      "Iter: 69, loss: 0.01554210763424635\n",
      "Iter: 70, loss: 0.01545032113790512\n",
      "Iter: 71, loss: 0.015053319744765759\n",
      "Iter: 72, loss: 0.01569032296538353\n",
      "Iter: 73, loss: 0.01589386537671089\n",
      "Iter: 74, loss: 0.014509586617350578\n",
      "Iter: 75, loss: 0.0151642095297575\n",
      "Iter: 76, loss: 0.015232451260089874\n",
      "Iter: 77, loss: 0.01506027951836586\n",
      "Iter: 78, loss: 0.01528453640639782\n",
      "Iter: 79, loss: 0.014677493833005428\n",
      "Iter: 80, loss: 0.014479007571935654\n",
      "Iter: 81, loss: 0.014986255206167698\n",
      "Iter: 82, loss: 0.01479263510555029\n",
      "Iter: 83, loss: 0.015311399474740028\n",
      "Iter: 84, loss: 0.014977656304836273\n",
      "Iter: 85, loss: 0.014958659186959267\n",
      "Iter: 86, loss: 0.014411842450499535\n",
      "Iter: 87, loss: 0.015533325262367725\n",
      "Iter: 88, loss: 0.015027905814349651\n",
      "Iter: 89, loss: 0.015029525384306908\n",
      "Iter: 90, loss: 0.014991236850619316\n",
      "Iter: 91, loss: 0.015080288052558899\n",
      "Iter: 92, loss: 0.014849782921373844\n",
      "Iter: 93, loss: 0.014772336930036545\n",
      "Iter: 94, loss: 0.01509519387036562\n",
      "Iter: 95, loss: 0.014456598088145256\n",
      "Iter: 96, loss: 0.014175932854413986\n",
      "Iter: 97, loss: 0.014104409143328667\n",
      "Iter: 98, loss: 0.014256770722568035\n",
      "Iter: 99, loss: 0.01441358495503664\n",
      "Iter: 100, loss: 0.014345506206154823\n",
      "Iter: 101, loss: 0.014654207043349743\n",
      "Iter: 102, loss: 0.014474834315478802\n",
      "Iter: 103, loss: 0.014606360346078873\n",
      "Iter: 104, loss: 0.014170381240546703\n",
      "Iter: 105, loss: 0.014529179781675339\n",
      "Iter: 106, loss: 0.014628003351390362\n",
      "Iter: 107, loss: 0.013931334018707275\n",
      "Iter: 108, loss: 0.01437203586101532\n",
      "Iter: 109, loss: 0.014498033560812473\n",
      "Iter: 110, loss: 0.01387301180511713\n",
      "Iter: 111, loss: 0.01383123081177473\n",
      "Iter: 112, loss: 0.014133669435977936\n",
      "Iter: 113, loss: 0.013668118976056576\n",
      "Iter: 114, loss: 0.01399938017129898\n",
      "Iter: 115, loss: 0.013760707341134548\n",
      "Iter: 116, loss: 0.013502645306289196\n",
      "Iter: 117, loss: 0.014021230861544609\n",
      "Iter: 118, loss: 0.014017329551279545\n",
      "Iter: 119, loss: 0.014533710666000843\n",
      "Iter: 120, loss: 0.014426929876208305\n",
      "Iter: 121, loss: 0.01421280950307846\n",
      "Iter: 122, loss: 0.013877885416150093\n",
      "Iter: 123, loss: 0.01327842939645052\n",
      "Iter: 124, loss: 0.012238799594342709\n",
      "Epoch duration: 31.741792678833008\n",
      "Epoch: 1\n",
      "Iter: 0, loss: 0.014247262850403786\n",
      "Iter: 1, loss: 0.01371875312179327\n",
      "Iter: 2, loss: 0.013710943050682545\n",
      "Iter: 3, loss: 0.013450320810079575\n",
      "Iter: 4, loss: 0.013892047107219696\n",
      "Iter: 5, loss: 0.013712471351027489\n",
      "Iter: 6, loss: 0.013838598504662514\n",
      "Iter: 7, loss: 0.013594302348792553\n",
      "Iter: 8, loss: 0.013583357445895672\n",
      "Iter: 9, loss: 0.012681050226092339\n",
      "Iter: 10, loss: 0.013414495624601841\n",
      "Iter: 11, loss: 0.013051750138401985\n",
      "Iter: 12, loss: 0.013984214514493942\n",
      "Iter: 13, loss: 0.01336715742945671\n",
      "Iter: 14, loss: 0.013617978431284428\n",
      "Iter: 15, loss: 0.013097302988171577\n",
      "Iter: 16, loss: 0.01278773695230484\n",
      "Iter: 17, loss: 0.013156063854694366\n",
      "Iter: 18, loss: 0.012959345243871212\n",
      "Iter: 19, loss: 0.013626080006361008\n",
      "Iter: 20, loss: 0.0130124781280756\n",
      "Iter: 21, loss: 0.013066615909337997\n",
      "Iter: 22, loss: 0.013718478381633759\n",
      "Iter: 23, loss: 0.013137048110365868\n",
      "Iter: 24, loss: 0.013035368174314499\n",
      "Iter: 25, loss: 0.013392980210483074\n",
      "Iter: 26, loss: 0.013449626974761486\n",
      "Iter: 27, loss: 0.013391012325882912\n",
      "Iter: 28, loss: 0.013197626918554306\n",
      "Iter: 29, loss: 0.012936670333147049\n",
      "Iter: 30, loss: 0.012872475199401379\n",
      "Iter: 31, loss: 0.012857428751885891\n",
      "Iter: 32, loss: 0.01268718857318163\n",
      "Iter: 33, loss: 0.012759584933519363\n",
      "Iter: 34, loss: 0.012644464150071144\n",
      "Iter: 35, loss: 0.012885240837931633\n",
      "Iter: 36, loss: 0.012529774568974972\n",
      "Iter: 37, loss: 0.012691999785602093\n",
      "Iter: 38, loss: 0.013005993328988552\n",
      "Iter: 39, loss: 0.01240423135459423\n",
      "Iter: 40, loss: 0.01256269309669733\n",
      "Iter: 41, loss: 0.013125699013471603\n",
      "Iter: 42, loss: 0.01250147819519043\n",
      "Iter: 43, loss: 0.013036467134952545\n",
      "Iter: 44, loss: 0.012870331294834614\n",
      "Iter: 45, loss: 0.012559846974909306\n",
      "Iter: 46, loss: 0.012869708240032196\n",
      "Iter: 47, loss: 0.012947293929755688\n",
      "Iter: 48, loss: 0.012331373989582062\n",
      "Iter: 49, loss: 0.012745335698127747\n",
      "Iter: 50, loss: 0.012512844055891037\n",
      "Iter: 51, loss: 0.01301377173513174\n",
      "Iter: 52, loss: 0.01225309818983078\n",
      "Iter: 53, loss: 0.012758452445268631\n",
      "Iter: 54, loss: 0.012970710173249245\n",
      "Iter: 55, loss: 0.01215255819261074\n",
      "Iter: 56, loss: 0.012574191205203533\n",
      "Iter: 57, loss: 0.012268075719475746\n",
      "Iter: 58, loss: 0.012569637969136238\n",
      "Iter: 59, loss: 0.012294732965528965\n",
      "Iter: 60, loss: 0.012116397731006145\n",
      "Iter: 61, loss: 0.012447905726730824\n",
      "Iter: 62, loss: 0.012327229604125023\n",
      "Iter: 63, loss: 0.012256293557584286\n",
      "Iter: 64, loss: 0.012324615381658077\n",
      "Iter: 65, loss: 0.01212217751890421\n",
      "Iter: 66, loss: 0.011802518740296364\n",
      "Iter: 67, loss: 0.01227064710110426\n",
      "Iter: 68, loss: 0.012333483435213566\n",
      "Iter: 69, loss: 0.012204908765852451\n",
      "Iter: 70, loss: 0.012132531963288784\n",
      "Iter: 71, loss: 0.011923125013709068\n",
      "Iter: 72, loss: 0.012296964414417744\n",
      "Iter: 73, loss: 0.012530392967164516\n",
      "Iter: 74, loss: 0.01153546292334795\n",
      "Iter: 75, loss: 0.01193738542497158\n",
      "Iter: 76, loss: 0.011993266642093658\n",
      "Iter: 77, loss: 0.011937621049582958\n",
      "Iter: 78, loss: 0.012151754461228848\n",
      "Iter: 79, loss: 0.011690585874021053\n",
      "Iter: 80, loss: 0.011580150574445724\n",
      "Iter: 81, loss: 0.012006213888525963\n",
      "Iter: 82, loss: 0.011752164922654629\n",
      "Iter: 83, loss: 0.012088272720575333\n",
      "Iter: 84, loss: 0.011910127475857735\n",
      "Iter: 85, loss: 0.011948877014219761\n",
      "Iter: 86, loss: 0.011492053978145123\n",
      "Iter: 87, loss: 0.012396261096000671\n",
      "Iter: 88, loss: 0.012011141516268253\n",
      "Iter: 89, loss: 0.011966905556619167\n",
      "Iter: 90, loss: 0.012010759674012661\n",
      "Iter: 91, loss: 0.011925927363336086\n",
      "Iter: 92, loss: 0.011841057799756527\n",
      "Iter: 93, loss: 0.011703498661518097\n",
      "Iter: 94, loss: 0.012126585468649864\n",
      "Iter: 95, loss: 0.011531194671988487\n",
      "Iter: 96, loss: 0.011456278152763844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 97, loss: 0.011339572258293629\n",
      "Iter: 98, loss: 0.011447019875049591\n",
      "Iter: 99, loss: 0.011545014567673206\n",
      "Iter: 100, loss: 0.011522823013365269\n",
      "Iter: 101, loss: 0.011801736429333687\n",
      "Iter: 102, loss: 0.011706486344337463\n",
      "Iter: 103, loss: 0.011780024506151676\n",
      "Iter: 104, loss: 0.011419154703617096\n",
      "Iter: 105, loss: 0.01167968474328518\n",
      "Iter: 106, loss: 0.011806605383753777\n",
      "Iter: 107, loss: 0.011295836418867111\n",
      "Iter: 108, loss: 0.011602933518588543\n",
      "Iter: 109, loss: 0.011758827604353428\n",
      "Iter: 110, loss: 0.011267127469182014\n",
      "Iter: 111, loss: 0.011211936362087727\n",
      "Iter: 112, loss: 0.011436959728598595\n",
      "Iter: 113, loss: 0.01112006139010191\n",
      "Iter: 114, loss: 0.011402169242501259\n",
      "Iter: 115, loss: 0.011062969453632832\n",
      "Iter: 116, loss: 0.010979493148624897\n",
      "Iter: 117, loss: 0.011338317766785622\n",
      "Iter: 118, loss: 0.011337358504533768\n",
      "Iter: 119, loss: 0.01176669541746378\n",
      "Iter: 120, loss: 0.01149203535169363\n",
      "Iter: 121, loss: 0.011538762599229813\n",
      "Iter: 122, loss: 0.011304404586553574\n",
      "Iter: 123, loss: 0.010815923102200031\n",
      "Iter: 124, loss: 0.01002968568354845\n",
      "Epoch duration: 31.386385917663574\n",
      "Epoch: 2\n",
      "Iter: 0, loss: 0.011580584570765495\n",
      "Iter: 1, loss: 0.011274566873908043\n",
      "Iter: 2, loss: 0.011148524470627308\n",
      "Iter: 3, loss: 0.010959343984723091\n",
      "Iter: 4, loss: 0.01113661378622055\n",
      "Iter: 5, loss: 0.011171914637088776\n",
      "Iter: 6, loss: 0.011251483112573624\n",
      "Iter: 7, loss: 0.011159462854266167\n",
      "Iter: 8, loss: 0.011090399697422981\n",
      "Iter: 9, loss: 0.010352792218327522\n",
      "Iter: 10, loss: 0.010887926444411278\n",
      "Iter: 11, loss: 0.010840796865522861\n",
      "Iter: 12, loss: 0.011357557959854603\n",
      "Iter: 13, loss: 0.010971926152706146\n",
      "Iter: 14, loss: 0.011179198510944843\n",
      "Iter: 15, loss: 0.010723600164055824\n",
      "Iter: 16, loss: 0.01044868677854538\n",
      "Iter: 17, loss: 0.0107438238337636\n",
      "Iter: 18, loss: 0.010735243558883667\n",
      "Iter: 19, loss: 0.011110791005194187\n",
      "Iter: 20, loss: 0.010714068077504635\n",
      "Iter: 21, loss: 0.01073693111538887\n",
      "Iter: 22, loss: 0.01114235632121563\n",
      "Iter: 23, loss: 0.010799098759889603\n",
      "Iter: 24, loss: 0.010712281800806522\n",
      "Iter: 25, loss: 0.011034899391233921\n",
      "Iter: 26, loss: 0.011065718717873096\n",
      "Iter: 27, loss: 0.011047746986150742\n",
      "Iter: 28, loss: 0.01082408893853426\n",
      "Iter: 29, loss: 0.010608837008476257\n",
      "Iter: 30, loss: 0.010752669535577297\n",
      "Iter: 31, loss: 0.010624974966049194\n",
      "Iter: 32, loss: 0.010490375570952892\n",
      "Iter: 33, loss: 0.010546589270234108\n",
      "Iter: 34, loss: 0.010457998141646385\n",
      "Iter: 35, loss: 0.010583128780126572\n",
      "Iter: 36, loss: 0.010357271879911423\n",
      "Iter: 37, loss: 0.010571840219199657\n",
      "Iter: 38, loss: 0.010746232233941555\n",
      "Iter: 39, loss: 0.010268162004649639\n",
      "Iter: 40, loss: 0.01042049191892147\n",
      "Iter: 41, loss: 0.010769893415272236\n",
      "Iter: 42, loss: 0.0102590536698699\n",
      "Iter: 43, loss: 0.010808806866407394\n",
      "Iter: 44, loss: 0.01060498133301735\n",
      "Iter: 45, loss: 0.010481934063136578\n",
      "Iter: 46, loss: 0.010736893862485886\n",
      "Iter: 47, loss: 0.010753841139376163\n",
      "Iter: 48, loss: 0.010244361124932766\n",
      "Iter: 49, loss: 0.0105753093957901\n",
      "Iter: 50, loss: 0.01038611400872469\n",
      "Iter: 51, loss: 0.010792519897222519\n",
      "Iter: 52, loss: 0.010137389414012432\n",
      "Iter: 53, loss: 0.010444997809827328\n",
      "Iter: 54, loss: 0.010688941925764084\n",
      "Iter: 55, loss: 0.010109353810548782\n",
      "Iter: 56, loss: 0.010491719469428062\n",
      "Iter: 57, loss: 0.010290147736668587\n",
      "Iter: 58, loss: 0.010509003885090351\n",
      "Iter: 59, loss: 0.010220815427601337\n",
      "Iter: 60, loss: 0.010092451237142086\n",
      "Iter: 61, loss: 0.010427720844745636\n",
      "Iter: 62, loss: 0.01028142124414444\n",
      "Iter: 63, loss: 0.01025044359266758\n",
      "Iter: 64, loss: 0.010166661813855171\n",
      "Iter: 65, loss: 0.010118778795003891\n",
      "Iter: 66, loss: 0.009960607625544071\n",
      "Iter: 67, loss: 0.010327237658202648\n",
      "Iter: 68, loss: 0.010250280611217022\n",
      "Iter: 69, loss: 0.010244693607091904\n",
      "Iter: 70, loss: 0.010139407590031624\n",
      "Iter: 71, loss: 0.00999193824827671\n",
      "Iter: 72, loss: 0.0103248106315732\n",
      "Iter: 73, loss: 0.010418010875582695\n",
      "Iter: 74, loss: 0.009678315371274948\n",
      "Iter: 75, loss: 0.00998043455183506\n",
      "Iter: 76, loss: 0.010061543434858322\n",
      "Iter: 77, loss: 0.009918591938912868\n",
      "Iter: 78, loss: 0.010107085108757019\n",
      "Iter: 79, loss: 0.009854678064584732\n",
      "Iter: 80, loss: 0.009741116315126419\n",
      "Iter: 81, loss: 0.010044763796031475\n",
      "Iter: 82, loss: 0.009883389808237553\n",
      "Iter: 83, loss: 0.010105593129992485\n",
      "Iter: 84, loss: 0.010047482326626778\n",
      "Iter: 85, loss: 0.010050781071186066\n",
      "Iter: 86, loss: 0.009704720228910446\n",
      "Iter: 87, loss: 0.010367675684392452\n",
      "Iter: 88, loss: 0.010052229277789593\n",
      "Iter: 89, loss: 0.010037730447947979\n",
      "Iter: 90, loss: 0.01002069003880024\n",
      "Iter: 91, loss: 0.010051549412310123\n",
      "Iter: 92, loss: 0.009869134053587914\n",
      "Iter: 93, loss: 0.009946199133992195\n",
      "Iter: 94, loss: 0.01021714136004448\n",
      "Iter: 95, loss: 0.009668270125985146\n",
      "Iter: 96, loss: 0.00972857978194952\n",
      "Iter: 97, loss: 0.009537960402667522\n",
      "Iter: 98, loss: 0.009697929956018925\n",
      "Iter: 99, loss: 0.009711658582091331\n",
      "Iter: 100, loss: 0.009677635505795479\n",
      "Iter: 101, loss: 0.00998481921851635\n",
      "Iter: 102, loss: 0.00986784603446722\n",
      "Iter: 103, loss: 0.009886936284601688\n",
      "Iter: 104, loss: 0.00957697257399559\n",
      "Iter: 105, loss: 0.009804402478039265\n",
      "Iter: 106, loss: 0.00995779037475586\n",
      "Iter: 107, loss: 0.009521102532744408\n",
      "Iter: 108, loss: 0.009877090342342854\n",
      "Iter: 109, loss: 0.009901055134832859\n",
      "Iter: 110, loss: 0.009622425772249699\n",
      "Iter: 111, loss: 0.009539810009300709\n",
      "Iter: 112, loss: 0.009697078727185726\n",
      "Iter: 113, loss: 0.009403632953763008\n",
      "Iter: 114, loss: 0.009673275984823704\n",
      "Iter: 115, loss: 0.009317334741353989\n",
      "Iter: 116, loss: 0.00938574131578207\n",
      "Iter: 117, loss: 0.00952824018895626\n",
      "Iter: 118, loss: 0.009683124721050262\n",
      "Iter: 119, loss: 0.009931204840540886\n",
      "Iter: 120, loss: 0.009737108834087849\n",
      "Iter: 121, loss: 0.009841627441346645\n",
      "Iter: 122, loss: 0.009619299322366714\n",
      "Iter: 123, loss: 0.009159611538052559\n",
      "Iter: 124, loss: 0.008539947681128979\n",
      "Epoch duration: 31.748489379882812\n",
      "Epoch: 3\n",
      "Iter: 0, loss: 0.009840885177254677\n",
      "Iter: 1, loss: 0.009571761824190617\n",
      "Iter: 2, loss: 0.009516574442386627\n",
      "Iter: 3, loss: 0.009301233105361462\n",
      "Iter: 4, loss: 0.009426170028746128\n",
      "Iter: 5, loss: 0.009556939825415611\n",
      "Iter: 6, loss: 0.009557565674185753\n",
      "Iter: 7, loss: 0.009569465182721615\n",
      "Iter: 8, loss: 0.00947793759405613\n",
      "Iter: 9, loss: 0.00885848980396986\n",
      "Iter: 10, loss: 0.00927745457738638\n",
      "Iter: 11, loss: 0.009177122265100479\n",
      "Iter: 12, loss: 0.00967587810009718\n",
      "Iter: 13, loss: 0.009480291977524757\n",
      "Iter: 14, loss: 0.009571006521582603\n",
      "Iter: 15, loss: 0.009178229607641697\n",
      "Iter: 16, loss: 0.009026124142110348\n",
      "Iter: 17, loss: 0.00920481514185667\n",
      "Iter: 18, loss: 0.009160247631371021\n",
      "Iter: 19, loss: 0.009515557438135147\n",
      "Iter: 20, loss: 0.00919022411108017\n",
      "Iter: 21, loss: 0.009208516217768192\n",
      "Iter: 22, loss: 0.009495575912296772\n",
      "Iter: 23, loss: 0.009239273145794868\n",
      "Iter: 24, loss: 0.00912546832114458\n",
      "Iter: 25, loss: 0.009470196440815926\n",
      "Iter: 26, loss: 0.009454119019210339\n",
      "Iter: 27, loss: 0.009392239153385162\n",
      "Iter: 28, loss: 0.009326726198196411\n",
      "Iter: 29, loss: 0.009014776907861233\n",
      "Iter: 30, loss: 0.009301402606070042\n",
      "Iter: 31, loss: 0.009070719592273235\n",
      "Iter: 32, loss: 0.009003297425806522\n",
      "Iter: 33, loss: 0.009050821885466576\n",
      "Iter: 34, loss: 0.009013121016323566\n",
      "Iter: 35, loss: 0.009000074118375778\n",
      "Iter: 36, loss: 0.008839020505547523\n",
      "Iter: 37, loss: 0.009113525971770287\n",
      "Iter: 38, loss: 0.009188130497932434\n",
      "Iter: 39, loss: 0.008811360225081444\n",
      "Iter: 40, loss: 0.008921896107494831\n",
      "Iter: 41, loss: 0.009198902174830437\n",
      "Iter: 42, loss: 0.008792443200945854\n",
      "Iter: 43, loss: 0.00925977062433958\n",
      "Iter: 44, loss: 0.009099971503019333\n",
      "Iter: 45, loss: 0.008991851471364498\n",
      "Iter: 46, loss: 0.009221875108778477\n",
      "Iter: 47, loss: 0.00924333743751049\n",
      "Iter: 48, loss: 0.008775072172284126\n",
      "Iter: 49, loss: 0.009076735004782677\n",
      "Iter: 50, loss: 0.008985876105725765\n",
      "Iter: 51, loss: 0.00931595265865326\n",
      "Iter: 52, loss: 0.0087271174415946\n",
      "Iter: 53, loss: 0.008978735655546188\n",
      "Iter: 54, loss: 0.00915356632322073\n",
      "Iter: 55, loss: 0.008620292879641056\n",
      "Iter: 56, loss: 0.009081000462174416\n",
      "Iter: 57, loss: 0.008809438906610012\n",
      "Iter: 58, loss: 0.009127127006649971\n",
      "Iter: 59, loss: 0.008792281150817871\n",
      "Iter: 60, loss: 0.008662361651659012\n",
      "Iter: 61, loss: 0.009049548767507076\n",
      "Iter: 62, loss: 0.008877730928361416\n",
      "Iter: 63, loss: 0.008895478211343288\n",
      "Iter: 64, loss: 0.008776792325079441\n",
      "Iter: 65, loss: 0.00871204026043415\n",
      "Iter: 66, loss: 0.008606815710663795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 67, loss: 0.00887337327003479\n",
      "Iter: 68, loss: 0.008852193132042885\n",
      "Iter: 69, loss: 0.008837568573653698\n",
      "Iter: 70, loss: 0.008760564960539341\n",
      "Iter: 71, loss: 0.008617549203336239\n",
      "Iter: 72, loss: 0.008941447362303734\n",
      "Iter: 73, loss: 0.009031088091433048\n",
      "Iter: 74, loss: 0.008357178419828415\n",
      "Iter: 75, loss: 0.00862843543291092\n",
      "Iter: 76, loss: 0.00871327891945839\n",
      "Iter: 77, loss: 0.008617350831627846\n",
      "Iter: 78, loss: 0.008771123364567757\n",
      "Iter: 79, loss: 0.008541226387023926\n",
      "Iter: 80, loss: 0.008450784720480442\n",
      "Iter: 81, loss: 0.008678791113197803\n",
      "Iter: 82, loss: 0.008580921217799187\n",
      "Iter: 83, loss: 0.008684307336807251\n",
      "Iter: 84, loss: 0.00868390966206789\n",
      "Iter: 85, loss: 0.008667011745274067\n",
      "Iter: 86, loss: 0.008426375687122345\n",
      "Iter: 87, loss: 0.008933648467063904\n",
      "Iter: 88, loss: 0.008654678240418434\n",
      "Iter: 89, loss: 0.008703500032424927\n",
      "Iter: 90, loss: 0.00869210995733738\n",
      "Iter: 91, loss: 0.008731120266020298\n",
      "Iter: 92, loss: 0.008545096963644028\n",
      "Iter: 93, loss: 0.008616551756858826\n",
      "Iter: 94, loss: 0.00881780032068491\n",
      "Iter: 95, loss: 0.008424021303653717\n",
      "Iter: 96, loss: 0.00845316331833601\n",
      "Iter: 97, loss: 0.008311294950544834\n",
      "Iter: 98, loss: 0.00849028117954731\n",
      "Iter: 99, loss: 0.008420608937740326\n",
      "Iter: 100, loss: 0.008400886319577694\n",
      "Iter: 101, loss: 0.008713055402040482\n",
      "Iter: 102, loss: 0.008582120761275291\n",
      "Iter: 103, loss: 0.00859831366688013\n",
      "Iter: 104, loss: 0.00832020491361618\n",
      "Iter: 105, loss: 0.008523116819560528\n",
      "Iter: 106, loss: 0.008693919517099857\n",
      "Iter: 107, loss: 0.008247067220509052\n",
      "Iter: 108, loss: 0.008630774915218353\n",
      "Iter: 109, loss: 0.008613768965005875\n",
      "Iter: 110, loss: 0.008379091508686543\n",
      "Iter: 111, loss: 0.008331520482897758\n",
      "Iter: 112, loss: 0.008449622429907322\n",
      "Iter: 113, loss: 0.008247199468314648\n",
      "Iter: 114, loss: 0.008467046543955803\n",
      "Iter: 115, loss: 0.008122701197862625\n",
      "Iter: 116, loss: 0.008211261592805386\n",
      "Iter: 117, loss: 0.008316039107739925\n",
      "Iter: 118, loss: 0.0085525531321764\n",
      "Iter: 119, loss: 0.008622660301625729\n",
      "Iter: 120, loss: 0.008420838974416256\n",
      "Iter: 121, loss: 0.008561006747186184\n",
      "Iter: 122, loss: 0.008409053087234497\n",
      "Iter: 123, loss: 0.008002526126801968\n",
      "Iter: 124, loss: 0.00753431860357523\n",
      "Epoch duration: 31.679824352264404\n",
      "Epoch: 4\n",
      "Iter: 0, loss: 0.00860651582479477\n",
      "Iter: 1, loss: 0.008422822691500187\n",
      "Iter: 2, loss: 0.008340973407030106\n",
      "Iter: 3, loss: 0.008155726827681065\n",
      "Iter: 4, loss: 0.00818219780921936\n",
      "Iter: 5, loss: 0.008346407674252987\n",
      "Iter: 6, loss: 0.008395454846322536\n",
      "Iter: 7, loss: 0.008452321402728558\n",
      "Iter: 8, loss: 0.00833161547780037\n",
      "Iter: 9, loss: 0.007802651729434729\n",
      "Iter: 10, loss: 0.00812834594398737\n",
      "Iter: 11, loss: 0.008050949312746525\n",
      "Iter: 12, loss: 0.008496342226862907\n",
      "Iter: 13, loss: 0.008361306972801685\n",
      "Iter: 14, loss: 0.008468417450785637\n",
      "Iter: 15, loss: 0.008020797744393349\n",
      "Iter: 16, loss: 0.007947039790451527\n",
      "Iter: 17, loss: 0.008088840171694756\n",
      "Iter: 18, loss: 0.008064888417720795\n",
      "Iter: 19, loss: 0.008322427049279213\n",
      "Iter: 20, loss: 0.008078775368630886\n",
      "Iter: 21, loss: 0.008019574917852879\n",
      "Iter: 22, loss: 0.008318924345076084\n",
      "Iter: 23, loss: 0.008143259212374687\n",
      "Iter: 24, loss: 0.00805510301142931\n",
      "Iter: 25, loss: 0.008271845988929272\n",
      "Iter: 26, loss: 0.008334251120686531\n",
      "Iter: 27, loss: 0.008191097527742386\n",
      "Iter: 28, loss: 0.00819399580359459\n",
      "Iter: 29, loss: 0.007961694151163101\n",
      "Iter: 30, loss: 0.008212253451347351\n",
      "Iter: 31, loss: 0.007939714938402176\n",
      "Iter: 32, loss: 0.007893596775829792\n",
      "Iter: 33, loss: 0.007936803624033928\n",
      "Iter: 34, loss: 0.00794441532343626\n",
      "Iter: 35, loss: 0.0079204635694623\n",
      "Iter: 36, loss: 0.007828258909285069\n",
      "Iter: 37, loss: 0.008063668385148048\n",
      "Iter: 38, loss: 0.00809455942362547\n",
      "Iter: 39, loss: 0.007744962349534035\n",
      "Iter: 40, loss: 0.007872458547353745\n",
      "Iter: 41, loss: 0.008082093670964241\n",
      "Iter: 42, loss: 0.0077687096782028675\n",
      "Iter: 43, loss: 0.008148906752467155\n",
      "Iter: 44, loss: 0.008030122146010399\n",
      "Iter: 45, loss: 0.007932322099804878\n",
      "Iter: 46, loss: 0.008167845197021961\n",
      "Iter: 47, loss: 0.008168910630047321\n",
      "Iter: 48, loss: 0.007643347606062889\n",
      "Iter: 49, loss: 0.008018803782761097\n",
      "Iter: 50, loss: 0.007930271327495575\n",
      "Iter: 51, loss: 0.00814864132553339\n",
      "Iter: 52, loss: 0.007645586505532265\n",
      "Iter: 53, loss: 0.007892191410064697\n",
      "Iter: 54, loss: 0.008038818836212158\n",
      "Iter: 55, loss: 0.007627070881426334\n",
      "Iter: 56, loss: 0.008013557642698288\n",
      "Iter: 57, loss: 0.007755958940833807\n",
      "Iter: 58, loss: 0.008025728166103363\n",
      "Iter: 59, loss: 0.007785971276462078\n",
      "Iter: 60, loss: 0.007605301216244698\n",
      "Iter: 61, loss: 0.008014168590307236\n",
      "Iter: 62, loss: 0.007834870368242264\n",
      "Iter: 63, loss: 0.007911466993391514\n",
      "Iter: 64, loss: 0.007745188660919666\n",
      "Iter: 65, loss: 0.007694907486438751\n",
      "Iter: 66, loss: 0.007618080358952284\n",
      "Iter: 67, loss: 0.007816732861101627\n",
      "Iter: 68, loss: 0.007826927118003368\n",
      "Iter: 69, loss: 0.00781263504177332\n",
      "Iter: 70, loss: 0.0077280704863369465\n",
      "Iter: 71, loss: 0.007604105398058891\n",
      "Iter: 72, loss: 0.007911305874586105\n",
      "Iter: 73, loss: 0.007988689467310905\n",
      "Iter: 74, loss: 0.007415100932121277\n",
      "Iter: 75, loss: 0.0076601835899055\n",
      "Iter: 76, loss: 0.007693438325077295\n",
      "Iter: 77, loss: 0.007612302433699369\n",
      "Iter: 78, loss: 0.007735169027000666\n",
      "Iter: 79, loss: 0.00756797892972827\n",
      "Iter: 80, loss: 0.007492090109735727\n",
      "Iter: 81, loss: 0.007755903992801905\n",
      "Iter: 82, loss: 0.007618023082613945\n",
      "Iter: 83, loss: 0.007651056628674269\n",
      "Iter: 84, loss: 0.007706662639975548\n",
      "Iter: 85, loss: 0.007719711400568485\n",
      "Iter: 86, loss: 0.007492950651794672\n",
      "Iter: 87, loss: 0.007866892963647842\n",
      "Iter: 88, loss: 0.007657976821064949\n",
      "Iter: 89, loss: 0.007717391941696405\n",
      "Iter: 90, loss: 0.007674111519008875\n",
      "Iter: 91, loss: 0.007687049452215433\n",
      "Iter: 92, loss: 0.007587811443954706\n",
      "Iter: 93, loss: 0.007681851740926504\n",
      "Iter: 94, loss: 0.007785343565046787\n",
      "Iter: 95, loss: 0.007473274599760771\n",
      "Iter: 96, loss: 0.007527464535087347\n",
      "Iter: 97, loss: 0.007383869960904121\n",
      "Iter: 98, loss: 0.007566142361611128\n",
      "Iter: 99, loss: 0.007426538970321417\n",
      "Iter: 100, loss: 0.007445287890732288\n",
      "Iter: 101, loss: 0.007767706178128719\n",
      "Iter: 102, loss: 0.007654258515685797\n",
      "Iter: 103, loss: 0.007605755235999823\n",
      "Iter: 104, loss: 0.007403669413179159\n",
      "Iter: 105, loss: 0.007563995197415352\n",
      "Iter: 106, loss: 0.0076914457604289055\n",
      "Iter: 107, loss: 0.007353796157985926\n",
      "Iter: 108, loss: 0.0076934825628995895\n",
      "Iter: 109, loss: 0.007646576501429081\n",
      "Iter: 110, loss: 0.007513043005019426\n",
      "Iter: 111, loss: 0.007401323411613703\n",
      "Iter: 112, loss: 0.007561727426946163\n",
      "Iter: 113, loss: 0.0073176184669137\n",
      "Iter: 114, loss: 0.007534250617027283\n",
      "Iter: 115, loss: 0.0071868570521473885\n",
      "Iter: 116, loss: 0.0073400563560426235\n",
      "Iter: 117, loss: 0.00739393150433898\n",
      "Iter: 118, loss: 0.007628895342350006\n",
      "Iter: 119, loss: 0.0076398784294724464\n",
      "Iter: 120, loss: 0.007485646288841963\n",
      "Iter: 121, loss: 0.007594900205731392\n",
      "Iter: 122, loss: 0.007510038558393717\n",
      "Iter: 123, loss: 0.007099352777004242\n",
      "Iter: 124, loss: 0.006681034341454506\n",
      "Epoch duration: 31.85764741897583\n",
      "Epoch: 5\n",
      "Iter: 0, loss: 0.007639327552169561\n",
      "Iter: 1, loss: 0.00754497991874814\n",
      "Iter: 2, loss: 0.0074285170994699\n",
      "Iter: 3, loss: 0.007268171291798353\n",
      "Iter: 4, loss: 0.007256846409291029\n",
      "Iter: 5, loss: 0.0074696713127195835\n",
      "Iter: 6, loss: 0.007461897097527981\n",
      "Iter: 7, loss: 0.007577727548778057\n",
      "Iter: 8, loss: 0.007481193169951439\n",
      "Iter: 9, loss: 0.007011239882558584\n",
      "Iter: 10, loss: 0.007217745296657085\n",
      "Iter: 11, loss: 0.007173172198235989\n",
      "Iter: 12, loss: 0.007590724620968103\n",
      "Iter: 13, loss: 0.00749598816037178\n",
      "Iter: 14, loss: 0.007572390604764223\n",
      "Iter: 15, loss: 0.007146501448005438\n",
      "Iter: 16, loss: 0.007093055639415979\n",
      "Iter: 17, loss: 0.007225333712995052\n",
      "Iter: 18, loss: 0.007179719395935535\n",
      "Iter: 19, loss: 0.00738281337544322\n",
      "Iter: 20, loss: 0.007237606216222048\n",
      "Iter: 21, loss: 0.007178083527833223\n",
      "Iter: 22, loss: 0.00741018308326602\n",
      "Iter: 23, loss: 0.007276173681020737\n",
      "Iter: 24, loss: 0.007205392699688673\n",
      "Iter: 25, loss: 0.0073691317811608315\n",
      "Iter: 26, loss: 0.007458751555532217\n",
      "Iter: 27, loss: 0.0073396493680775166\n",
      "Iter: 28, loss: 0.007336028385907412\n",
      "Iter: 29, loss: 0.007091781124472618\n",
      "Iter: 30, loss: 0.007346249185502529\n",
      "Iter: 31, loss: 0.007077366579324007\n",
      "Iter: 32, loss: 0.007026394829154015\n",
      "Iter: 33, loss: 0.007118132896721363\n",
      "Iter: 34, loss: 0.007107732817530632\n",
      "Iter: 35, loss: 0.007090114057064056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 36, loss: 0.006943654268980026\n",
      "Iter: 37, loss: 0.007237760350108147\n",
      "Iter: 38, loss: 0.007261067628860474\n",
      "Iter: 39, loss: 0.006926509086042643\n",
      "Iter: 40, loss: 0.007037026807665825\n",
      "Iter: 41, loss: 0.007225599605590105\n",
      "Iter: 42, loss: 0.006966614630073309\n",
      "Iter: 43, loss: 0.007298057898879051\n",
      "Iter: 44, loss: 0.007183251436799765\n",
      "Iter: 45, loss: 0.007109316997230053\n",
      "Iter: 46, loss: 0.0073044574819505215\n",
      "Iter: 47, loss: 0.007288237102329731\n",
      "Iter: 48, loss: 0.006847213488072157\n",
      "Iter: 49, loss: 0.007197159808129072\n",
      "Iter: 50, loss: 0.00712421303614974\n",
      "Iter: 51, loss: 0.007385751698166132\n",
      "Iter: 52, loss: 0.00685857841745019\n",
      "Iter: 53, loss: 0.0070550041273236275\n",
      "Iter: 54, loss: 0.007219631224870682\n",
      "Iter: 55, loss: 0.00681339344009757\n",
      "Iter: 56, loss: 0.0071955774910748005\n",
      "Iter: 57, loss: 0.006944354623556137\n",
      "Iter: 58, loss: 0.0071913860738277435\n",
      "Iter: 59, loss: 0.007017304189503193\n",
      "Iter: 60, loss: 0.006772950757294893\n",
      "Iter: 61, loss: 0.007211155258119106\n",
      "Iter: 62, loss: 0.007061890326440334\n",
      "Iter: 63, loss: 0.007144815754145384\n",
      "Iter: 64, loss: 0.0069443886168301105\n",
      "Iter: 65, loss: 0.0069085946306586266\n",
      "Iter: 66, loss: 0.0068434011191129684\n",
      "Iter: 67, loss: 0.007026097271591425\n",
      "Iter: 68, loss: 0.007027959916740656\n",
      "Iter: 69, loss: 0.006999838165938854\n",
      "Iter: 70, loss: 0.006919782143086195\n",
      "Iter: 71, loss: 0.006831686478108168\n",
      "Iter: 72, loss: 0.007063650526106358\n",
      "Iter: 73, loss: 0.0071850186213850975\n",
      "Iter: 74, loss: 0.0066863782703876495\n",
      "Iter: 75, loss: 0.006886675488203764\n",
      "Iter: 76, loss: 0.006903586909174919\n",
      "Iter: 77, loss: 0.0068250964395701885\n",
      "Iter: 78, loss: 0.006963648367673159\n",
      "Iter: 79, loss: 0.0067955865524709225\n",
      "Iter: 80, loss: 0.006753591820597649\n",
      "Iter: 81, loss: 0.006949512753635645\n",
      "Iter: 82, loss: 0.00687749870121479\n",
      "Iter: 83, loss: 0.006844003219157457\n",
      "Iter: 84, loss: 0.006958261132240295\n",
      "Iter: 85, loss: 0.006920640356838703\n",
      "Iter: 86, loss: 0.006775819696485996\n",
      "Iter: 87, loss: 0.007097337394952774\n",
      "Iter: 88, loss: 0.00688602589070797\n",
      "Iter: 89, loss: 0.006962509825825691\n",
      "Iter: 90, loss: 0.006896596867591143\n",
      "Iter: 91, loss: 0.006888872478157282\n",
      "Iter: 92, loss: 0.006809687707573175\n",
      "Iter: 93, loss: 0.0069060493260622025\n",
      "Iter: 94, loss: 0.007029564585536718\n",
      "Iter: 95, loss: 0.00671565905213356\n",
      "Iter: 96, loss: 0.006778317037969828\n",
      "Iter: 97, loss: 0.006679859943687916\n",
      "Iter: 98, loss: 0.006820958107709885\n",
      "Iter: 99, loss: 0.006699946243315935\n",
      "Iter: 100, loss: 0.006684637162834406\n",
      "Iter: 101, loss: 0.00699196383357048\n",
      "Iter: 102, loss: 0.006893521640449762\n",
      "Iter: 103, loss: 0.006834570784121752\n",
      "Iter: 104, loss: 0.006641088053584099\n",
      "Iter: 105, loss: 0.006819096393883228\n",
      "Iter: 106, loss: 0.006977471522986889\n",
      "Iter: 107, loss: 0.006637586746364832\n",
      "Iter: 108, loss: 0.006940336432307959\n",
      "Iter: 109, loss: 0.0068862223997712135\n",
      "Iter: 110, loss: 0.006822149734944105\n",
      "Iter: 111, loss: 0.006702200043946505\n",
      "Iter: 112, loss: 0.006811619270592928\n",
      "Iter: 113, loss: 0.006636984646320343\n",
      "Iter: 114, loss: 0.006835206411778927\n",
      "Iter: 115, loss: 0.00650301156565547\n",
      "Iter: 116, loss: 0.006649327930063009\n",
      "Iter: 117, loss: 0.006673922296613455\n",
      "Iter: 118, loss: 0.006930470000952482\n",
      "Iter: 119, loss: 0.006904162000864744\n",
      "Iter: 120, loss: 0.006751784589141607\n",
      "Iter: 121, loss: 0.006856527645140886\n",
      "Iter: 122, loss: 0.006790844723582268\n",
      "Iter: 123, loss: 0.006417118478566408\n",
      "Iter: 124, loss: 0.006100800819694996\n",
      "Epoch duration: 31.878207445144653\n",
      "Epoch: 6\n",
      "Iter: 0, loss: 0.006912209093570709\n",
      "Iter: 1, loss: 0.00686591025441885\n",
      "Iter: 2, loss: 0.006724038626998663\n",
      "Iter: 3, loss: 0.006502231117337942\n",
      "Iter: 4, loss: 0.006539958994835615\n",
      "Iter: 5, loss: 0.006769771222025156\n",
      "Iter: 6, loss: 0.006724145729094744\n",
      "Iter: 7, loss: 0.006865872535854578\n",
      "Iter: 8, loss: 0.006831158418208361\n",
      "Iter: 9, loss: 0.006375137250870466\n",
      "Iter: 10, loss: 0.006529804319143295\n",
      "Iter: 11, loss: 0.006512997206300497\n",
      "Iter: 12, loss: 0.006916365120559931\n",
      "Iter: 13, loss: 0.006838019471615553\n",
      "Iter: 14, loss: 0.006891273427754641\n",
      "Iter: 15, loss: 0.0065031033009290695\n",
      "Iter: 16, loss: 0.006433713249862194\n",
      "Iter: 17, loss: 0.00653945654630661\n",
      "Iter: 18, loss: 0.006561109330505133\n",
      "Iter: 19, loss: 0.0066720834001898766\n",
      "Iter: 20, loss: 0.006565035320818424\n",
      "Iter: 21, loss: 0.0064890398643910885\n",
      "Iter: 22, loss: 0.006698465906083584\n",
      "Iter: 23, loss: 0.006581065710633993\n",
      "Iter: 24, loss: 0.006548003293573856\n",
      "Iter: 25, loss: 0.006681639701128006\n",
      "Iter: 26, loss: 0.006808313075453043\n",
      "Iter: 27, loss: 0.006638062186539173\n",
      "Iter: 28, loss: 0.00665827002376318\n",
      "Iter: 29, loss: 0.006405716761946678\n",
      "Iter: 30, loss: 0.006702308543026447\n",
      "Iter: 31, loss: 0.006412541959434748\n",
      "Iter: 32, loss: 0.006332701072096825\n",
      "Iter: 33, loss: 0.006456737406551838\n",
      "Iter: 34, loss: 0.006458560936152935\n",
      "Iter: 35, loss: 0.006442881189286709\n",
      "Iter: 36, loss: 0.006302829831838608\n",
      "Iter: 37, loss: 0.006593846715986729\n",
      "Iter: 38, loss: 0.006571418140083551\n",
      "Iter: 39, loss: 0.006268039811402559\n",
      "Iter: 40, loss: 0.0064256018958985806\n",
      "Iter: 41, loss: 0.006498055532574654\n",
      "Iter: 42, loss: 0.006317563820630312\n",
      "Iter: 43, loss: 0.00661747669801116\n",
      "Iter: 44, loss: 0.006498745176941156\n",
      "Iter: 45, loss: 0.006484672427177429\n",
      "Iter: 46, loss: 0.006607779301702976\n",
      "Iter: 47, loss: 0.0066163199953734875\n",
      "Iter: 48, loss: 0.006247816141694784\n",
      "Iter: 49, loss: 0.006534760817885399\n",
      "Iter: 50, loss: 0.0064660473726689816\n",
      "Iter: 51, loss: 0.006714972667396069\n",
      "Iter: 52, loss: 0.006210125982761383\n",
      "Iter: 53, loss: 0.006378355901688337\n",
      "Iter: 54, loss: 0.006549118086695671\n",
      "Iter: 55, loss: 0.00618299376219511\n",
      "Iter: 56, loss: 0.0065283216536045074\n",
      "Iter: 57, loss: 0.006282994989305735\n",
      "Iter: 58, loss: 0.006535018794238567\n",
      "Iter: 59, loss: 0.006348032969981432\n",
      "Iter: 60, loss: 0.006130528636276722\n",
      "Iter: 61, loss: 0.0065468731336295605\n",
      "Iter: 62, loss: 0.0064549632370471954\n",
      "Iter: 63, loss: 0.0065003857016563416\n",
      "Iter: 64, loss: 0.006287663709372282\n",
      "Iter: 65, loss: 0.006291107274591923\n",
      "Iter: 66, loss: 0.006221158895641565\n",
      "Iter: 67, loss: 0.0063951751217246056\n",
      "Iter: 68, loss: 0.006383530329912901\n",
      "Iter: 69, loss: 0.006354195065796375\n",
      "Iter: 70, loss: 0.006239293143153191\n",
      "Iter: 71, loss: 0.006198118440806866\n",
      "Iter: 72, loss: 0.006438852287828922\n",
      "Iter: 73, loss: 0.006522627547383308\n",
      "Iter: 74, loss: 0.006103597115725279\n",
      "Iter: 75, loss: 0.006299435626715422\n",
      "Iter: 76, loss: 0.006281489972025156\n",
      "Iter: 77, loss: 0.006197100039571524\n",
      "Iter: 78, loss: 0.006342458073049784\n",
      "Iter: 79, loss: 0.0061791264452040195\n",
      "Iter: 80, loss: 0.00613428233191371\n",
      "Iter: 81, loss: 0.006335291545838118\n",
      "Iter: 82, loss: 0.006267977878451347\n",
      "Iter: 83, loss: 0.00621441937983036\n",
      "Iter: 84, loss: 0.0063354745507240295\n",
      "Iter: 85, loss: 0.006307781673967838\n",
      "Iter: 86, loss: 0.006188962608575821\n",
      "Iter: 87, loss: 0.00644791079685092\n",
      "Iter: 88, loss: 0.00627664802595973\n",
      "Iter: 89, loss: 0.006358194164931774\n",
      "Iter: 90, loss: 0.006228026933968067\n",
      "Iter: 91, loss: 0.006275940220803022\n",
      "Iter: 92, loss: 0.006198907736688852\n",
      "Iter: 93, loss: 0.0063226912170648575\n",
      "Iter: 94, loss: 0.006425145082175732\n",
      "Iter: 95, loss: 0.0061386278830468655\n",
      "Iter: 96, loss: 0.006207467056810856\n",
      "Iter: 97, loss: 0.006101023405790329\n",
      "Iter: 98, loss: 0.006228457670658827\n",
      "Iter: 99, loss: 0.006084631662815809\n",
      "Iter: 100, loss: 0.006073563359677792\n",
      "Iter: 101, loss: 0.006387033965438604\n",
      "Iter: 102, loss: 0.0063279964961111546\n",
      "Iter: 103, loss: 0.006239474285393953\n",
      "Iter: 104, loss: 0.006053526885807514\n",
      "Iter: 105, loss: 0.006215665489435196\n",
      "Iter: 106, loss: 0.006378661375492811\n",
      "Iter: 107, loss: 0.006042502820491791\n",
      "Iter: 108, loss: 0.006326172035187483\n",
      "Iter: 109, loss: 0.006292897276580334\n",
      "Iter: 110, loss: 0.006226817611604929\n",
      "Iter: 111, loss: 0.00614613201469183\n",
      "Iter: 112, loss: 0.006233084015548229\n",
      "Iter: 113, loss: 0.006072251126170158\n",
      "Iter: 114, loss: 0.006264055613428354\n",
      "Iter: 115, loss: 0.005902773700654507\n",
      "Iter: 116, loss: 0.006060926243662834\n",
      "Iter: 117, loss: 0.0060926442965865135\n",
      "Iter: 118, loss: 0.006320350803434849\n",
      "Iter: 119, loss: 0.006295928731560707\n",
      "Iter: 120, loss: 0.006152155809104443\n",
      "Iter: 121, loss: 0.006282119080424309\n",
      "Iter: 122, loss: 0.006224303040653467\n",
      "Iter: 123, loss: 0.005856509320437908\n",
      "Iter: 124, loss: 0.005605214741080999\n",
      "Epoch duration: 31.71293044090271\n",
      "Epoch: 7\n",
      "Iter: 0, loss: 0.0063340868800878525\n",
      "Iter: 1, loss: 0.006297172978520393\n",
      "Iter: 2, loss: 0.006164714228361845\n",
      "Iter: 3, loss: 0.005909183528274298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 4, loss: 0.005956504959613085\n",
      "Iter: 5, loss: 0.006206505466252565\n",
      "Iter: 6, loss: 0.006158994510769844\n",
      "Iter: 7, loss: 0.006321229971945286\n",
      "Iter: 8, loss: 0.006258407142013311\n",
      "Iter: 9, loss: 0.005867192056030035\n",
      "Iter: 10, loss: 0.005944552831351757\n",
      "Iter: 11, loss: 0.005935914348810911\n",
      "Iter: 12, loss: 0.006349125411361456\n",
      "Iter: 13, loss: 0.006305607967078686\n",
      "Iter: 14, loss: 0.006304513197392225\n",
      "Iter: 15, loss: 0.005967371165752411\n",
      "Iter: 16, loss: 0.005892177578061819\n",
      "Iter: 17, loss: 0.005989509169012308\n",
      "Iter: 18, loss: 0.006001384928822517\n",
      "Iter: 19, loss: 0.006092861294746399\n",
      "Iter: 20, loss: 0.00605185329914093\n",
      "Iter: 21, loss: 0.005936257541179657\n",
      "Iter: 22, loss: 0.006147680804133415\n",
      "Iter: 23, loss: 0.0060217492282390594\n",
      "Iter: 24, loss: 0.006027007941156626\n",
      "Iter: 25, loss: 0.006092998664826155\n",
      "Iter: 26, loss: 0.0062633659690618515\n",
      "Iter: 27, loss: 0.00607368303462863\n",
      "Iter: 28, loss: 0.006102191749960184\n",
      "Iter: 29, loss: 0.005827110260725021\n",
      "Iter: 30, loss: 0.006162941921502352\n",
      "Iter: 31, loss: 0.005877353250980377\n",
      "Iter: 32, loss: 0.0057967533357441425\n",
      "Iter: 33, loss: 0.005920764524489641\n",
      "Iter: 34, loss: 0.005931442137807608\n",
      "Iter: 35, loss: 0.00590196717530489\n",
      "Iter: 36, loss: 0.005743312183767557\n",
      "Iter: 37, loss: 0.00606483593583107\n",
      "Iter: 38, loss: 0.006036106031388044\n",
      "Iter: 39, loss: 0.005741443019360304\n",
      "Iter: 40, loss: 0.005905759520828724\n",
      "Iter: 41, loss: 0.005962524097412825\n",
      "Iter: 42, loss: 0.005807976704090834\n",
      "Iter: 43, loss: 0.006065419875085354\n",
      "Iter: 44, loss: 0.005922948941588402\n",
      "Iter: 45, loss: 0.005930616054683924\n",
      "Iter: 46, loss: 0.006071713287383318\n",
      "Iter: 47, loss: 0.006059745326638222\n",
      "Iter: 48, loss: 0.005720491986721754\n",
      "Iter: 49, loss: 0.005999068263918161\n",
      "Iter: 50, loss: 0.0059424820356070995\n",
      "Iter: 51, loss: 0.006164996884763241\n",
      "Iter: 52, loss: 0.005690357182174921\n",
      "Iter: 53, loss: 0.005904859397560358\n",
      "Iter: 54, loss: 0.006008984986692667\n",
      "Iter: 55, loss: 0.005650380626320839\n",
      "Iter: 56, loss: 0.006029075011610985\n",
      "Iter: 57, loss: 0.005756278056651354\n",
      "Iter: 58, loss: 0.006001491565257311\n",
      "Iter: 59, loss: 0.005818895995616913\n",
      "Iter: 60, loss: 0.005598660558462143\n",
      "Iter: 61, loss: 0.0060104550793766975\n",
      "Iter: 62, loss: 0.005931847263127565\n",
      "Iter: 63, loss: 0.005974627565592527\n",
      "Iter: 64, loss: 0.005776469595730305\n",
      "Iter: 65, loss: 0.005768541246652603\n",
      "Iter: 66, loss: 0.005730125587433577\n",
      "Iter: 67, loss: 0.005898797418922186\n",
      "Iter: 68, loss: 0.0058692521415650845\n",
      "Iter: 69, loss: 0.005826757289469242\n",
      "Iter: 70, loss: 0.0057227108627557755\n",
      "Iter: 71, loss: 0.0057017384096980095\n",
      "Iter: 72, loss: 0.0059087928384542465\n",
      "Iter: 73, loss: 0.0059975250624120235\n",
      "Iter: 74, loss: 0.005596655420958996\n",
      "Iter: 75, loss: 0.005792647134512663\n",
      "Iter: 76, loss: 0.005786736961454153\n",
      "Iter: 77, loss: 0.005699956323951483\n",
      "Iter: 78, loss: 0.005841752048581839\n",
      "Iter: 79, loss: 0.005677208304405212\n",
      "Iter: 80, loss: 0.005638068541884422\n",
      "Iter: 81, loss: 0.005813256371766329\n",
      "Iter: 82, loss: 0.005755784455686808\n",
      "Iter: 83, loss: 0.005694964434951544\n",
      "Iter: 84, loss: 0.0058204662054777145\n",
      "Iter: 85, loss: 0.005795329809188843\n",
      "Iter: 86, loss: 0.005714115686714649\n",
      "Iter: 87, loss: 0.005930111277848482\n",
      "Iter: 88, loss: 0.005766297224909067\n",
      "Iter: 89, loss: 0.0058609130792319775\n",
      "Iter: 90, loss: 0.005717576015740633\n",
      "Iter: 91, loss: 0.005779315251857042\n",
      "Iter: 92, loss: 0.00568237854167819\n",
      "Iter: 93, loss: 0.005825352389365435\n",
      "Iter: 94, loss: 0.005930533166974783\n",
      "Iter: 95, loss: 0.005645389668643475\n",
      "Iter: 96, loss: 0.005692833103239536\n",
      "Iter: 97, loss: 0.005613885819911957\n",
      "Iter: 98, loss: 0.0057527790777385235\n",
      "Iter: 99, loss: 0.005598505027592182\n",
      "Iter: 100, loss: 0.0055952430702745914\n",
      "Iter: 101, loss: 0.005882665980607271\n",
      "Iter: 102, loss: 0.0058771539479494095\n",
      "Iter: 103, loss: 0.005749286152422428\n",
      "Iter: 104, loss: 0.005577773787081242\n",
      "Iter: 105, loss: 0.005708424840122461\n",
      "Iter: 106, loss: 0.0058815800584852695\n",
      "Iter: 107, loss: 0.005565926898270845\n",
      "Iter: 108, loss: 0.005831937771290541\n",
      "Iter: 109, loss: 0.005801109597086906\n",
      "Iter: 110, loss: 0.005768920294940472\n",
      "Iter: 111, loss: 0.0056610023602843285\n",
      "Iter: 112, loss: 0.005746797658503056\n",
      "Iter: 113, loss: 0.005607254337519407\n",
      "Iter: 114, loss: 0.005782313644886017\n",
      "Iter: 115, loss: 0.005424569360911846\n",
      "Iter: 116, loss: 0.005598653107881546\n",
      "Iter: 117, loss: 0.00561353238299489\n",
      "Iter: 118, loss: 0.005840091034770012\n",
      "Iter: 119, loss: 0.005803863052278757\n",
      "Iter: 120, loss: 0.00565779022872448\n",
      "Iter: 121, loss: 0.005822952836751938\n",
      "Iter: 122, loss: 0.005755724385380745\n",
      "Iter: 123, loss: 0.0054221679456532\n",
      "Iter: 124, loss: 0.005178856663405895\n",
      "Epoch duration: 31.69497513771057\n",
      "Epoch: 8\n",
      "Iter: 0, loss: 0.005840673577040434\n",
      "Iter: 1, loss: 0.005837361793965101\n",
      "Iter: 2, loss: 0.005689401179552078\n",
      "Iter: 3, loss: 0.005448386073112488\n",
      "Iter: 4, loss: 0.005482236854732037\n",
      "Iter: 5, loss: 0.005721728317439556\n",
      "Iter: 6, loss: 0.005692991428077221\n",
      "Iter: 7, loss: 0.005825267639011145\n",
      "Iter: 8, loss: 0.005800914950668812\n",
      "Iter: 9, loss: 0.005418306216597557\n",
      "Iter: 10, loss: 0.005471087526530027\n",
      "Iter: 11, loss: 0.0054632616229355335\n",
      "Iter: 12, loss: 0.005872855894267559\n",
      "Iter: 13, loss: 0.005823501851409674\n",
      "Iter: 14, loss: 0.005849415902048349\n",
      "Iter: 15, loss: 0.005511279217898846\n",
      "Iter: 16, loss: 0.005406106356531382\n",
      "Iter: 17, loss: 0.0055168140679597855\n",
      "Iter: 18, loss: 0.005537422839552164\n",
      "Iter: 19, loss: 0.0056333537213504314\n",
      "Iter: 20, loss: 0.005602694116532803\n",
      "Iter: 21, loss: 0.005469085182994604\n",
      "Iter: 22, loss: 0.005673635751008987\n",
      "Iter: 23, loss: 0.005571448709815741\n",
      "Iter: 24, loss: 0.005550788715481758\n",
      "Iter: 25, loss: 0.005666691344231367\n",
      "Iter: 26, loss: 0.005792493466287851\n",
      "Iter: 27, loss: 0.005593178328126669\n",
      "Iter: 28, loss: 0.005658838432282209\n",
      "Iter: 29, loss: 0.005405453033745289\n",
      "Iter: 30, loss: 0.005737321451306343\n",
      "Iter: 31, loss: 0.0054552857764065266\n",
      "Iter: 32, loss: 0.0053489357233047485\n",
      "Iter: 33, loss: 0.005482393782585859\n",
      "Iter: 34, loss: 0.005485266912728548\n",
      "Iter: 35, loss: 0.0054625398479402065\n",
      "Iter: 36, loss: 0.0052818614058196545\n",
      "Iter: 37, loss: 0.005636189132928848\n",
      "Iter: 38, loss: 0.005575343035161495\n",
      "Iter: 39, loss: 0.0053415121510624886\n",
      "Iter: 40, loss: 0.005461696069687605\n",
      "Iter: 41, loss: 0.005506600718945265\n",
      "Iter: 42, loss: 0.005362831521779299\n",
      "Iter: 43, loss: 0.005620049778372049\n",
      "Iter: 44, loss: 0.0054451702162623405\n",
      "Iter: 45, loss: 0.005500539671629667\n",
      "Iter: 46, loss: 0.005613341461867094\n",
      "Iter: 47, loss: 0.005590036977082491\n",
      "Iter: 48, loss: 0.005292024929076433\n",
      "Iter: 49, loss: 0.005557065363973379\n",
      "Iter: 50, loss: 0.005502344574779272\n",
      "Iter: 51, loss: 0.005720397457480431\n",
      "Iter: 52, loss: 0.005269474349915981\n",
      "Iter: 53, loss: 0.0054525467567145824\n",
      "Iter: 54, loss: 0.005561494268476963\n",
      "Iter: 55, loss: 0.005221706815063953\n",
      "Iter: 56, loss: 0.0055841184221208096\n",
      "Iter: 57, loss: 0.005321362521499395\n",
      "Iter: 58, loss: 0.005563446786254644\n",
      "Iter: 59, loss: 0.0053775799460709095\n",
      "Iter: 60, loss: 0.0051544732414186\n",
      "Iter: 61, loss: 0.00559513783082366\n",
      "Iter: 62, loss: 0.005504111759364605\n",
      "Iter: 63, loss: 0.005561198573559523\n",
      "Iter: 64, loss: 0.005384356714785099\n",
      "Iter: 65, loss: 0.005339271854609251\n",
      "Iter: 66, loss: 0.005318860057741404\n",
      "Iter: 67, loss: 0.0054621584713459015\n",
      "Iter: 68, loss: 0.005439847242087126\n",
      "Iter: 69, loss: 0.0053919400088489056\n",
      "Iter: 70, loss: 0.005315027665346861\n",
      "Iter: 71, loss: 0.005286267958581448\n",
      "Iter: 72, loss: 0.005495044402778149\n",
      "Iter: 73, loss: 0.00556146539747715\n",
      "Iter: 74, loss: 0.005184408277273178\n",
      "Iter: 75, loss: 0.0053744870238006115\n",
      "Iter: 76, loss: 0.005371695384383202\n",
      "Iter: 77, loss: 0.005280665587633848\n",
      "Iter: 78, loss: 0.005427743308246136\n",
      "Iter: 79, loss: 0.0052656251937150955\n",
      "Iter: 80, loss: 0.005244009662419558\n",
      "Iter: 81, loss: 0.00539974682033062\n",
      "Iter: 82, loss: 0.005332001950591803\n",
      "Iter: 83, loss: 0.005261838901787996\n",
      "Iter: 84, loss: 0.00542583828791976\n",
      "Iter: 85, loss: 0.0053841425105929375\n",
      "Iter: 86, loss: 0.00530817499384284\n",
      "Iter: 87, loss: 0.0054971445351839066\n",
      "Iter: 88, loss: 0.005342818330973387\n",
      "Iter: 89, loss: 0.005445214454084635\n",
      "Iter: 90, loss: 0.005305228289216757\n",
      "Iter: 91, loss: 0.005351165309548378\n",
      "Iter: 92, loss: 0.005260691046714783\n",
      "Iter: 93, loss: 0.005404898431152105\n",
      "Iter: 94, loss: 0.005505261942744255\n",
      "Iter: 95, loss: 0.005257639568299055\n",
      "Iter: 96, loss: 0.005267275031656027\n",
      "Iter: 97, loss: 0.005225545261055231\n",
      "Iter: 98, loss: 0.0053579905070364475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 99, loss: 0.005205238237977028\n",
      "Iter: 100, loss: 0.005162783898413181\n",
      "Iter: 101, loss: 0.005496329162269831\n",
      "Iter: 102, loss: 0.005427616182714701\n",
      "Iter: 103, loss: 0.0053450074046850204\n",
      "Iter: 104, loss: 0.005186091177165508\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-ff87ae95d539>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmolopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmolopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/gitrepos/tum-thesis/mol_opt/train_mol_opt.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args, molopt, data_loader)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# in your training loop:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# zero the gradient buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmolopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# Does the update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gitrepos/tum-thesis/mol_opt/mol_opt.py\u001b[0m in \u001b[0;36mforward_train\u001b[0;34m(self, x_batch, y_batch)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# using the OT permutation matrix between the two embeddings,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;31m# align them so that we can determine the delta vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;31m# keep the original alignment of the x vector, of course\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mlenx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gitrepos/tum-thesis/mol_opt/mol_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_batch)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mx_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mx_delta_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_delta_hat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gitrepos/tum-thesis/mol_opt/mol_opt.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGCN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mol_ot/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gitrepos/tum-thesis/otgnn/models/gcn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, mol_graph)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmol_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         graph_inputs, scope = mol_graph.get_graph_inputs(\n\u001b[0;32m--> 121\u001b[0;31m             device=self.args.device)\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mfatoms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfbonds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gitrepos/tum-thesis/otgnn/graph/mol_graph.py\u001b[0m in \u001b[0;36mget_graph_inputs\u001b[0;34m(self, device, output_tensors)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0matom_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matom\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matoms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m                 \u001b[0matom_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_atom_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m                 \u001b[0mfatoms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matom_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mnei_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbond\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbonds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gitrepos/tum-thesis/otgnn/graph/mol_features.py\u001b[0m in \u001b[0;36mget_atom_features\u001b[0;34m(atom)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mdegree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monek_unk_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEGREES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mexp_valence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monek_unk_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_valence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEXPLICIT_VALENCES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mimp_valence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monek_unk_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimp_valence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMPLICIT_VALENCES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0maro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0matom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maro\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gitrepos/tum-thesis/otgnn/graph/mol_features.py\u001b[0m in \u001b[0;36monek_unk_encoding\u001b[0;34m(x, set)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'UNK'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gitrepos/tum-thesis/otgnn/graph/mol_features.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'UNK'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "molopt = main(args, molopt, data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try out some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data_loader:\n",
    "    X = (MolGraph(i[0]))\n",
    "    Y = (MolGraph(i[1]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_embedding, x_delta = molopt.forward(X)\n",
    "y_embedding = molopt.encode(Y)\n",
    "y_aligned = molopt.align(x_embedding, X, y_embedding, Y)\n",
    "xhat_delta = molopt.delta(x_embedding, y_aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1041, 50])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0329,  0.0609, -0.0998,  ...,  0.0075,  0.0955,  0.0417],\n",
       "        [-0.0989,  0.0230, -0.0382,  ...,  0.0112,  0.0739,  0.0399],\n",
       "        [-0.0075,  0.0569, -0.0903,  ..., -0.0105,  0.1205,  0.0362],\n",
       "        ...,\n",
       "        [-0.0320,  0.0622, -0.1089,  ...,  0.0226,  0.0922,  0.0389],\n",
       "        [-0.0710,  0.0140, -0.0761,  ..., -0.0365,  0.0978,  0.0615],\n",
       "        [-0.0357,  0.0151, -0.0686,  ..., -0.0102,  0.0744,  0.0069]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.3743e-02,  4.4010e-02, -3.0419e-02,  ..., -3.1573e-02,\n",
       "          3.3295e-02,  5.1158e-02],\n",
       "        [-1.7625e-01,  1.2730e-01, -6.4805e-02,  ...,  5.5192e-02,\n",
       "          1.4270e-01, -6.1634e-03],\n",
       "        [-7.5377e-02, -1.6244e-02,  5.7163e-02,  ...,  5.2657e-02,\n",
       "          6.2681e-02,  2.1037e-02],\n",
       "        ...,\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [-1.9689e-01, -5.8483e-02,  9.4051e-03,  ...,  1.4330e-01,\n",
       "          3.4291e-01,  1.2687e-01],\n",
       "        [-8.1267e-03,  1.4335e-04, -5.4330e-04,  ..., -4.7268e-03,\n",
       "          1.3943e-02, -8.6641e-03]], device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xhat_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0209,  0.0169, -0.0694,  ...,  0.0390,  0.0622, -0.0094],\n",
       "        [ 0.0773, -0.1043,  0.0266,  ..., -0.0440, -0.0688,  0.0460],\n",
       "        [ 0.0679,  0.0732, -0.1474,  ..., -0.0631,  0.0578,  0.0152],\n",
       "        ...,\n",
       "        [-0.0320,  0.0622, -0.1089,  ...,  0.0226,  0.0922,  0.0389],\n",
       "        [ 0.1259,  0.0725, -0.0856,  ..., -0.1798, -0.2451, -0.0654],\n",
       "        [-0.0276,  0.0149, -0.0681,  ..., -0.0054,  0.0605,  0.0156]],\n",
       "       device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_delta - xhat_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0016, -0.0741, -0.0745,  ...,  0.0228, -0.0114, -0.0270],\n",
       "        [ 0.0706, -0.2145, -0.2110,  ...,  0.3129,  0.1174, -0.0711],\n",
       "        [-0.1300, -0.2984,  0.1859,  ..., -0.0819,  0.1519, -0.2849],\n",
       "        ...,\n",
       "        [ 0.2385, -0.1195, -0.1575,  ...,  0.0702, -0.1347,  0.0785],\n",
       "        [ 0.0352,  0.1964, -0.0075,  ..., -0.0421,  0.1371, -0.1475],\n",
       "        [-0.0727,  0.1660,  0.0481,  ...,  0.0867,  0.1587, -0.0863]],\n",
       "       device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "molopt.delta(x_embedding, y) - xhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<otgnn.graph.mol_graph.Molecule at 0x7fec37dd4f60>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.mols[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
