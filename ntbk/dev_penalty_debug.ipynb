{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/octav/gitrepos/tum-thesis\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if \"ntbk\" in os.getcwd():\n",
    "    os.chdir(\"..\")\n",
    "print(os.getcwd())\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), \"otgnn\"))\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from otgnn.models import GCN\n",
    "from otgnn.graph import MolGraph\n",
    "from otgnn.utils import save_model, load_model, StatsTracker\n",
    "from otgnn.graph import SYMBOLS, FORMAL_CHARGES, BOND_TYPES, get_bt_index\n",
    "\n",
    "from mol_opt.mol_opt import MolOpt\n",
    "from mol_opt.data_mol_opt import MolOptDataset\n",
    "from mol_opt.data_mol_opt import get_loader\n",
    "from mol_opt.decoder_mol_opt import MolOptDecoder\n",
    "from mol_opt.arguments import get_args\n",
    "from mol_opt.train_mol_opt import main, get_latest_model\n",
    "from mol_opt.ot_utils import encode_target, FGW\n",
    "from mol_opt.train_mol_opt import ft\n",
    "from mol_opt.ot_utils import Penalty as PenaltyOld\n",
    "\n",
    "from rdkit.Chem import MolFromSmiles\n",
    "\n",
    "from molgen.metrics.Penalty import Penalty as PenaltyNew\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MolOpt(\n",
      "  (GCN): GCN(\n",
      "    (W_message_i): Linear(in_features=100, out_features=250, bias=False)\n",
      "    (W_message_h): Linear(in_features=250, out_features=250, bias=False)\n",
      "    (W_message_o): Linear(in_features=343, out_features=100, bias=True)\n",
      "    (W_mol_h): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (W_mol_o): Linear(in_features=100, out_features=1, bias=True)\n",
      "    (dropout_gcn): Dropout(p=0.0, inplace=False)\n",
      "    (dropout_ffn): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (opt0): Linear(in_features=100, out_features=250, bias=True)\n",
      "  (opt1): Linear(in_features=250, out_features=100, bias=True)\n",
      ") Namespace(N_transformer=6, agg_func='sum', annealing_rate=0.0005, batch_norm=False, batch_size=6, connectivity=False, connectivity_hard=False, connectivity_lambda=5e-05, cuda=True, device='cuda:0', dim_tangent_space=40, dropout_ffn=0.0, dropout_gcn=0.0, dropout_transformer=0.1, euler_characteristic_penalty=False, euler_lambda=5e-05, ffn_activation='LeakyReLU', init_decoder_model='pointwise10-onebatch_decode', init_model='pointwise10-onebatch', linear_out=False, model_type='pointwise', n_epochs=2000, n_ffn_hidden=100, n_ffn_transformer=100, n_heads_transformer=10, n_hidden=250, n_labels=1, n_layers=5, one_batch_train=True, ot_solver='emd', output_dir='mol_opt/output_pointwise10-onebatch', pc_hidden=100, pred_hidden=150, scale_lambdas=False, sinkhorn_entropy=0.1, sinkhorn_max_it=10000, task='qed', tb_logs_dir='mol_opt/logs_pointwise10-onebatch', valency=False, valency_hard=False, valency_lambda=0.0007)\n",
      "MolOptDecoder(\n",
      "  (fc1_SYMBOLS): Linear(in_features=100, out_features=150, bias=True)\n",
      "  (fc2_SYMBOLS): Linear(in_features=150, out_features=64, bias=True)\n",
      "  (fc1_CHARGES): Linear(in_features=100, out_features=150, bias=True)\n",
      "  (fc2_CHARGES): Linear(in_features=150, out_features=5, bias=True)\n",
      "  (fc1_BONDS): Linear(in_features=200, out_features=300, bias=True)\n",
      "  (fc2_BONDS): Linear(in_features=300, out_features=5, bias=True)\n",
      ") Namespace(N_transformer=6, agg_func='sum', annealing_rate=0.0005, batch_norm=False, batch_size=6, connectivity=False, connectivity_hard=False, connectivity_lambda=5e-05, cuda=True, device='cuda:0', dim_tangent_space=40, dropout_ffn=0.0, dropout_gcn=0.0, dropout_transformer=0.1, euler_characteristic_penalty=False, euler_lambda=5e-05, ffn_activation='LeakyReLU', init_decoder_model='pointwise10-onebatch_decode', init_model='pointwise10-onebatch', linear_out=False, model_type='pointwise', n_epochs=2000, n_ffn_hidden=100, n_ffn_transformer=100, n_heads_transformer=10, n_hidden=250, n_labels=1, n_layers=5, one_batch_train=True, ot_solver='emd', output_dir='mol_opt/output_pointwise10-onebatch', pc_hidden=100, pred_hidden=150, scale_lambdas=False, sinkhorn_entropy=0.1, sinkhorn_max_it=10000, task='qed', tb_logs_dir='mol_opt/logs_pointwise10-onebatch', valency=False, valency_hard=False, valency_lambda=0.0007)\n"
     ]
    }
   ],
   "source": [
    "sys.argv = [\"\", \"-cuda\"]\n",
    "args = get_args()\n",
    "model = \"pointwise10-onebatch\"\n",
    "args.output_dir = f\"mol_opt/output_{model}/\"\n",
    "\n",
    "model_iter = 1000 \n",
    "\n",
    "model_name = \"model_{}_{}\".format(model, model_iter)\n",
    "model_decode_name = \"model_{}_decode_{}\".format(model, model_iter)\n",
    "\n",
    "molopt, config = load_model(args.output_dir + model_name, MolOpt, args.device)\n",
    "print (molopt, config)\n",
    "\n",
    "molopt_decoder, config_decoder = load_model(args.output_dir + model_decode_name, MolOptDecoder, args.device)\n",
    "print (molopt_decoder, config_decoder)\n",
    "\n",
    "loss = FGW(alpha = 0.5)\n",
    "\n",
    "n_data = 36\n",
    "data_loader = get_loader(\"iclr19-graph2graph/data/qed\", \"train_pairs\", n_data, True)\n",
    "for i in data_loader:\n",
    "    X = (MolGraph(i[0]))\n",
    "    Y = (MolGraph(i[1]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.conn_penalty_function = 'capped_logdet2'\n",
    "pen = PenaltyNew(args, prev_epoch = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_embedding = molopt.forward(X)\n",
    "yhat_logits = molopt_decoder.forward(yhat_embedding, X, Y)\n",
    "yhat_labels = molopt_decoder.discretize(*yhat_logits)\n",
    "\n",
    "pred_pack = (yhat_labels, yhat_logits, Y.scope), Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.7847447\n",
      " euler_penalty:25.6435683\n",
      "Penalty params: tau=1.00000 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=1\n",
      "epoch=1\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.5047135\n",
      " euler_penalty:25.6922082\n",
      "Penalty params: tau=1.00000 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=1\n",
      "epoch=2\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.4603640\n",
      " euler_penalty:25.8834195\n",
      "Penalty params: tau=0.99990 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=2\n",
      "epoch=3\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.5447070\n",
      " euler_penalty:25.5283271\n",
      "Penalty params: tau=0.99980 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=3\n",
      "epoch=4\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:0.9759809\n",
      " euler_penalty:26.2329356\n",
      "Penalty params: tau=0.99970 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=4\n",
      "epoch=5\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.8451453\n",
      " euler_penalty:25.8235575\n",
      "Penalty params: tau=0.99960 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=5\n",
      "epoch=6\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.3505206\n",
      " euler_penalty:25.7318827\n",
      "Penalty params: tau=0.99950 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=6\n",
      "epoch=7\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.7443433\n",
      " euler_penalty:25.6441735\n",
      "Penalty params: tau=0.99940 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=7\n",
      "epoch=8\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.7355396\n",
      " euler_penalty:25.8410102\n",
      "Penalty params: tau=0.99930 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=8\n",
      "epoch=9\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.1042779\n",
      " euler_penalty:25.8952416\n",
      "Penalty params: tau=0.99920 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=9\n",
      "epoch=10\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.2182999\n",
      " euler_penalty:25.9640249\n",
      "Penalty params: tau=0.99910 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=10\n",
      "epoch=11\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.6154504\n",
      " euler_penalty:25.8400777\n",
      "Penalty params: tau=0.99900 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=11\n",
      "epoch=12\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.4552252\n",
      " euler_penalty:25.5140669\n",
      "Penalty params: tau=0.99890 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=12\n",
      "epoch=13\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.4328904\n",
      " euler_penalty:25.6484646\n",
      "Penalty params: tau=0.99880 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=13\n",
      "epoch=14\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.6782213\n",
      " euler_penalty:25.7719116\n",
      "Penalty params: tau=0.99870 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=14\n",
      "epoch=15\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.0701127\n",
      " euler_penalty:25.9729479\n",
      "Penalty params: tau=0.99860 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=15\n",
      "epoch=16\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.2709655\n",
      " euler_penalty:25.7089064\n",
      "Penalty params: tau=0.99850 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=16\n",
      "epoch=17\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.5852016\n",
      " euler_penalty:25.3894060\n",
      "Penalty params: tau=0.99840 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=17\n",
      "epoch=18\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.3630994\n",
      " euler_penalty:25.8439365\n",
      "Penalty params: tau=0.99830 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=18\n",
      "epoch=19\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.6773083\n",
      " euler_penalty:25.3632812\n",
      "Penalty params: tau=0.99820 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=19\n",
      "epoch=20\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.2746309\n",
      " euler_penalty:25.9384732\n",
      "Penalty params: tau=0.99810 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=20\n",
      "epoch=21\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.4289663\n",
      " euler_penalty:25.6766900\n",
      "Penalty params: tau=0.99800 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=21\n",
      "epoch=22\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.4387601\n",
      " euler_penalty:25.4761081\n",
      "Penalty params: tau=0.99790 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=22\n",
      "epoch=23\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.1344429\n",
      " euler_penalty:25.8342506\n",
      "Penalty params: tau=0.99780 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=23\n",
      "epoch=24\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.8599817\n",
      " euler_penalty:25.2957323\n",
      "Penalty params: tau=0.99770 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=24\n",
      "epoch=25\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.5461138\n",
      " euler_penalty:25.7810465\n",
      "Penalty params: tau=0.99760 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=25\n",
      "epoch=26\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.1503879\n",
      " euler_penalty:25.8862288\n",
      "Penalty params: tau=0.99750 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=26\n",
      "epoch=27\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.3664716\n",
      " euler_penalty:25.9876166\n",
      "Penalty params: tau=0.99740 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=27\n",
      "epoch=28\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.4908940\n",
      " euler_penalty:25.4710642\n",
      "Penalty params: tau=0.99730 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=28\n",
      "epoch=29\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.4131387\n",
      " euler_penalty:25.7867974\n",
      "Penalty params: tau=0.99720 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=29\n",
      "epoch=30\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.1963461\n",
      " euler_penalty:25.8926239\n",
      "Penalty params: tau=0.99710 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=30\n",
      "epoch=31\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.4223852\n",
      " euler_penalty:25.9605696\n",
      "Penalty params: tau=0.99700 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=31\n",
      "epoch=32\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.1471824\n",
      " euler_penalty:26.1038140\n",
      "Penalty params: tau=0.99690 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=32\n",
      "epoch=33\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.2121373\n",
      " euler_penalty:26.0929362\n",
      "Penalty params: tau=0.99680 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=33\n",
      "epoch=34\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.5400913\n",
      " euler_penalty:25.5840997\n",
      "Penalty params: tau=0.99671 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=34\n",
      "epoch=35\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.2615469\n",
      " euler_penalty:26.1605292\n",
      "Penalty params: tau=0.99661 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=35\n",
      "epoch=36\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.1816001\n",
      " euler_penalty:26.0837911\n",
      "Penalty params: tau=0.99651 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=36\n",
      "epoch=37\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.3540972\n",
      " euler_penalty:25.9758759\n",
      "Penalty params: tau=0.99641 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=37\n",
      "epoch=38\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.0437404\n",
      " euler_penalty:26.0817362\n",
      "Penalty params: tau=0.99631 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=38\n",
      "epoch=39\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.1669916\n",
      " euler_penalty:25.9464654\n",
      "Penalty params: tau=0.99621 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=39\n",
      "epoch=40\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.5207836\n",
      " euler_penalty:25.6059435\n",
      "Penalty params: tau=0.99611 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=40\n",
      "epoch=41\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.5601764\n",
      " euler_penalty:25.7162035\n",
      "Penalty params: tau=0.99601 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=41\n",
      "epoch=42\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.4765846\n",
      " euler_penalty:25.8536157\n",
      "Penalty params: tau=0.99591 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=42\n",
      "epoch=43\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.7220984\n",
      " euler_penalty:25.8624234\n",
      "Penalty params: tau=0.99581 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=43\n",
      "epoch=44\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:2.1525146\n",
      " euler_penalty:25.2470924\n",
      "Penalty params: tau=0.99571 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=44\n",
      "epoch=45\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.4788173\n",
      " euler_penalty:26.0160556\n",
      "Penalty params: tau=0.99561 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=45\n",
      "epoch=46\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.1196070\n",
      " euler_penalty:25.9901140\n",
      "Penalty params: tau=0.99551 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=46\n",
      "epoch=47\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.4602540\n",
      " euler_penalty:25.7476705\n",
      "Penalty params: tau=0.99541 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=47\n",
      "epoch=48\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.5655121\n",
      " euler_penalty:25.7007175\n",
      "Penalty params: tau=0.99531 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=48\n",
      "epoch=49\n",
      " conn_penalty:0.0000004\n",
      " val_penalty:1.0852628\n",
      " euler_penalty:26.2146233\n",
      "Penalty params: tau=0.99521 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=49\n"
     ]
    }
   ],
   "source": [
    "for idx in range(50):\n",
    "    stats_tracker = StatsTracker()\n",
    "    con_loss, val_loss, eul_loss = pen(*pred_pack, idx)\n",
    "    stats_tracker.add_stat('conn_penalty', con_loss.item(), n_data)\n",
    "    stats_tracker.add_stat('val_penalty', val_loss.item(), n_data)\n",
    "    stats_tracker.add_stat('euler_penalty', eul_loss.item(), n_data)\n",
    "    \n",
    "    stats_tracker.print_stats(\"epoch={}\".format(idx))\n",
    "    pen.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penalty params: tau=1.00000 conn_l=0.02500 val_l=0.07000 euler_l=0.30000 epoch=1\n"
     ]
    }
   ],
   "source": [
    "PenaltyNew(args, prev_epoch = 0).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adjM = pen(*pred_pack, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1921e-06, 1.3630e-02, 1.0782e-04, 4.4374e-03, 1.8051e-01, 8.4739e-02,\n",
       "         0.0000e+00, 8.5733e-02, 9.3007e-04, 1.4305e-06, 0.0000e+00, 2.6822e-06,\n",
       "         0.0000e+00, 5.4988e-01, 1.2422e-02, 6.8890e-01, 3.5245e-01, 7.2306e-04,\n",
       "         2.2233e-05, 5.0822e-01, 2.1683e-01, 1.0255e-02],\n",
       "        [1.3630e-02, 3.9140e-03, 5.3970e-01, 2.2346e-03, 3.8520e-02, 8.8930e-04,\n",
       "         2.3283e-01, 4.9831e-01, 2.1935e-05, 3.9616e-01, 5.9605e-08, 7.7458e-01,\n",
       "         2.7418e-06, 7.3087e-02, 5.0057e-01, 2.1463e-01, 2.4846e-02, 6.6979e-02,\n",
       "         1.3762e-01, 1.4181e-01, 3.7023e-02, 7.1169e-02],\n",
       "        [1.0782e-04, 5.3970e-01, 7.7593e-02, 3.8449e-01, 9.0742e-02, 1.0008e-01,\n",
       "         1.4413e-01, 2.2877e-01, 5.5884e-03, 2.4866e-02, 0.0000e+00, 3.4104e-02,\n",
       "         0.0000e+00, 1.4942e-02, 6.2261e-02, 2.6948e-02, 6.4849e-02, 6.2976e-02,\n",
       "         4.5100e-01, 3.7974e-01, 8.5898e-01, 2.9276e-01],\n",
       "        [4.4374e-03, 2.2346e-03, 3.8449e-01, 1.2787e-03, 3.2146e-01, 3.9279e-05,\n",
       "         1.8100e-01, 3.6682e-01, 3.2187e-06, 2.2549e-01, 8.3447e-07, 2.4369e-01,\n",
       "         9.6946e-01, 8.7332e-02, 1.8843e-01, 2.9255e-02, 2.2362e-03, 2.2261e-03,\n",
       "         1.8684e-01, 3.9589e-03, 3.2784e-03, 1.7802e-01],\n",
       "        [1.8051e-01, 3.8520e-02, 9.0742e-02, 3.2146e-01, 5.5588e-02, 3.2130e-02,\n",
       "         1.3192e-02, 1.4488e-03, 1.9649e-03, 4.3935e-01, 0.0000e+00, 3.9828e-03,\n",
       "         2.9802e-07, 6.2353e-02, 1.8912e-01, 5.5398e-02, 8.5474e-01, 2.2727e-01,\n",
       "         2.5835e-01, 8.8083e-02, 4.0521e-01, 6.1901e-02],\n",
       "        [8.4739e-02, 8.8930e-04, 1.0008e-01, 3.9279e-05, 3.2130e-02, 9.9156e-03,\n",
       "         3.6837e-02, 2.0359e-01, 1.0431e-05, 2.3731e-01, 1.7583e-04, 2.7830e-03,\n",
       "         8.0976e-03, 7.8816e-03, 5.8036e-03, 2.6581e-01, 1.2457e-04, 3.4642e-04,\n",
       "         1.0487e-02, 1.5619e-03, 5.6088e-05, 5.9958e-01],\n",
       "        [0.0000e+00, 2.3283e-01, 1.4413e-01, 1.8100e-01, 1.3192e-02, 3.6837e-02,\n",
       "         1.3375e-04, 3.1279e-01, 6.0856e-02, 5.7349e-03, 0.0000e+00, 6.6757e-02,\n",
       "         0.0000e+00, 4.3370e-03, 1.7172e-02, 1.9915e-01, 1.8426e-01, 4.2121e-01,\n",
       "         3.8646e-02, 7.3638e-02, 3.0168e-01, 1.2006e-01],\n",
       "        [8.5733e-02, 4.9831e-01, 2.2877e-01, 3.6682e-01, 1.4488e-03, 2.0359e-01,\n",
       "         3.1279e-01, 5.6511e-04, 9.0654e-01, 5.6626e-02, 6.9261e-05, 1.7106e-02,\n",
       "         1.4777e-02, 2.1281e-02, 1.6898e-02, 7.8782e-03, 2.7549e-02, 8.4391e-02,\n",
       "         6.9671e-02, 3.5113e-02, 4.2951e-01, 7.7373e-03],\n",
       "        [9.3007e-04, 2.1935e-05, 5.5884e-03, 3.2187e-06, 1.9649e-03, 1.0431e-05,\n",
       "         6.0856e-02, 9.0654e-01, 2.3842e-07, 6.8427e-01, 9.9960e-01, 1.2676e-03,\n",
       "         3.2949e-02, 4.0287e-04, 5.4955e-04, 6.9860e-03, 2.7162e-04, 7.4923e-05,\n",
       "         6.0588e-04, 3.3671e-04, 1.6451e-04, 5.0461e-01],\n",
       "        [1.4305e-06, 3.9616e-01, 2.4866e-02, 2.2549e-01, 4.3935e-01, 2.3731e-01,\n",
       "         5.7349e-03, 5.6626e-02, 6.8427e-01, 9.6560e-06, 0.0000e+00, 1.3105e-02,\n",
       "         0.0000e+00, 1.3250e-01, 2.4045e-02, 1.6333e-03, 6.4737e-02, 3.7083e-01,\n",
       "         8.0702e-02, 2.0016e-02, 1.4314e-01, 4.5421e-03],\n",
       "        [0.0000e+00, 5.9605e-08, 0.0000e+00, 8.3447e-07, 0.0000e+00, 1.7583e-04,\n",
       "         0.0000e+00, 6.9261e-05, 9.9960e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 4.2176e-04, 0.0000e+00, 3.9041e-05, 4.7088e-06, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 4.7684e-07, 6.5565e-07],\n",
       "        [2.6822e-06, 7.7458e-01, 3.4104e-02, 2.4369e-01, 3.9828e-03, 2.7830e-03,\n",
       "         6.6757e-02, 1.7106e-02, 1.2676e-03, 1.3105e-02, 0.0000e+00, 1.9832e-02,\n",
       "         0.0000e+00, 6.0704e-02, 8.5624e-02, 4.9844e-02, 5.6489e-02, 6.0115e-01,\n",
       "         4.2488e-02, 5.0175e-02, 2.1316e-01, 6.5680e-02],\n",
       "        [0.0000e+00, 2.7418e-06, 0.0000e+00, 9.6946e-01, 2.9802e-07, 8.0976e-03,\n",
       "         0.0000e+00, 1.4777e-02, 3.2949e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 1.2290e-03, 0.0000e+00, 2.2011e-03, 1.2308e-03, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 5.7161e-05, 1.1125e-03],\n",
       "        [5.4988e-01, 7.3087e-02, 1.4942e-02, 8.7332e-02, 6.2353e-02, 7.8816e-03,\n",
       "         4.3370e-03, 2.1281e-02, 4.0287e-04, 1.3250e-01, 4.2176e-04, 6.0704e-02,\n",
       "         1.2290e-03, 2.1496e-03, 2.9329e-01, 9.7556e-01, 1.4604e-02, 1.8318e-02,\n",
       "         1.3252e-01, 1.0917e-02, 9.2651e-02, 1.3552e-01],\n",
       "        [1.2422e-02, 5.0057e-01, 6.2261e-02, 1.8843e-01, 1.8912e-01, 5.8036e-03,\n",
       "         1.7172e-02, 1.6898e-02, 5.4955e-04, 2.4045e-02, 0.0000e+00, 8.5624e-02,\n",
       "         0.0000e+00, 2.9329e-01, 1.5160e-02, 7.4729e-02, 1.7821e-01, 1.3680e-01,\n",
       "         3.0847e-01, 9.8482e-01, 1.1378e-01, 6.6438e-02],\n",
       "        [6.8890e-01, 2.1463e-01, 2.6948e-02, 2.9255e-02, 5.5398e-02, 2.6581e-01,\n",
       "         1.9915e-01, 7.8782e-03, 6.9860e-03, 1.6333e-03, 3.9041e-05, 4.9844e-02,\n",
       "         2.2011e-03, 9.7556e-01, 7.4729e-02, 3.3688e-02, 3.1209e-01, 2.3706e-02,\n",
       "         2.1200e-01, 3.9457e-02, 3.2046e-01, 1.5715e-02],\n",
       "        [3.5245e-01, 2.4846e-02, 6.4849e-02, 2.2362e-03, 8.5474e-01, 1.2457e-04,\n",
       "         1.8426e-01, 2.7549e-02, 2.7162e-04, 6.4737e-02, 4.7088e-06, 5.6489e-02,\n",
       "         1.2308e-03, 1.4604e-02, 1.7821e-01, 3.1209e-01, 1.5013e-01, 7.1589e-02,\n",
       "         8.9713e-02, 1.1436e-01, 1.4987e-01, 1.3149e-01],\n",
       "        [7.2306e-04, 6.6979e-02, 6.2976e-02, 2.2261e-03, 2.2727e-01, 3.4642e-04,\n",
       "         4.2121e-01, 8.4391e-02, 7.4923e-05, 3.7083e-01, 0.0000e+00, 6.0115e-01,\n",
       "         0.0000e+00, 1.8318e-02, 1.3680e-01, 2.3706e-02, 7.1589e-02, 1.7366e-01,\n",
       "         9.4358e-01, 1.3968e-01, 2.1780e-01, 2.5299e-03],\n",
       "        [2.2233e-05, 1.3762e-01, 4.5100e-01, 1.8684e-01, 2.5835e-01, 1.0487e-02,\n",
       "         3.8646e-02, 6.9671e-02, 6.0588e-04, 8.0702e-02, 0.0000e+00, 4.2488e-02,\n",
       "         0.0000e+00, 1.3252e-01, 3.0847e-01, 2.1200e-01, 8.9713e-02, 9.4358e-01,\n",
       "         4.9782e-02, 4.2420e-01, 2.0927e-02, 1.1739e-01],\n",
       "        [5.0822e-01, 1.4181e-01, 3.7974e-01, 3.9589e-03, 8.8083e-02, 1.5619e-03,\n",
       "         7.3638e-02, 3.5113e-02, 3.3671e-04, 2.0016e-02, 0.0000e+00, 5.0175e-02,\n",
       "         0.0000e+00, 1.0917e-02, 9.8482e-01, 3.9457e-02, 1.1436e-01, 1.3968e-01,\n",
       "         4.2420e-01, 3.6214e-02, 4.9527e-02, 1.4082e-02],\n",
       "        [2.1683e-01, 3.7023e-02, 8.5898e-01, 3.2784e-03, 4.0521e-01, 5.6088e-05,\n",
       "         3.0168e-01, 4.2951e-01, 1.6451e-04, 1.4314e-01, 4.7684e-07, 2.1316e-01,\n",
       "         5.7161e-05, 9.2651e-02, 1.1378e-01, 3.2046e-01, 1.4987e-01, 2.1780e-01,\n",
       "         2.0927e-02, 4.9527e-02, 1.4681e-02, 6.8404e-02],\n",
       "        [1.0255e-02, 7.1169e-02, 2.9276e-01, 1.7802e-01, 6.1901e-02, 5.9958e-01,\n",
       "         1.2006e-01, 7.7373e-03, 5.0461e-01, 4.5421e-03, 6.5565e-07, 6.5680e-02,\n",
       "         1.1125e-03, 1.3552e-01, 6.6438e-02, 1.5715e-02, 1.3149e-01, 2.5299e-03,\n",
       "         1.1739e-01, 1.4082e-02, 6.8404e-02, 4.0376e-03]], device='cuda:0',\n",
       "       grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct adjacency matrix from this thing\n",
    "adjM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.randint(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonds = Y.get_graph_outputs()[0][\"BOND_TYPES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjM = 1 - bonds[:,:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pen.conn_penalty(adjM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6787, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = adjM.shape[0]\n",
    "device = adjM.device\n",
    "\n",
    "# get Laplacian\n",
    "L = torch.diag(torch.matmul(adjM, torch.ones(N, device=device))) - adjM\n",
    "L_mod = L + torch.ones_like(L, device=device) / N\n",
    "\n",
    "# calculate log dets\n",
    "# comment this line to use the rescaling procedure\n",
    "# return (-torch.logdet(L_mod + self.conn_eps * torch.eye(N, device=device)))\n",
    "\n",
    "# calculate rescaled eigenvalues\n",
    "eigvals = torch.symeig(L_mod, eigenvectors = True)[0]\n",
    "torch.sum(torch.exp(-eigvals))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap_value = 1e-02\n",
    "conn_eps = 1e-09\n",
    "\n",
    "eigenvals = torch.symeig(L_mod + conn_eps * torch.eye(N, device=device))[0]\n",
    "-torch.sum(torch.log(eigenvals.clamp(max = cap_value))) + len(eigvals) * np.log(cap_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e-09, 7.2240e-01, 1.0000e+00, 1.3127e+00, 1.5558e+00, 2.1292e+00,\n",
       "        2.2047e+00, 2.3685e+00, 2.4458e+00, 2.6390e+00, 2.7848e+00, 3.0165e+00,\n",
       "        3.1404e+00, 3.4076e+00, 3.6440e+00, 3.8691e+00, 4.0975e+00, 4.2596e+00,\n",
       "        4.3029e+00, 4.5827e+00, 4.8254e+00, 5.2925e+00], device='cuda:0',\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps = 1e-9\n",
    "beta = 1e-3\n",
    "- torch.sum(torch.log(eigvals.clamp(min = eps, max = beta))) + len(eigvals) * np.log(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5753, 0.7224, 1.0000, 1.3127, 1.5558, 2.1292, 2.2047, 2.3685, 2.4458,\n",
       "        2.6390, 2.7848, 3.0165, 3.1404, 3.4076, 3.6440, 3.8691, 4.0975, 4.2596,\n",
       "        4.3029, 4.5827, 4.8254, 5.2925], device='cuda:0',\n",
       "       grad_fn=<SymeigBackward>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5753, 0.7224, 1.0000, 1.3127, 1.5558, 2.1292, 2.2047, 2.3685, 2.4458,\n",
       "        2.6390, 2.7848, 3.0165, 3.1404, 3.4076, 3.6440, 3.8691, 4.0975, 4.2596,\n",
       "        4.3029, 4.5827, 4.8254, 5.2925], device='cuda:0',\n",
       "       grad_fn=<SymeigBackward>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.symeig(\n",
       "eigenvalues=tensor([0.5753, 0.7224, 1.0000, 1.3127, 1.5558, 2.1292, 2.2047, 2.3685, 2.4458,\n",
       "        2.6390, 2.7848, 3.0165, 3.1404, 3.4076, 3.6440, 3.8691, 4.0975, 4.2596,\n",
       "        4.3029, 4.5827, 4.8254, 5.2925], device='cuda:0',\n",
       "       grad_fn=<SymeigBackward>),\n",
       "eigenvectors=tensor([[-1.0833e-01, -1.3210e-01, -2.1320e-01,  5.5375e-02, -4.8060e-01,\n",
       "         -5.8712e-02,  8.7189e-02, -8.2707e-03, -1.6828e-02, -9.8001e-03,\n",
       "         -2.6583e-01,  6.7055e-02,  6.1515e-01, -2.1690e-01,  1.8174e-01,\n",
       "          1.0664e-01,  2.4309e-01,  4.4703e-02, -1.9365e-01, -1.8781e-01,\n",
       "         -1.6044e-02, -2.5964e-02],\n",
       "        [-5.5351e-02, -6.2991e-02, -2.1320e-01,  8.6333e-02,  2.0112e-01,\n",
       "         -1.8538e-01,  3.3151e-02,  2.3933e-03,  1.0188e-01,  2.0134e-02,\n",
       "         -2.3071e-01,  5.7943e-02, -2.1760e-01, -5.2096e-02, -2.0261e-01,\n",
       "          4.9707e-01,  4.1638e-01,  1.8071e-01,  7.4568e-02,  1.0142e-01,\n",
       "          1.4616e-01,  4.6405e-01],\n",
       "        [-6.2844e-02, -3.5755e-02, -2.1320e-01,  2.0978e-02,  1.3148e-01,\n",
       "          3.5224e-02, -6.3310e-02, -2.0361e-01, -1.8807e-03,  1.0022e-01,\n",
       "         -1.0836e-01, -5.1074e-01, -7.6849e-02, -1.9998e-01, -1.2706e-01,\n",
       "          2.4444e-01, -2.2418e-01,  1.7710e-01, -5.0775e-02, -2.6352e-01,\n",
       "          3.6638e-01, -4.3883e-01],\n",
       "        [-7.8511e-02,  2.8683e-01, -2.1320e-01,  1.3487e-02,  6.3118e-02,\n",
       "          3.8847e-02,  2.1580e-02, -2.7480e-02,  6.6845e-02,  4.7487e-02,\n",
       "         -3.3630e-03, -1.2277e-01, -1.4315e-01, -5.1425e-02,  7.6184e-01,\n",
       "          1.6658e-01, -2.1236e-01, -2.4641e-01, -1.4094e-01,  7.0021e-02,\n",
       "          9.2081e-02,  2.5894e-01],\n",
       "        [-7.7677e-02, -5.4421e-02, -2.1320e-01,  7.6697e-02, -4.4035e-02,\n",
       "          4.1302e-01, -9.7645e-02,  3.1108e-01,  2.1746e-02, -5.5513e-03,\n",
       "          3.1172e-02, -1.1015e-01, -1.3526e-01, -9.5930e-02,  2.5287e-01,\n",
       "         -3.1153e-01,  3.8707e-01,  2.2426e-01,  4.8310e-01, -1.9069e-02,\n",
       "          1.2736e-01, -9.3021e-02],\n",
       "        [ 5.2009e-04, -8.6649e-02, -2.1320e-01, -8.2685e-01, -1.8930e-03,\n",
       "         -1.3963e-01, -1.7574e-01,  1.5021e-01, -2.2394e-01, -3.1663e-01,\n",
       "         -1.2051e-01, -3.0037e-02, -6.0975e-02,  6.5979e-02,  2.7017e-02,\n",
       "         -4.2058e-02, -2.9715e-02,  4.6705e-02, -1.9478e-02, -7.6375e-02,\n",
       "          2.7735e-02,  6.2958e-02],\n",
       "        [-5.1655e-02, -5.0350e-02, -2.1320e-01,  4.8962e-02,  2.1553e-01,\n",
       "          1.6598e-01,  4.2627e-01, -3.7895e-01, -5.5068e-01, -1.4405e-01,\n",
       "          1.2264e-01,  3.5926e-01, -8.6078e-02, -1.8744e-01,  5.6520e-02,\n",
       "         -8.7446e-03, -1.0516e-02,  6.9660e-02,  7.3039e-02, -1.3269e-01,\n",
       "         -6.5087e-02, -3.9680e-02],\n",
       "        [ 6.4963e-02, -2.4587e-03, -2.1320e-01, -4.2745e-02,  1.6279e-01,\n",
       "          8.8057e-02,  2.2176e-01, -2.6204e-01,  2.0080e-01, -7.9597e-02,\n",
       "         -3.5855e-01, -8.0968e-02,  5.7414e-02,  5.0586e-01,  1.1808e-01,\n",
       "         -5.4952e-02,  1.4049e-01,  1.3730e-01, -7.4148e-02,  3.7095e-01,\n",
       "         -2.2738e-01, -3.2416e-01],\n",
       "        [ 3.6419e-01,  1.5922e-02, -2.1320e-01, -5.0802e-02,  8.3662e-02,\n",
       "          1.2820e-01,  1.3574e-01, -1.2540e-01,  3.7282e-01,  2.9714e-02,\n",
       "         -6.8992e-02,  2.0913e-01,  1.7562e-01,  2.4738e-01, -9.0599e-02,\n",
       "         -1.6635e-01, -1.7981e-01, -1.0889e-01,  1.9808e-01, -4.5777e-01,\n",
       "          3.0414e-01,  2.4437e-01],\n",
       "        [ 4.6393e-02, -2.8983e-02, -2.1320e-01, -4.6623e-02,  1.3513e-01,\n",
       "          1.9754e-01,  6.3564e-02,  2.0028e-01,  5.0629e-01, -3.7635e-01,\n",
       "          1.3253e-01,  2.1075e-01, -2.5970e-02, -4.6365e-01, -1.1105e-01,\n",
       "          1.4561e-01, -1.3381e-01, -5.4844e-02, -1.2772e-01,  1.1367e-01,\n",
       "         -2.3201e-01, -1.8666e-01],\n",
       "        [ 8.5638e-01,  5.7024e-02, -2.1320e-01,  1.6297e-01, -1.5022e-01,\n",
       "         -1.1339e-01, -1.1275e-01,  9.1616e-02, -2.5786e-01, -1.8091e-02,\n",
       "          3.8597e-02, -1.0368e-01, -8.1952e-02, -1.0276e-01,  3.4249e-02,\n",
       "          5.7991e-02,  5.8051e-02,  3.3340e-02, -5.9938e-02,  1.2773e-01,\n",
       "         -7.9479e-02, -5.6914e-02],\n",
       "        [-8.5517e-02, -4.7230e-02, -2.1320e-01,  1.6256e-01,  3.6963e-01,\n",
       "         -5.0815e-01,  1.3605e-01,  4.9017e-01, -7.6934e-02,  2.3482e-01,\n",
       "         -2.0386e-01,  1.2227e-01,  7.2637e-02, -1.9463e-02,  7.0468e-02,\n",
       "         -1.6697e-01, -1.8215e-01, -5.1028e-04,  1.0638e-01, -1.2212e-01,\n",
       "         -1.4297e-01, -1.7041e-01],\n",
       "        [-1.3942e-01,  8.9819e-01, -2.1320e-01, -1.3442e-02, -1.2374e-01,\n",
       "         -3.8316e-02, -2.3857e-02,  2.4780e-02, -5.5312e-02, -2.7538e-02,\n",
       "          6.3374e-03,  5.7138e-02,  6.3182e-02,  1.3965e-02, -2.8205e-01,\n",
       "         -5.4712e-02,  6.8633e-02,  7.4612e-02,  4.0025e-02, -1.6435e-02,\n",
       "         -2.5375e-02, -5.9739e-02],\n",
       "        [-1.0032e-01, -1.1339e-01, -2.1320e-01,  3.4720e-02, -4.4770e-01,\n",
       "         -2.9963e-01,  2.8126e-01, -3.3265e-02,  2.0715e-01,  9.8106e-03,\n",
       "          3.1797e-01, -1.2257e-02, -3.6018e-01,  1.1273e-01,  2.7137e-02,\n",
       "         -1.7707e-01, -1.4491e-01,  4.3759e-01, -1.4725e-01, -3.7988e-02,\n",
       "         -4.4143e-03,  7.4612e-02],\n",
       "        [-9.8610e-02, -7.7610e-02, -2.1320e-01,  1.4868e-01, -1.6589e-02,\n",
       "         -1.1175e-01, -4.3257e-01, -1.9256e-01,  5.4294e-02, -2.9267e-03,\n",
       "         -4.9171e-02,  2.6176e-01, -3.6227e-01,  6.7701e-02,  1.1445e-03,\n",
       "         -1.9545e-01,  3.1669e-01, -3.5048e-01, -3.0786e-01, -2.5997e-01,\n",
       "         -3.1349e-03, -2.1520e-01],\n",
       "        [-9.6501e-02, -1.1589e-01, -2.1320e-01, -9.9549e-03, -3.4146e-01,\n",
       "         -1.4079e-01,  2.3247e-01,  1.4721e-02, -3.5467e-02, -4.5319e-02,\n",
       "          6.5416e-02, -6.5708e-02, -8.1858e-02,  8.4441e-02, -1.5668e-01,\n",
       "          1.7273e-01, -4.1895e-02, -6.0777e-01,  4.3313e-01,  2.3836e-01,\n",
       "          1.3013e-01, -1.6854e-01],\n",
       "        [-9.3273e-02, -1.0097e-01, -2.1320e-01,  8.9306e-02, -1.7350e-01,\n",
       "          5.0912e-01, -5.4581e-02,  3.7756e-01, -2.2383e-01,  2.1461e-01,\n",
       "         -1.4712e-01,  1.6678e-01, -1.1994e-01,  2.9507e-01, -1.8996e-01,\n",
       "          2.1527e-01, -2.9428e-01,  1.6255e-02, -2.6805e-01,  1.1468e-02,\n",
       "         -3.2210e-02,  5.8781e-02],\n",
       "        [-7.4145e-02, -7.2100e-02, -2.1320e-01,  1.3916e-01,  2.4764e-01,\n",
       "         -4.7767e-02, -2.6014e-02,  1.4087e-01, -1.0222e-01, -2.1432e-01,\n",
       "          4.2843e-01, -5.1662e-02,  3.3608e-01,  1.7510e-01, -6.9744e-02,\n",
       "         -1.4681e-01,  1.0565e-01, -3.3090e-02, -3.1282e-01,  2.8718e-01,\n",
       "          4.8318e-01,  4.0899e-02],\n",
       "        [-8.5008e-02, -6.4903e-02, -2.1320e-01,  1.1282e-01,  1.0043e-01,\n",
       "         -1.4536e-02, -2.2813e-01, -4.3963e-02, -1.7867e-02, -1.2112e-01,\n",
       "          4.1881e-01, -2.4453e-01,  1.9587e-01,  3.0122e-01,  2.4367e-02,\n",
       "          2.8874e-01,  1.9514e-02,  1.0724e-02,  1.9228e-01, -3.1557e-01,\n",
       "         -5.0916e-01,  8.0985e-02],\n",
       "        [-1.0396e-01, -9.9985e-02, -2.1320e-01,  1.6106e-01, -6.5702e-02,\n",
       "         -8.4056e-02, -5.1529e-01, -3.0020e-01, -2.2130e-02, -5.4491e-02,\n",
       "         -1.4727e-01,  2.0191e-01,  1.3374e-01, -1.0954e-01, -1.8666e-02,\n",
       "         -1.2586e-01, -4.1458e-01,  1.9567e-01,  2.8472e-01,  3.4504e-01,\n",
       "          1.4728e-02,  1.3524e-01],\n",
       "        [-6.3918e-02, -7.1777e-02, -2.1320e-01,  5.7369e-02,  5.3285e-02,\n",
       "          1.0951e-01,  1.3386e-01, -8.1013e-02, -4.4166e-02,  1.8061e-02,\n",
       "         -1.7371e-01, -4.8540e-01, -2.3401e-02, -2.1893e-01, -2.6974e-01,\n",
       "         -4.4999e-01, -1.6909e-02, -2.2739e-01, -1.5967e-01,  3.2164e-02,\n",
       "         -2.6450e-01,  4.0302e-01],\n",
       "        [ 4.2593e-02, -4.0405e-02, -2.1320e-01, -3.8005e-01,  8.1622e-02,\n",
       "          5.6616e-02, -4.2999e-02, -1.4693e-01,  9.7290e-02,  7.4092e-01,\n",
       "          3.1556e-01,  1.0389e-01,  1.2578e-01, -1.5134e-01, -3.7275e-02,\n",
       "          5.4199e-03,  1.2954e-01, -1.9238e-02, -2.3019e-02,  1.9131e-01,\n",
       "         -9.0138e-02, -4.4725e-02]], device='cuda:0', grad_fn=<SymeigBackward>))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.symeig(L_mod, eigenvectors = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
